{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# GBM\n",
        "\n",
        "이정재  \n",
        "2024-03-16\n",
        "\n",
        "# GBM (Gradient Boosting Machine)\n",
        "\n",
        "`-` 다음은\n",
        "[wikipedia](https://en.wikipedia.org/wiki/Gradient_boosting)의 Gradient\n",
        "Boosting Algorithm이다. 다음의 수식을 먼저 읽고 최대한 이해하려고\n",
        "노력해보자\n",
        "\n",
        "## Algorithm\n",
        "\n",
        "1.  **initialize model with a constant value:**  \n",
        "    $F_0(x) = \\underset{\\gamma}{\\arg\\min}\\sum_{i=1}^n L(y_i, \\gamma)$\n",
        "\n",
        "2.  **For m = 1 to M**:\n",
        "\n",
        "    1.  Compute so-called pseudo-residuals\n",
        "        $r_{im} = - \\Bigg[\\displaystyle\\frac{\\partial L(y_i, F(x_i))}{\\partial F(x_i)}\\Bigg]_{F(x) = F_{m-1}(x)}$\n",
        "        for $i = 1, ... ,n$\n",
        "    2.  Fit a base learner (or weak learner, e.g. tree) closed under\n",
        "        scaling $h_m(x)$ to pseuo-residuals, i.e. train it using the\n",
        "        training set $\\{(x_i, r_{im})\\}_{i=1}^{n}$\n",
        "    3.  Compute multiplier $\\gamma_{m}$ by solving the following\n",
        "        one-dimensional optimization problem:  \n",
        "        $\\gamma_m = \\underset{\\gamma}{\\arg\\min}\\sum_{i=1}^n L(y_i, F_{m-1}(x_i) + \\gamma h_m(x_i))$\n",
        "    4.  Update the model:  \n",
        "        $F_m(x) = F_{m-1}(x) + \\gamma_m h_m(x)$\n",
        "\n",
        "3.  **Output $\\hat{f(x)} = F_M(x)$**\n",
        "\n",
        "수식으로만 완전히 이해하기가 어렵기 때문에 간단한 예시와\n",
        "`pandas dataframe`을 이용하여 설명해보겠습니다.  \n",
        "다음과 같은 데이터가 있다고 가정해보겠습니다."
      ],
      "id": "63585d8e-f0ca-4cdc-afee-13d994eb989d"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "data = pd.DataFrame({'공부시간(m)':[150, 120, 60, 80], '성별':['여','남','남', '여'], '수학점수':[90, 65, 55, '???']})\n",
        "data"
      ],
      "id": "504f9bae-f503-421c-b970-31e8f7723ab8"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "다음과 같은 데이터가 있다고 할 때, 저희는 `공부시간(m)`을 통하여\n",
        "`수학점수`를 예측하고 싶습니다.\n",
        "\n",
        "#### **1단계**\n",
        "\n",
        "1.  **initialize model with a constant value:**  \n",
        "    $F_0(x) = \\underset{\\gamma}{\\arg\\min}\\sum_{i=1}^n L(y_i, \\gamma)$\n",
        "\n",
        "2.  초기모델을 상수로 정의합니다. 여기서 상수는 $y_i$와 $\\gamma$입니다.\n",
        "\n",
        "3.  $y_i$는 수학점수를 의미하고 \\$\\$는 초기 예측값입니다.\n",
        "\n",
        "    -   $y_1$ = 90, $y_2$ = 65, $y_3$ = 60\n",
        "\n",
        "4.  여기서의 Loss Function\n",
        "    $L(y_i, \\gamma) = \\displaystyle\\frac1n \\sum_{i=1}^{n} (y_i - \\gamma)^2$입니다.\n",
        "    (잘 모르시는 분들은 MSE 학습하시면 좋을 것 같습니다.)\n",
        "\n",
        "5.  예시의 수치를 대입하면  \n",
        "    $L(y_i, \\gamma) = \\displaystyle\\frac13 \\sum_{i=1}^{3} (y_i - \\gamma)^2 =  \\displaystyle\\frac13(90 - \\gamma)^2 + \\displaystyle\\frac13(65 - \\gamma)^2 + \\displaystyle\\frac13(55 - \\gamma)^2$입니다.\n",
        "\n",
        "6.  $L(y_i, \\gamma)$가 최소가 되는 $\\gamma$를 찾아야 하므로\n",
        "    $\\displaystyle\\frac{\\partial L}{\\partial \\gamma} = \\displaystyle\\frac23(\\gamma - 90) + \\displaystyle\\frac23(\\gamma - 65) + \\displaystyle\\frac23(\\gamma - 55) = \\displaystyle\\frac23(3\\gamma - 210)$입니다.\n",
        "\n",
        "7.  $\\displaystyle\\frac{\\partial L}{\\partial \\gamma} = 0$이 되어야\n",
        "    하므로 $\\gamma$ = 70입니다.\n",
        "\n",
        "#### **2단계**\n",
        "\n",
        "1.  **For $m$ = 1 to $M$**:\n",
        "    1.  Compute so-called pseudo-residuals\n",
        "        $r_{im} = - \\Bigg[\\displaystyle\\frac{\\partial L(y_i, F(x_i))}{\\partial F(x_i)}\\Bigg]_{F(x) = F_{m-1}(x)}$\n",
        "        for $i = 1, ... ,n$\n",
        "    2.  Fit a base learner (or weak learner, e.g. tree) closed under\n",
        "        scaling $h_m(x)$ to pseuo-residuals, i.e. train it using the\n",
        "        training set $\\{(x_i, r_{im})\\}_{i=1}^{n}$\n",
        "\n",
        "$r_{im}$은 가짜 잔차(pseudo-residuals)라고 부릅니다.\n",
        "\n",
        "$L(y_i, F(x_i)) = \\displaystyle\\frac1n \\sum_{i=1}^{n}(y_i - F(x_i))^2$이므로  \n",
        "$r_{im} = -\\Bigg[\\displaystyle\\frac{\\partial L(y_i, F(x_i))}{\\partial F(x_i)} \\Bigg]_{F(x)=F_{m-1}(x)} =\\displaystyle\\frac2n\\sum_{i=1}^{n}(y_i - F_{m-1}(x_i))$입니다.\n",
        "정리하면,  \n",
        "$r_{im} = \\displaystyle\\frac2n\\sum_{i=1}^{n}(y_i - F_{m-1}(x_i))$입니다.\n",
        "\n",
        "1.  여기서 $M$은 예측기의 개수, $n$은 데이터의 개수입니다.\n",
        "2.  For $m$ = 1 to $M$의 의미는 M개의 예측기에 대해서 특정 작업을\n",
        "    수행한다는 의미입니다.\n",
        "3.  for $i$ = $1, ... ,n$은 $n$개의 설명변수 $x$에 대해서 $x_1$부터\n",
        "    $x_n$까지 특정 작업을 수행한다는 의미입니다.\n",
        "4.  $r_{im}$은 가짜 잔차(pseudo-residuals)라고 부릅니다.\n",
        "5.  $(m-1)$번째 학습한 모델은 $F_{m-1}(x)$입니다. 이떄의 Loss Function을\n",
        "    $F_{m-1}(x)$로 미분한 것이 $-r_{im}$입니다.\n",
        "6.  식을 확인해보면 실제 잔차$(y_i - F_{m-1}(x))$와 유사해서 가짜 잔차로\n",
        "    불립니다.\n",
        "7.  이때 $h_{m}(x)$는 가짜 잔차를 학습합니다.\n",
        "8.  즉, $h_{m}(x) = \\widehat{y-F_{m-1}(x)}$\n",
        "\n",
        "#### **3단계**\n",
        "\n",
        "1.  **For m = 1 to M**:  \n",
        "    C. Compute multiplier $\\gamma_{m}$ by solving the following\n",
        "    one-dimensional optimization problem:  \n",
        "    $\\gamma_m = \\underset{\\gamma}{\\arg\\min}\\sum_{i=1}^n L(y_i, F_{m-1}(x_i) + \\gamma h_m(x_i))$"
      ],
      "id": "f6d5c0c1-7c45-49e6-9401-c8483e484021"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "codemirror_mode": {
        "name": "ipython",
        "version": "3"
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    }
  }
}