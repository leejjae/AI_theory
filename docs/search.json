[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/ANN(3) - Overfitting.html",
    "href": "posts/ANN(3) - Overfitting.html",
    "title": "ANN(3) - Overfitting",
    "section": "",
    "text": "import matplotlib.pyplot as plt\nimport numpy as np\nimport tensorflow as tf\nimport tensorflow_datasets as tfds\n\nfrom keras import layers\nimport keras\n\n\n# (train_ds, val_ds, test_ds), metadata = tfds.load(\n#     'cats_vs_dogs',\n#     split=['train[:80%]', 'train[80%:90%]', 'train[90%:]'],\n#     with_info = True,\n#     as_supervised=True,\n# )\n\n\nnum_classes = metadata.features['label'].num_classes\nprint(num_classes)\n\n2\n\n\n\nget_label_name = metadata.features['label'].int2str\ntrain_iter = iter(train_ds)\nfig = plt.figure(figsize=(7,8))\n\nfor x in range(4):\n    image, label = next(train_iter)\n    fig.add_subplot(1, 4, x+1)\n    plt.imshow(image)\n    plt.axis('off')\n    plt.title(get_label_name(label))\n\n\n\n\n\n\n\n\n\n\n\nIMG_SIZE = 180\nresize_and_rescale = keras.Sequential([\n    layers.Resizing(IMG_SIZE, IMG_SIZE),\n    layers.Rescaling(1./255)\n])\n\nresult = resize_and_rescale(image)\nplt.axis('off')\nplt.imshow(result);\n\n\n\n\n\n\n\n\n\n\n\n\ndata_augmentation = keras.Sequential([\n    layers.RandomFlip(\"horizontal_and_vertical\"),\n    layers.RandomRotation(0.4),\n])\n\nplt.figure(figsize=(8,7))\nfor i in range(6):\n    augmented_image = data_augmentation(image)\n    ax = plt.subplot(2, 3, i+1)\n    plt.imshow(augmented_image.numpy()/255)\n    plt.axis('off')\n\n\n\n\n\n\n\n\n\nimage, label = next(iter(train_ds))\nplt.imshow(image)\nplt.title(get_label_name(label));\n\n\n\n\n\n\n\n\n\ndef visualize(original, augmented):\n    fig = plt.figure()\n    plt.subplot(1,2,1)\n    plt.title('Original image')\n    plt.imshow(original)\n    plt.axis('off')\n\n    plt.subplot(1,2,2)\n    plt.title('Augmented image')\n    plt.imshow(augmented)\n    plt.axis('off')\n\n\n\n\n\nflipped = tf.image.flip_left_right(image)\nvisualize(image, flipped)\n\n\n\n\n\n\n\n\n\n\n\n\ngrayscale = tf.image.rgb_to_grayscale(image)\nvisualize(image, tf.squeeze(grayscale))\n\n\n\n\n\n\n\n\n\n\n\n\nsaturated = tf.image.adjust_saturation(image, 3)\nvisualize(image, saturated)\n\n\n\n\n\n\n\n\n\n\n\n\nbright = tf.image.adjust_brightness(image, 0.4)\nvisualize(image, bright)\n\n\n\n\n\n\n\n\n\n\n\n\ncropped = tf.image.central_crop(image, central_fraction=0.5)\nvisualize(image, cropped)\n\n\n\n\n\n\n\n\n\n\n\n\nrotated = tf.image.rot90(image)\nvisualize(image, rotated)\n\n\n\n\n\n\n\n\n\n\n\n\nfor i in range(3):\n  seed = (i, 0)  # tuple of size (2,)\n  stateless_random_brightness = tf.image.stateless_random_brightness(\n      image, max_delta=0.95, seed=seed)\n  visualize(image, stateless_random_brightness)"
  },
  {
    "objectID": "posts/ANN(3) - Overfitting.html#data-augmentation",
    "href": "posts/ANN(3) - Overfitting.html#data-augmentation",
    "title": "ANN(3) - Overfitting",
    "section": "",
    "text": "import matplotlib.pyplot as plt\nimport numpy as np\nimport tensorflow as tf\nimport tensorflow_datasets as tfds\n\nfrom keras import layers\nimport keras\n\n\n# (train_ds, val_ds, test_ds), metadata = tfds.load(\n#     'cats_vs_dogs',\n#     split=['train[:80%]', 'train[80%:90%]', 'train[90%:]'],\n#     with_info = True,\n#     as_supervised=True,\n# )\n\n\nnum_classes = metadata.features['label'].num_classes\nprint(num_classes)\n\n2\n\n\n\nget_label_name = metadata.features['label'].int2str\ntrain_iter = iter(train_ds)\nfig = plt.figure(figsize=(7,8))\n\nfor x in range(4):\n    image, label = next(train_iter)\n    fig.add_subplot(1, 4, x+1)\n    plt.imshow(image)\n    plt.axis('off')\n    plt.title(get_label_name(label))\n\n\n\n\n\n\n\n\n\n\n\nIMG_SIZE = 180\nresize_and_rescale = keras.Sequential([\n    layers.Resizing(IMG_SIZE, IMG_SIZE),\n    layers.Rescaling(1./255)\n])\n\nresult = resize_and_rescale(image)\nplt.axis('off')\nplt.imshow(result);\n\n\n\n\n\n\n\n\n\n\n\n\ndata_augmentation = keras.Sequential([\n    layers.RandomFlip(\"horizontal_and_vertical\"),\n    layers.RandomRotation(0.4),\n])\n\nplt.figure(figsize=(8,7))\nfor i in range(6):\n    augmented_image = data_augmentation(image)\n    ax = plt.subplot(2, 3, i+1)\n    plt.imshow(augmented_image.numpy()/255)\n    plt.axis('off')\n\n\n\n\n\n\n\n\n\nimage, label = next(iter(train_ds))\nplt.imshow(image)\nplt.title(get_label_name(label));\n\n\n\n\n\n\n\n\n\ndef visualize(original, augmented):\n    fig = plt.figure()\n    plt.subplot(1,2,1)\n    plt.title('Original image')\n    plt.imshow(original)\n    plt.axis('off')\n\n    plt.subplot(1,2,2)\n    plt.title('Augmented image')\n    plt.imshow(augmented)\n    plt.axis('off')\n\n\n\n\n\nflipped = tf.image.flip_left_right(image)\nvisualize(image, flipped)\n\n\n\n\n\n\n\n\n\n\n\n\ngrayscale = tf.image.rgb_to_grayscale(image)\nvisualize(image, tf.squeeze(grayscale))\n\n\n\n\n\n\n\n\n\n\n\n\nsaturated = tf.image.adjust_saturation(image, 3)\nvisualize(image, saturated)\n\n\n\n\n\n\n\n\n\n\n\n\nbright = tf.image.adjust_brightness(image, 0.4)\nvisualize(image, bright)\n\n\n\n\n\n\n\n\n\n\n\n\ncropped = tf.image.central_crop(image, central_fraction=0.5)\nvisualize(image, cropped)\n\n\n\n\n\n\n\n\n\n\n\n\nrotated = tf.image.rot90(image)\nvisualize(image, rotated)\n\n\n\n\n\n\n\n\n\n\n\n\nfor i in range(3):\n  seed = (i, 0)  # tuple of size (2,)\n  stateless_random_brightness = tf.image.stateless_random_brightness(\n      image, max_delta=0.95, seed=seed)\n  visualize(image, stateless_random_brightness)"
  },
  {
    "objectID": "posts/ANN(3) - Overfitting.html#dropout",
    "href": "posts/ANN(3) - Overfitting.html#dropout",
    "title": "ANN(3) - Overfitting",
    "section": "Dropout",
    "text": "Dropout\n\nFigure1\n\n\n(a)는 Standard Neural Net(FC layer)으로, 모든 노드가 연결되어 있습니다.\n(b)는 Dropout을 적용한 Neural Net으로, 무작위로 노드가 끊어져있습니다.\n\\(\\rightarrow\\) 무작위로 노드를 끊음으로써 과적합 방지를 할 수 있습니다.\nMini-batch학습을 시킬 때마다 활용되는 node들의 평균을 이용합니다.\n\n\n\nFigure2\n\n\n그렇다면 test시 노드들은 어떻게 적용이 될까요?\ntrain에서는 확률 \\(p\\)에 의해 노드에 연결될 수도, 안될 수도 있습니다.\ntest에서는 각 노드의 가중치에 확률을 곱한 \\(pw\\)를 가중치로 사용합니다.\n(이 부분이 이해가 잘…)\n\n\n\nFigure3\n\n\n레이어 : \\(l \\in \\left\\{1,, ... , L\\right\\}\\)\n\\(z^{(l)}\\) : \\(l\\)의 출력 벡터\n\\(y^{(l)}\\) : \\(l\\)의 입력 벡터\n\\(W^{(l)}\\) : \\(l\\)의 가중치(weight)행렬\n\\(b^{(l)}\\) : \\(l\\)의 편향(bias) 벡터\nForward Propagation\n\\(z_i^{(l+1)} = w_i^{(l+1)}y^l + b_i^{(l+1)}\\) , \\(y_i^{(l+1)} = f(z_i^{(l+1)})\\) (\\(f\\) : Activation Function)\n\\(r_j^{(l)} \\sim Bernoulli(p)\\)\n\\(\\tilde{y}^{(l)} = r^{(l)}y^{(l)}\\)\n\\(z_i^{(l+1)} = w_i^{(l+1)}\\tilde{y}^l + b_i^{(l+1)}\\)\n\\(z_i^{(l+1)} = w_i^{(l+1)}y^l + b_i^{(l+1)}\\)"
  },
  {
    "objectID": "posts/ANN(3) - Overfitting.html#dropconnect",
    "href": "posts/ANN(3) - Overfitting.html#dropconnect",
    "title": "ANN(3) - Overfitting",
    "section": "Dropconnect",
    "text": "Dropconnect\n\n\nDropout의 일반화\nDropout은 각 layer의 node 중 일부 node의 activation을 0으로 만들어서 모델을 경량화 시켰다면, Dropconnect는 가중치(weight)를 0으로 만들어 모델을 경량화 시킵니다.\n\n\nMotivation\n\ninput : \\(v = [v_1, v_2, ... , v_n]^T\\)\noutput : \\(r = [r_1, r_2, ... , r_d]^T\\)\nweight : \\(W\\)(of size \\(d\\) X \\(n\\))\n\\(r = a(u) = a(Wv)\\)"
  },
  {
    "objectID": "posts/ANN(3) - Overfitting.html#regularization",
    "href": "posts/ANN(3) - Overfitting.html#regularization",
    "title": "ANN(3) - Overfitting",
    "section": "Regularization",
    "text": "Regularization\n\nMSE\n\\(\\begin{align}\n    E(MSE) & =E\\left[(Y-\\hat{Y})^2|X\\right]\\\\\n        & =E\\left(Y^2-2Y\\hat{Y}+\\hat{Y}^2 |X\\right)\\\\\n        & = \\sigma^2  + \\left\\{E(\\hat{Y}) - \\hat{Y} \\right\\}^2\\\\\n        & = \\sigma^2 + \\text{Bias}\\left(\\hat{Y}\\right) + \\text{Var}(\\hat{Y})\n\\end{align}\\)\n\\(\\hat{\\beta}^{\\text{LS}} = \\underset{\\beta}\\arg\\min \\left\\{\\displaystyle\\sum_{i=1}^{n} (y-x_i\\beta)^2\\right\\}\\)\n\\(\\Rightarrow \\hat{\\beta}^{\\text{LS}} : \\text{unbiased estimator}\\)\n\\(\\Rightarrow\\) 아이디어 : Bias가 올라가더라도 Variance를 더 낮출 수 있지 않을까?\n\\[\\hat{y} = \\beta_1x_1 + \\beta_2x_2 + ... + \\beta_{p}x_p\\] \\[\\text{or}\\] \\[\\hat{y} = \\beta_1x + \\beta_2x^2 + ... + \\beta_{p}x^p\\]\n\n\nRidge\n\\(L(\\beta)=\\underset{\\beta}\\min \\displaystyle\\sum_{i=1}^{n} (y_i-\\hat{y}_i)^2 + \\lambda \\displaystyle\\sum_{j=1}^{p} \\beta_j^2 (\\lambda : \\text{hyper parameter})\\)\n- if) \\(\\lambda = \\infty\\)\n\\(\\Rightarrow \\beta_k \\approx 0(1 \\leqq k \\leqq p)\\)\n\\(\\Rightarrow \\text{Underfitting}\\)\n- if) \\(\\lambda=0\\)\n\\(\\Rightarrow \\text{Overfitting(LSE)}\\)\n\n\nLasso\n\\(L(\\beta) = \\underset{\\beta}\\arg\\min \\left\\{\\displaystyle\\sum_{i=1}^{n}(y_i-\\hat{y})^2+\\lambda\\displaystyle\\sum_{j=1}^{p}\\vert \\beta_j\\vert\\right\\}\\)\n- if) \\(\\lambda = \\infty\\)\n\\(\\Rightarrow \\beta_k \\approx 0(1 \\leqq k \\leqq p)\\)\n\\(\\Rightarrow \\text{Underfitting}\\)\n- if) \\(\\lambda=0\\)\n\\(\\Rightarrow \\text{Overfitting(LSE)}\\)\n\n\nVisualization\n\\(\\begin{align}\n    MSE(\\beta_1, \\beta_2) & = \\sum_{i=1}^{n} (y_i-\\beta_1x_{i1}-\\beta_2x_{i2})^2 \\\\\n    & = \\sum_{i=1}^{n} y_i^2 - 2\\sum_{i=1}^{n}y_i(\\beta_1x_{i1}+\\beta_2x_{i2}) + \\sum_{i=1}^{n}(\\beta_1x_{i1} + \\beta_2x_{i2})^2 \\\\\n    & = \\sum_{i=1}^{n} y_i^2 - 2(\\sum_{i=1}^{n} y_ix_{i1}) - 2(\\sum_{i=1}^{n} y_ix_{i2}) + \\sum_{i=1}^{n}(\\beta_1^2x_{i1}^2 + \\beta_2^2x_{i2}^2 + 2\\beta_1\\beta_2x_{i1}x_{i2}) \\\\\n    & = (\\sum_{i=1}^{n} x_{i1})^2\\beta_1^2+(\\sum_{i=1}^{n} x_{i2})^2\\beta_2^2 + 2(\\sum_{i=1}^{n} x_{i1}x_{i2})\\beta_1\\beta_2-2(\\sum_{i=1}^{n}y_ix_{i1})\\beta_1-2(\\sum_{i=1}^{n}y_ix_{i2})\\beta_2 + \\sum_{i=1}^n y_i^2 \\\\\n    & = A\\beta_1^2 + B\\beta_1\\beta_2 + C\\beta_2^2 + D\\beta_1 + E\\beta_2 + F\n\\end{align}\\)\n\n\n판별식\n\n\\(B^2-4AC = 0\\) : 포물선\n\\(B^2-4AC &gt; 0\\) : 쌍곡선\n\\(B^2-4AC &lt; 0\\) : 타원\n\\(B=0, C=0\\) : 원\n\n\\(\\begin{align}\n    B^2-4AC & = \\Big(2\\sum_{i=1}^{n}x_{i1}x_{i2}\\Big)^2 - 4\\Big(\\sum_{i=1}^{n} x_{i1}^2\\Big)\\Big(\\sum_{i=1}^{n} x_{i2}^2\\Big) \\\\\n    & = 4\\left\\{\\Big(\\sum_{i=1}^{n} x_{i1}x_{i2}\\Big)^2-\\Big(\\sum_{i=1}^{n} x_{i1}^2\\Big)\\Big(\\sum_{i=1}^{n} x_{i2}^2\\Big)\\right\\} &lt; 0\n\\end{align}\\)\nBy Cauchy - Shwartz inequality\n 출처:https://stats.stackexchange.com/questions/587492/geometrical-interpretation-of-why-cant-ridge-regression-shrink-coefficients-to\n\n\\(\\hat{\\beta}^{\\text{LSE}}\\)가 규제하는 범위 안에 들어와 있다면 \\(\\hat{\\beta}^{\\text{LSE}}\\) = \\(\\hat{\\beta}^{\\text{Ridge}}\\) or \\(\\hat{\\beta}^{\\text{LSE}}\\) = \\(\\hat{\\beta}^{\\text{Lasso}}\\)"
  },
  {
    "objectID": "posts/CNN.html",
    "href": "posts/CNN.html",
    "title": "CNN(Convolutional Neural Network)",
    "section": "",
    "text": "(사진 0) https://www.ml-science.com/convolutional-neural-networks\n\n\n앞 부분이 Feature Extractor, 뒷 부분이 FC layer이다.\nFeature Extractor에서는 말 그대로 사진의 특징을 추출하는 부분이다.\nFC layer에서는 추출된 특징을 가지고 학습하는 부분이다."
  },
  {
    "objectID": "posts/CNN.html#cnns-structure",
    "href": "posts/CNN.html#cnns-structure",
    "title": "CNN(Convolutional Neural Network)",
    "section": "",
    "text": "(사진 0) https://www.ml-science.com/convolutional-neural-networks\n\n\n앞 부분이 Feature Extractor, 뒷 부분이 FC layer이다.\nFeature Extractor에서는 말 그대로 사진의 특징을 추출하는 부분이다.\nFC layer에서는 추출된 특징을 가지고 학습하는 부분이다."
  },
  {
    "objectID": "posts/CNN.html#feature-extractor",
    "href": "posts/CNN.html#feature-extractor",
    "title": "CNN(Convolutional Neural Network)",
    "section": "Feature Extractor",
    "text": "Feature Extractor\n\n(사진 1) https://d2l.ai/chapter_convolutional-neural-networks/lenet.html\n\n다음은 LeNet Architecture의 구조이다.\n28 x 28 imgae의 사진에서 feature를 추출하는 모습을 대략적을 보여주고 있다.\nFeature Extractor는 CNN에서 메인이 되는 부분이다.\n\n\n1. Convolution\n\n(사진 1)의 28x28 image에 보이는 파란색 사각형이 이미지를 훑으면서 Feature를 추출하고 있다.\n파란색 사각형은 Convolutional filter(Conv filter), kernel 등으로 불린다.\nkernel의 사이즈는 3x3, 5x5, 7x7, 등 다양하며, 경우에 따라 사용되는 사이즈가 다르다.\n\n\n- Striding\n\nkernel이 image를 훑는 과정에서 몇칸씩 이동할 것인지 정하는 개념이다.\n예를 들어 striding=1이라면, kernel은 한칸씩, striding=2라면, kernel은 두칸씩 이동한다.\n다음은 Striding=1인 kernel이 image를 훑는 과정을 시각화 한 것이다.\n\n\n(사진2) https://stats.stackexchange.com/questions/296679/what-does-kernel-size-mean/296701\n\n사진(2)에서는 3x3 kernel이 쓰였다.\nkernel이 이동할 때 마다, kernel에 적혀진 숫자와 image의 pixel을 곱하고 더한 뒤에(합성곱을 해준 뒤에), 한 pixel에 나타낸다.(9개의 pixel \\(\\rightarrow\\) 1개의 pixel로 변환)\n\n\n(사진 2.5) https://jeiwan.net/posts/til-convolution-filters-are-weights/\n\nConv filter 의 간단한 예시와 결과이다.\n\n\n\n- Padding\n\n9개의 pixel이 1개의 pixel로 축소됨에 따라 원래 이미지의 사이즈는 줄어들 수 밖에 없다.\n사이즈 축소를 막기 위해 사용된 것이 padding으로 (사진 2)에서 보이는 것 처럼, 화소 0값으로 pixel들을 둘러 쌓고 있는 것을 말한다.\n\n\n(사진 1) https://d2l.ai/chapter_convolutional-neural-networks/lenet.html\n\n(사진 1)을 다시 살펴보자. C1 feature map에는 6개의 feature맵이 생성되었다.\n6개의 kernel이 사용되었다는 말과 동치이다.\nC1 feature map \\(\\rightarrow\\) S2 featuremap 과정을 살펴보면, feature map 크기가 줄었다.\n이는 feature map의 size는 padding으로 인해 유지하였지만, 0이 아닌 pixel의 수는 줄어들었기 때문이다.\nPooling 때문이다.\n\n\n\n- Pooling\n\n(사진 3) https://paperswithcode.com/method/max-pooling\n\n(사진 3)은 2x2 Max Pooling을 시각화 한 것이다.(Max 이외에도 Average, Min Pooling등이 가능하다.)\nimage를 2x2로 쪼갠 뒤에, 그 중에서 가장 큰 값만 추출하는 것이다.\nMax Pooling을 사용하는 이유는, 주변 pixel 중에서 최대값만 추출하여(특징이 가장 잘 나타나는 pixel만 추출하여), 좀 더 세부적인 특징을 추출하기 위함이다.\nMax Pooling을 하게 되면 0이 아닌 pixel의 수가 줄면서(=특징이 가장 잘 나타나는 pixel만 남으면서) 좀 더 자세한 특징들을 추출할 수 있다.\n즉, 적어진 pixel들을 같은 size의 kernel이 지나가면서 더 자세한 특징들을 추출할 수 있게 된다.\n\n\n\n- Generalization\n\n(사진 4)\n\n(사진 5)\n(사진 4)\n\n앞서 설명했던 내용을 그림으로 나타내면 다음과 같다.\n컬러 이미지 \\(X\\)는 첫번째 kernel을 통과하는데, 이때 kernel의 개수는 \\(l_1\\)개이고, kernel의 사이즈는 \\(f^{[1]}\\) x \\(f^{[1]}\\)이다.\n\\(k_{i}^{[1]}\\) 커널을 지나간 이미지는 \\(A_i^{[1]}\\)로 표현된다. (\\(A_i^{[1]} \\in R^{n^{[1]} \\,\\text{x} \\,n^{[1]}}\\))\n\n(사진 5)\n\n(사진 4)과정을 계속 이어나가는 것을 시각화한 것이다.\nkernel을 지나가는 과정에서 padding과 striding=1이 적용되었다.\n\n\n\n- Visualization\n\n(사진 4)와 (사진 5)를 시각화한 것이다.\n\n\nhttps://stats.stackexchange.com/questions/296679/what-does-kernel-size-mean/296701"
  },
  {
    "objectID": "posts/ANN(2).html",
    "href": "posts/ANN(2).html",
    "title": "ANN(2) - Vanishing Gradient & Optimization",
    "section": "",
    "text": "\\[ Sigmoid(x) = \\displaystyle\\frac{1}{1+e^{-x}} \\Rightarrow \\displaystyle\\frac{d}{dx}Sigmoid(x) = \\frac{e^{-x}}{(1+e^{-x})^2}\\]\n\n\n\nsigmoid.png\n\n\n\nGradient Descent를 하다보면 Activation Function도 미분을 해야되는데 Sigmoid의 최대값은 \\(\\displaystyle\\frac14\\), 그외의 값은 거의 0에 수렴한다.\nChain Rule을 적용해야하는 입장에서 보면, Sigmoid는 엄청난 기울기 손실, Vanishing Gradient 문제를 가져온다. Chain Rule : \\(\\displaystyle\\frac{\\partial L}{\\partial x} = \\displaystyle\\frac{\\partial L}{\\partial a} \\displaystyle\\frac{\\partial a}{\\partial z}\\displaystyle\\frac{\\partial z}{\\partial x} \\;\\;\\;\\;\\text{if)}\\displaystyle\\frac{\\partial a}{\\partial z}\\approx 0\\)\n\n\n\n\n\\[tanh(x) = \\displaystyle\\frac{e^{x} - e^{-x}}{e^{x}+e^{-x}} \\Rightarrow \\displaystyle\\frac{d}{dx} tanh(x) = \\displaystyle\\frac{4}{(e^x+e^{-x})^2}\\]\n\n\n\ntanh.png\n\n\n\n또 하나의 Activation Function인 tanh도 마찬가지로 최대값은 1이고 그 외의 값은 0에 수렴한다.\nSigmoid와 같이 Vanishing Gradient 문제가 발생한다.\n\n\n\n\n\n\n\nReLU.png\n\n\n\nReLU의 경우에는 \\(x \\geq 0\\)인 상황에서 기울기는 항상 1이므로 Vanishing Gradient 문제에 대한 돌파구가 될 수 있다.\n\n\n\n\n\n\n\nLeaky ReLU.png\n\n\n\nLeaky ReLU는 \\(x \\leq 0\\)인 상황에서의 Vanishing Gradient 문제를 해결하고자 만들어진 대안책이다."
  },
  {
    "objectID": "posts/ANN(2).html#vanishing-gradient",
    "href": "posts/ANN(2).html#vanishing-gradient",
    "title": "ANN(2) - Vanishing Gradient & Optimization",
    "section": "",
    "text": "\\[ Sigmoid(x) = \\displaystyle\\frac{1}{1+e^{-x}} \\Rightarrow \\displaystyle\\frac{d}{dx}Sigmoid(x) = \\frac{e^{-x}}{(1+e^{-x})^2}\\]\n\n\n\nsigmoid.png\n\n\n\nGradient Descent를 하다보면 Activation Function도 미분을 해야되는데 Sigmoid의 최대값은 \\(\\displaystyle\\frac14\\), 그외의 값은 거의 0에 수렴한다.\nChain Rule을 적용해야하는 입장에서 보면, Sigmoid는 엄청난 기울기 손실, Vanishing Gradient 문제를 가져온다. Chain Rule : \\(\\displaystyle\\frac{\\partial L}{\\partial x} = \\displaystyle\\frac{\\partial L}{\\partial a} \\displaystyle\\frac{\\partial a}{\\partial z}\\displaystyle\\frac{\\partial z}{\\partial x} \\;\\;\\;\\;\\text{if)}\\displaystyle\\frac{\\partial a}{\\partial z}\\approx 0\\)\n\n\n\n\n\\[tanh(x) = \\displaystyle\\frac{e^{x} - e^{-x}}{e^{x}+e^{-x}} \\Rightarrow \\displaystyle\\frac{d}{dx} tanh(x) = \\displaystyle\\frac{4}{(e^x+e^{-x})^2}\\]\n\n\n\ntanh.png\n\n\n\n또 하나의 Activation Function인 tanh도 마찬가지로 최대값은 1이고 그 외의 값은 0에 수렴한다.\nSigmoid와 같이 Vanishing Gradient 문제가 발생한다.\n\n\n\n\n\n\n\nReLU.png\n\n\n\nReLU의 경우에는 \\(x \\geq 0\\)인 상황에서 기울기는 항상 1이므로 Vanishing Gradient 문제에 대한 돌파구가 될 수 있다.\n\n\n\n\n\n\n\nLeaky ReLU.png\n\n\n\nLeaky ReLU는 \\(x \\leq 0\\)인 상황에서의 Vanishing Gradient 문제를 해결하고자 만들어진 대안책이다."
  },
  {
    "objectID": "posts/ANN(2).html#optimization",
    "href": "posts/ANN(2).html#optimization",
    "title": "ANN(2) - Vanishing Gradient & Optimization",
    "section": "Optimization",
    "text": "Optimization\n\nGradient descent\n\\(W = W - \\alpha\\displaystyle\\frac{\\partial}{\\partial W} L(W,b)\\)\n\\(b = b - \\alpha\\displaystyle\\frac{\\partial}{\\partial b} L(W,b)\\)\n\\(\\alpha\\) : learning rate & hypyer parameter\n\n\nSGD(Stochastic Gradient Descent)\n\\(W = W - \\alpha\\displaystyle\\frac{\\partial}{\\partial W} L(W,b\\,;\\,x^{(i)}, y^{(i)})\\)\n\\(b = b - \\alpha\\displaystyle\\frac{\\partial}{\\partial b} L(W,b)\\,;\\,x^{(i)}, y^{(i)})\\)\n\n\nMomentum\n\n\n\n.\n\n\n\n물리에서 사용되는 운동량의 개념에서 시작됨.\n공이 내리막길을 다 내려가도 앞으로 나아가려는 성질을 이용함.\n즉, local minima 문제를 해결할 수 있는 아이디어\n\n\\(Vdw := \\beta_1\\cdot V  dw + (1-\\beta_1) \\cdot dw \\left(dw = \\displaystyle\\frac{d}{dw}  L(w,b) \\right)\\)\n\\(Vdb := \\beta_1\\cdot V  db + (1-\\beta_1) \\cdot db \\left(db = \\displaystyle\\frac{d}{db}  L(w,b) \\right)\\)\n\\(\\beta_1\\)는 RMSprop에서 사용되는 \\(\\beta_2\\) 와 구분되기 위한 첨자임.\n\\(W := W - \\alpha \\cdot Vdw\\)\n\\(b := b - \\alpha \\cdot Vdb\\)\n\n\\(\\beta\\) 역시 \\(\\alpha\\)(learning rate)처럼 하이퍼 파라미터이다. \\(\\rightarrow\\) 사람이 직접 정해줘야 하는 파라미터\n경험적으로 \\(\\beta\\)는 0.9로 많이 쓰인다. 식으로 살펴보자\n$Vdw := 0.9V dw + 0.1 dw \\(\\\n\\)$ 이는 \\(dw\\)를 10으로 나눈 것으로 과거 기울기 10개의 평균치를 보겠다는 것과 같은 의미이다.\n과거 기울기가 클수록 \\(Vdw\\)는 커지게 되고 이는 다음 \\(W\\)를 update하는데 영향을 준다. \\(\\rightarrow\\) 기울기가 클수록 공이 다음 언덕으로 올라갈 확률이 높아진다.\n\n\n\nRMSprop (Root Mean Square probability)\n\\(Sdw = \\beta_2\\cdot V  dw + (1-\\beta_2) \\cdot (dw)^2 \\left(dw = \\displaystyle\\frac{d}{dw}  L(w,b) \\right)\\)\n\\(Sdb = \\beta_2\\cdot V  db + (1-\\beta_2) \\cdot (db)^2 \\left(db = \\displaystyle\\frac{d}{db}  L(w,b) \\right)\\)\n\\(\\beta_2\\)는 Momentum에서 사용되는 \\(\\beta_1\\) 와 구분되기 위한 첨자임.\n\\(W  := W - \\alpha \\cdot \\displaystyle\\frac{dw}{\\sqrt{Sdw}} + \\epsilon\\)\n\\(b  := b - \\alpha \\cdot \\displaystyle\\frac{dw}{\\sqrt{Sdb}} + \\epsilon\\)\n\nMomentum과 비슷한 수식을 가지고 있지만, RMSprob에서는 \\(dw\\)가 제곱의 형태임. \\((dw) \\rightarrow (dw)^2\\)\n\\(\\beta_2\\)역시 경험적으로 0.9가 많이 사용되는데, 이는 최근 10개의 기울기에 대해 가중치를 두어서 parameter를 업데이트 하겠다는 의미이다.\n\n\n\nAdam(Adaptive Momentum Estimation) : RMSprop + Momentum\n\\(m_0 = 0 \\, , \\, v_0 = 0\\)\n\\(m_{t+1} := \\beta_1 \\cdot m_t + (1-\\beta_1) \\cdot \\displaystyle\\frac{\\partial L}{\\partial \\theta}\\)\n$v_{t+1} := _2 v_m + (1-_2) ()^2 $\n\\(\\theta := \\theta - \\displaystyle\\frac{\\epsilon}{\\sqrt{v_t} + e^{-5}}\\)"
  },
  {
    "objectID": "posts/JBIG코드실습_타이타닉1.html",
    "href": "posts/JBIG코드실습_타이타닉1.html",
    "title": "JBIG코드실습_타이타닉(1주차)",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\n\n\ntrain = pd.read_csv('./train_1.csv')\ntrain.head()\nset(train['Pclass'])\n\n{1, 2, 3}\n\n\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier\nimport lightgbm as lgb\nfrom sklearn.metrics import accuracy_score\n\n# 데이터 불러오기\ntrain = pd.read_csv('./train_1.csv')\n\n# 전처리 (필요시 전처리 추가할 것)#########################################################################\ntrain['Age'] = train['Age'].fillna(29)\ntrain['Sex'] = train['Sex'].map({'male': 1, 'female': 0})\n# 위 5개 칼럼만 사용할 것\ntrain = train[[ 'Pclass', 'SibSp', 'Age', 'Sex', 'Survived']] \n\n# X값과 y값 구하기\nX = train.drop(columns=['Survived'])\ny = train['Survived']\n\n# 데이터 세트 분리\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n\n# 랜덤포레스트를 정의 및 학습하고 Accuracy로 평가해보세요\n\n# 랜덤 포레스트 정의\nrf_classifier = RandomForestClassifier(random_state=42)\n\n# 랜덤 포레스트 학습\nrf_classifier.fit(X_train, y_train)\n\n# 테스트 데이터에 대한 예측\ny_pred_rf = rf_classifier.predict(X_test)\n\n# 정확도 평가\naccuracy_rf = accuracy_score(y_test, y_pred_rf)\nprint(\"Random Forest의 정확도:\", accuracy_rf)\n\nRandom Forest의 정확도: 0.8268156424581006\n\n\n\n# LightGBM를 정의 및 학습하고 Accuracy로 평가해보세요\n\n# LightGBM 모델 정의\nlgb_classifier = lgb.LGBMClassifier(random_state=42)\n\n# LightGBM 모델 학습\nlgb_classifier.fit(X_train, y_train)\n\n# 테스트 데이터에 대한 예측\ny_pred_lgb = lgb_classifier.predict(X_test)\n\n# 정확도 평가\naccuracy_lgb = accuracy_score(y_test, y_pred_lgb)\nprint(\"LightGBM의 정확도:\", accuracy_lgb)\n\nLightGBM의 정확도: 0.8100558659217877\n\n\n\n# 둘 중 더 높은 Accuracy가 나온 모델의 하이퍼 파라미터 default값을 '구글링'을 통해 확인하고 변경해보세요(Grid_Search말고 직접 바꿀것)\n\n\n# 랜덤 포레스트 정의\nrf_classifier = RandomForestClassifier(\n    n_estimators=13,\n    min_samples_split=5,\n    min_samples_leaf=1,\n    max_features='sqrt',\n    max_depth=7,\n    max_leaf_nodes=10,\n    random_state=42)\n\n# 랜덤 포레스트 학습\nrf_classifier.fit(X_train, y_train)\n\n# 테스트 데이터에 대한 예측\ny_pred_rf = rf_classifier.predict(X_test)\n\n# 정확도 평가\naccuracy_rf = accuracy_score(y_test, y_pred_rf)\nprint(\"Random Forest의 정확도:\", accuracy_rf)\n\nRandom Forest의 정확도: 0.8044692737430168\n\n\n\n# LightGBM를 정의 및 학습하고 Accuracy로 평가해보세요\n\n# LightGBM 모델 정의\nlgb_classifier = lgb.LGBMClassifier(\n    num_iterations=130,\n    learning_rate=0.09,\n    max_depth=6,\n    min_data_in_leaf=30,\n    num_leaves=40,\n    boosting='gbdt',\n    bagging_fraction=1.0,\n    feature_fraction=1.0,\n    random_state=42)\n\n# LightGBM 모델 학습\nlgb_classifier.fit(X_train, y_train)\n\n# 테스트 데이터에 대한 예측\ny_pred_lgb = lgb_classifier.predict(X_test)\n\n# 정확도 평가\naccuracy_lgb = accuracy_score(y_test, y_pred_lgb)\nprint(\"LightGBM의 정확도:\", accuracy_lgb)\n\n/root/anaconda3/envs/ag/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n\n\n[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\nLightGBM의 정확도: 0.8268156424581006\n\n\n\n# VotingClassifier를 통해서 앙상블;보팅 한후 Accuracy를 계산해보세요\n\nfrom sklearn.ensemble import VotingClassifier\n\n# VotingClassifier 정의\nvoting_classifier = VotingClassifier(estimators=[\n    ('random_forest', rf_classifier),\n    ('lightgbm', lgb_classifier)\n])\n\n# VotingClassifier 학습\nvoting_classifier.fit(X_train, y_train)\n\n# 테스트 데이터에 대한 예측\ny_pred_voting = voting_classifier.predict(X_test)\n\n# 정확도 평가\naccuracy_voting = accuracy_score(y_test, y_pred_voting)\nprint(\"VotingClassifier의 정확도:\", accuracy_voting)\n\n/root/anaconda3/envs/ag/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n\n\n[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\nVotingClassifier의 정확도: 0.8100558659217877"
  },
  {
    "objectID": "posts/로지스틱회귀.html",
    "href": "posts/로지스틱회귀.html",
    "title": "로지스틱회귀",
    "section": "",
    "text": "\\(f(x|p) = p^x(1-p)^{1-x} (x=1,0)\\)\n즉 \\(x=1\\)일 확률이 \\(p\\), \\(x=0\\)일 확률이 \\(1-p\\)입니다.\n\n\n\n\n\n베르누이 분포에서 1이 나올 확률 \\(p\\)와 0이 나올 확률 \\(1-p\\)의 비율입니다.\n\\(odds\\, ratio = \\displaystyle\\frac{p}{1-p}    \\left(0\\leq odds\\,ratio\\leq \\infty \\right) = \\displaystyle\\frac{\\text{1일확률}}{\\text{0일확률}}\\)\n확률 \\(p\\)를 \\(odds\\,ratio\\)로 변환하면 0부터 무한대까지의 값을 가질 수 있다.\n승산비(\\(odds\\,ratio\\))를 로그변환하면 다음과 같다.\n\\(z = logit\\left(\\displaystyle\\frac{p}{1-p} \\right) = log\\left(\\displaystyle\\frac{p}{1-p}\\right)\\)\n\n- 식을 다음과 같이 변환해보자\n\\(log\\left(\\displaystyle\\frac{p}{1-p}\\right) = z\\)\n\n\\(\\Leftrightarrow \\displaystyle\\frac{p}{1-p} = e^z\\)\n\n\\(\\Leftrightarrow p = e^z(1-p)\\)\n\n\\(\\Leftrightarrow p = e^z - pe^k\\)\n\n\\(\\Leftrightarrow p(1+e^z) = e^z\\)\n\n\\(\\Leftrightarrow p = \\displaystyle\\frac{e^z}{1+e^z} = \\displaystyle\\frac{1}{1+e^{-z}}\\)\n\n이처럼 \\(logit\\,function\\)의 역함수가 \\(logistic\\, function\\)이다. 따라서\n\\(logistic(z) = \\mu(z) = \\displaystyle\\frac{1}{1+e^{-z}}\\)\n\n- \\(logistic\\,function\\)의 그래프는 다음과 같다.(출처)\n\n\n\nsigmoid\n\n\n\n\n\n- 그러다면 \\(logistic function\\)을 왜 사용하는 것일까?\n 사진출처 : ratsgo’s blog\n\n나이에 따른 질병 유무에 대한 분류문제를 해결해야한다고 가정해보자.\n그림과 같이 분류 문제를 선형 회귀로 해결하려고 하면 그림이 많이 이상해집니다.\n따라서 앞서 언급한 \\(logistic\\,function\\)을 이용하여 문제를 해결합니다. \\(logistic\\,function(z) = \\displaystyle\\frac{1}{1+e^{-z}} = \\begin{cases}1&z&gt;\\frac12\\\\\\frac12&z=\\frac12\\\\0&z&lt;\\frac12\\end{cases}\\)\n\n\n\n\n(추가 업로드 예정)\n\n\n\n(추가 업로드 예정)"
  },
  {
    "objectID": "posts/로지스틱회귀.html#로지스틱-회귀logistic-regression",
    "href": "posts/로지스틱회귀.html#로지스틱-회귀logistic-regression",
    "title": "로지스틱회귀",
    "section": "",
    "text": "\\(f(x|p) = p^x(1-p)^{1-x} (x=1,0)\\)\n즉 \\(x=1\\)일 확률이 \\(p\\), \\(x=0\\)일 확률이 \\(1-p\\)입니다.\n\n\n\n\n\n베르누이 분포에서 1이 나올 확률 \\(p\\)와 0이 나올 확률 \\(1-p\\)의 비율입니다.\n\\(odds\\, ratio = \\displaystyle\\frac{p}{1-p}    \\left(0\\leq odds\\,ratio\\leq \\infty \\right) = \\displaystyle\\frac{\\text{1일확률}}{\\text{0일확률}}\\)\n확률 \\(p\\)를 \\(odds\\,ratio\\)로 변환하면 0부터 무한대까지의 값을 가질 수 있다.\n승산비(\\(odds\\,ratio\\))를 로그변환하면 다음과 같다.\n\\(z = logit\\left(\\displaystyle\\frac{p}{1-p} \\right) = log\\left(\\displaystyle\\frac{p}{1-p}\\right)\\)\n\n- 식을 다음과 같이 변환해보자\n\\(log\\left(\\displaystyle\\frac{p}{1-p}\\right) = z\\)\n\n\\(\\Leftrightarrow \\displaystyle\\frac{p}{1-p} = e^z\\)\n\n\\(\\Leftrightarrow p = e^z(1-p)\\)\n\n\\(\\Leftrightarrow p = e^z - pe^k\\)\n\n\\(\\Leftrightarrow p(1+e^z) = e^z\\)\n\n\\(\\Leftrightarrow p = \\displaystyle\\frac{e^z}{1+e^z} = \\displaystyle\\frac{1}{1+e^{-z}}\\)\n\n이처럼 \\(logit\\,function\\)의 역함수가 \\(logistic\\, function\\)이다. 따라서\n\\(logistic(z) = \\mu(z) = \\displaystyle\\frac{1}{1+e^{-z}}\\)\n\n- \\(logistic\\,function\\)의 그래프는 다음과 같다.(출처)\n\n\n\nsigmoid\n\n\n\n\n\n- 그러다면 \\(logistic function\\)을 왜 사용하는 것일까?\n 사진출처 : ratsgo’s blog\n\n나이에 따른 질병 유무에 대한 분류문제를 해결해야한다고 가정해보자.\n그림과 같이 분류 문제를 선형 회귀로 해결하려고 하면 그림이 많이 이상해집니다.\n따라서 앞서 언급한 \\(logistic\\,function\\)을 이용하여 문제를 해결합니다. \\(logistic\\,function(z) = \\displaystyle\\frac{1}{1+e^{-z}} = \\begin{cases}1&z&gt;\\frac12\\\\\\frac12&z=\\frac12\\\\0&z&lt;\\frac12\\end{cases}\\)\n\n\n\n\n(추가 업로드 예정)\n\n\n\n(추가 업로드 예정)"
  },
  {
    "objectID": "posts/JBIG코드실습_타이타닉(2주차).html",
    "href": "posts/JBIG코드실습_타이타닉(2주차).html",
    "title": "JBIG코드실습_타이타닉(2주차)",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\n\n\ntrain = pd.read_csv('./train_2.csv')\ntrain.head()\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n\n\n\n\n\n\n\n\n# Matplolib 으로 Survived의 분포 확인\nimport matplotlib.pyplot as plt\n\nsurvived_counts = train['Survived'].value_counts()\n\n# 막대 그래프 그리기\nplt.bar(survived_counts.index, survived_counts.values)\n\n# 그래프에 제목과 레이블 추가\nplt.title('Survived Distribution')\nplt.xlabel('Survived')\nplt.ylabel('Count')\n\n# x축의 눈금 레이블 설정\nplt.xticks(survived_counts.index, ['Not Survived', 'Survived'])\n\n# 그래프 출력\nplt.show()\n\n\n\n\n\n\n\n\n\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n# 데이터 불러오기\ntrain = pd.read_csv('./train_2.csv')\n\n# 전처리 (필요시 전처리 추가할 것)#########################################################################\ntrain['Age'] = train['Age'].fillna(29)\ntrain['Sex'] = train['Sex'].map({'male': 1, 'female': 0})\n# 위 5개 칼럼만 사용할 것\ntrain = train[[ 'Pclass', 'SibSp', 'Age', 'Sex', 'Survived']] \n\n# X값과 y값 구하기\nX = train.drop(columns=['Survived'])\ny = train['Survived']\n\n# 데이터 세트 분리\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n\n# 스케일링과 데이터 인코딩이 필요한 칼럼을 확인한 후 전처리를 진행하세요\n\n\n# PyCaret으로 가장 좋은 성능을 보이는 모델을 찾으세요.\nfrom pycaret.classification import *\n# PyCaret 설정\nexp1 = setup(train, target='Survived', session_id=123,use_gpu=True) \n\n# 모델 비교 및 평가\nbest_model = compare_models()\n\n[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n[LightGBM] [Info] Number of positive: 1, number of negative: 1\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 0\n[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n[LightGBM] [Info] Number of positive: 1, number of negative: 1\n[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n[LightGBM] [Info] Number of positive: 1, number of negative: 1\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 0\n[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n[LightGBM] [Info] Number of positive: 1, number of negative: 1\n[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n[LightGBM] [Info] Number of positive: 1, number of negative: 1\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 0\n[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n[LightGBM] [Info] Number of positive: 1, number of negative: 1\n[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n[LightGBM] [Info] Number of positive: 1, number of negative: 1\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 0\n[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n[LightGBM] [Info] Number of positive: 1, number of negative: 1\n[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n[LightGBM] [Info] Number of positive: 1, number of negative: 1\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 0\n[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n[LightGBM] [Info] Number of positive: 1, number of negative: 1\n[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n[LightGBM] [Info] Number of positive: 1, number of negative: 1\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 0\n[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n[LightGBM] [Info] Number of positive: 1, number of negative: 1\n[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n[LightGBM] [Info] Number of positive: 1, number of negative: 1\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 0\n[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n[LightGBM] [Info] Number of positive: 1, number of negative: 1\n[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n[LightGBM] [Info] Number of positive: 1, number of negative: 1\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 0\n[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n[LightGBM] [Info] Number of positive: 1, number of negative: 1\n\n\n\n\n\n\n\n \nDescription\nValue\n\n\n\n\n0\nSession id\n123\n\n\n1\nTarget\nSurvived\n\n\n2\nTarget type\nBinary\n\n\n3\nOriginal data shape\n(891, 5)\n\n\n4\nTransformed data shape\n(891, 5)\n\n\n5\nTransformed train set shape\n(623, 5)\n\n\n6\nTransformed test set shape\n(268, 5)\n\n\n7\nNumeric features\n4\n\n\n8\nPreprocess\nTrue\n\n\n9\nImputation type\nsimple\n\n\n10\nNumeric imputation\nmean\n\n\n11\nCategorical imputation\nmode\n\n\n12\nFold Generator\nStratifiedKFold\n\n\n13\nFold Number\n10\n\n\n14\nCPU Jobs\n-1\n\n\n15\nUse GPU\nTrue\n\n\n16\nLog Experiment\nFalse\n\n\n17\nExperiment Name\nclf-default-name\n\n\n18\nUSI\n58d4\n\n\n\n\n\n\n\n\n\n\n\n\n\n \nModel\nAccuracy\nAUC\nRecall\nPrec.\nF1\nKappa\nMCC\nTT (Sec)\n\n\n\n\ngbc\nGradient Boosting Classifier\n0.8073\n0.8457\n0.6946\n0.7800\n0.7322\n0.5831\n0.5874\n0.0800\n\n\nlightgbm\nLight Gradient Boosting Machine\n0.8025\n0.8472\n0.6906\n0.7725\n0.7259\n0.5731\n0.5780\n1.4880\n\n\nada\nAda Boost Classifier\n0.8024\n0.8450\n0.7362\n0.7502\n0.7393\n0.5808\n0.5847\n0.0830\n\n\nridge\nRidge Classifier\n0.8008\n0.0000\n0.6946\n0.7663\n0.7258\n0.5706\n0.5745\n0.0160\n\n\nlda\nLinear Discriminant Analysis\n0.7992\n0.8544\n0.6946\n0.7638\n0.7245\n0.5677\n0.5717\n0.0170\n\n\nrf\nRandom Forest Classifier\n0.7977\n0.8239\n0.6987\n0.7601\n0.7242\n0.5654\n0.5704\n0.2370\n\n\nqda\nQuadratic Discriminant Analysis\n0.7928\n0.8521\n0.7321\n0.7320\n0.7263\n0.5606\n0.5661\n0.0170\n\n\nlr\nLogistic Regression\n0.7897\n0.8549\n0.6989\n0.7420\n0.7148\n0.5494\n0.5545\n0.0210\n\n\net\nExtra Trees Classifier\n0.7881\n0.7965\n0.6777\n0.7481\n0.7084\n0.5430\n0.5470\n0.2040\n\n\ndt\nDecision Tree Classifier\n0.7866\n0.7693\n0.6781\n0.7417\n0.7057\n0.5395\n0.5431\n0.0170\n\n\nknn\nK Neighbors Classifier\n0.7624\n0.8037\n0.6114\n0.7284\n0.6609\n0.4814\n0.4882\n0.0760\n\n\nnb\nNaive Bayes\n0.7577\n0.8426\n0.7071\n0.6797\n0.6874\n0.4907\n0.4970\n0.0170\n\n\nsvm\nSVM - Linear Kernel\n0.6257\n0.0000\n0.8321\n0.6098\n0.6406\n0.3214\n0.3598\n0.0160\n\n\ndummy\nDummy Classifier\n0.6164\n0.5000\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n0.0150\n\n\n\n\n\n\n\n\n\n# PyCaret에서 찾은 모델을 \n# 직접 sklearn의 GridSearch를 통해 하이퍼 파라미터를 튜닝해보세요.\n\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.model_selection import GridSearchCV\n\n# Gradient Boosting Classifier 모델 정의\ngb_classifier = GradientBoostingClassifier()\n\n# 탐색할 하이퍼파라미터 그리드 정의\nparam_grid = {\n    'n_estimators': [50, 100, 150],\n    'learning_rate': [0.05, 0.1, 0.2],\n    'max_depth': [3, 4, 5]\n}\n\n# GridSearchCV를 사용하여 하이퍼파라미터 튜닝\ngrid_search = GridSearchCV(estimator=gb_classifier, param_grid=param_grid, cv=5)\ngrid_search.fit(X, y)\n\n# 최적의 하이퍼파라미터와 점수 출력\nprint(\"Best Parameters:\", grid_search.best_params_)\nprint(\"Best Score:\", grid_search.best_score_)\n\nBest Parameters: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 150}\nBest Score: 0.8327788588286987\n\n\n\n# best params로 교차검증 및 StratifiedKFold를 진행하세요.\n# 최적의 하이퍼파라미터로 교차검증 및 StratifiedKFold 진행\nfrom sklearn.model_selection import cross_val_score, StratifiedKFold\nbest_params = grid_search.best_params_\ngb_classifier = GradientBoostingClassifier(**best_params)\nstratified_kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\ncv_scores = cross_val_score(gb_classifier, X, y, cv=stratified_kfold)\n\n# 교차검증 결과 출력\nprint(\"Cross Validation Scores:\", cv_scores)\nprint(\"Mean CV Score:\", cv_scores.mean())\n\nCross Validation Scores: [0.84916201 0.8258427  0.79775281 0.80898876 0.83707865]\nMean CV Score: 0.8237649865042999\n\n\n\n# 앙상블 기법 중 배깅을 사용해보세요.\n\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\n\n# 데이터를 훈련 세트와 테스트 세트로 나눔\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# 배깅 분류기 정의\nbagging_classifier = BaggingClassifier(gb_classifier, n_estimators=10, random_state=42)\n\n# 배깅 분류기를 훈련 데이터에 적합\ngb_classifier.fit(X, y)\n\n# 테스트 데이터에 대한 예측\ny_pred = gb_classifier.predict(X)\n\n# 정확도 평가\naccuracy = accuracy_score(y, y_pred)\nprint(\"Accuracy:\", accuracy)\n\nAccuracy: 0.8731762065095399"
  },
  {
    "objectID": "posts/랜덤포레스트.html",
    "href": "posts/랜덤포레스트.html",
    "title": "랜덤포레스트(배깅,보팅)",
    "section": "",
    "text": "유튜브 김성범 교수님 강의 , 전북대학교 최규빈 교수님 강의노트로 학습한 것을 토대로 작성하였습니다.\n\n\n\n\n\n계층적 구조로 인해 중간에 에러가 발생하면 다음 단계에도 에러가 계속 전파됩니다.\n학습 데이터의 미세한 변동에도 최종 결과에 크게 영향을 끼칠 수 있습니다.\n적은 개수의 노이즈에도 크게 영향을 받습니다.\n나무의 최종노드 개수를 늘리면 과적합(Overfitting) 위험이 발생합니다.\n\\(\\rightarrow\\) 해결방법 : 랜덤포레스트\n\n\n\n\n\n여러 Base모델(의사결정나무)들의 예측을 다수결법칙 or 평균을 이용해 통합하여 예측 정확성을 향상시키는 모델\n랜덤포레스트 구조를 시각화하면 다음과 같습니다.\n\n\n\n\nRandom_Forest_Structure.jpg\n\n\n\n여기서 Base모델은 서로 독립이고, 무작위 예측보다 성능이 좋아야한다는 전제조건이 필요합니다.\n랜덤포레스트의 중요한 두가지 키워드는 Diversity와 Random subspace입니다.\n\n\n\n\n\n여러 Training data를 생성하여 각 데이터마다 개별 의사결정나무 모델로 구축\n그럼 개별 의사결정나무에 어떻게 데이터를 할당해주지? \\(\\rightarrow\\) Bagging 이용\n\n\n\n\n각 모델은 서로 다른 학습 데이터 셋을 이용합니다.\n각 모델에 쓰이는 데이터는 원본데이터의 복원추출로 생성됩니다.\n각 데이터 셋은 원본 데이터의 개수만큼 복원추출을 시행합니다.\n\\(\\rightarrow\\) 원본 데이터의 개수가 n개라면 n번의 복원추출 시행\n위에 랜덤포레스트 구조를 시각화한 것이 이제는 이해가 가시나요?\n\n\n\n\n\n각 개별 트리가 노드를 뻗어나갈 때 필요한 변수는 무작위로 선택됩니다.\n\n\n\n\n\n\n학습된 각각의 의사결정나무들로 어떻게 결과를 도출할까요? \\(\\rightarrow\\) Voting이용\n다음 소개되는 Voting 방법들의 예시는 모두 Classification입니다.\nRegression : \\(f(x) = \\displaystyle\\sum_{m=1}^{M}c_mI(x \\in R_m)\\)\nClassification : \\(f(x) = \\displaystyle\\sum_{m=1}^{M} k(m)I\\left\\{(x_1,x_2)\\in R_m\\right\\}\\)\n\n\n\n\n\\(Ensemble(\\hat{y}) = \\underset{i}\\arg\\max \\left(\\displaystyle\\sum_{i=1}^{n}I(\\hat{y} \\in i), i \\in \\left\\{0,1\\right\\} \\right)\\)\nEX) \\(\\displaystyle\\sum_{i=1}^{n}I(\\hat{y} \\in 0)=4\\) , \\(\\displaystyle\\sum_{i=1}^{n}I(\\hat{y} \\in 1)=6 \\;\\Rightarrow\\; Ensemble(\\hat{y})=1\\)\n\n1이라고 예측한 모델이 6개, 0이라고 예측한 모델이 4개 즉, 다수결 투표로 1이 예측값이 되었습니다.(Hard Voting)\n\n\n\n\n\n\\(Ensemble(\\hat{y}) = \\underset{i}\\arg\\max \\Bigg(\\displaystyle\\frac{\\sum_{j=1}^{n}(TrainAcc_{j})I(\\hat{y}=1)}{\\sum_{j=1}^{n}(TrainAcc_{j})}, i \\in \\left\\{0,1 \\right\\}\\Bigg)\\)\n\nEX) \\(\\displaystyle\\frac{\\sum_{j=1}^{n}(TrainAcc_{j})I(\\hat{y}=0)}{\\sum_{j=1}^{n}(TrainAcc_{j})} = 0.4\\) , \\(\\displaystyle\\frac{\\sum_{j=1}^{n}(TrainAcc_{j})I(\\hat{y}=1)}{\\sum_{j=1}^{n}(TrainAcc_{j})} = 0.6\\)\n\n\\(\\Rightarrow\\; Ensemble(\\hat{y}) = 1\\) * 각 모델의 Accuracy를 바탕으로 가중치가 부여됩니다. 높은 Accuracy를 가진 모델은 더 많은 가중치를 갖게 됩니다.(가중치 평균을 이용한 Voting)\n\n\n\n\n\\(Ensemble(\\hat{y}) = \\underset{i}\\arg\\max\\big(\\displaystyle\\frac1n \\sum_{j=1}^{n}P(\\hat{y}=i), i \\in \\left\\{0,1 \\right\\}\\big)\\)\n\nEX) \\(\\displaystyle\\frac1n \\sum_{j=1}^{n}P(\\hat{y}=0)=0.3\\) , \\(\\displaystyle\\frac1n \\sum_{j=1}^{n}P(\\hat{y}=1)=0.7 \\;\\Rightarrow\\; Ensemble(\\hat{y}) = 1\\)\n\n각 모델들이 예측할 확률들의 평균을 구한 후에, 다수결을 이용하여 최조 선택하는 방식입니다.\n0이 나올 확률들의 평균값은 0.3 , 1이 나올 확률들의 평균값은 0.7이므로 1이 예측값이 되었습니다.(Soft Voting)\n\n\n\n실제로 Voting은 랜덤포레스트에서만 사용되는 앙상블 기법은 아닙니다. 랜덤포레스트에서 Voting은 동일한 모델(의사결정나무)들에서 Voting을 하는 반면에, 좀 더 일반적인 Voting은 다양한 모델에서 투표를 하는 방식으로 이루어집니다.\n\n\n\n\n\n(추후 업로드 예정)"
  },
  {
    "objectID": "posts/랜덤포레스트.html#랜덤포레스트",
    "href": "posts/랜덤포레스트.html#랜덤포레스트",
    "title": "랜덤포레스트(배깅,보팅)",
    "section": "",
    "text": "유튜브 김성범 교수님 강의 , 전북대학교 최규빈 교수님 강의노트로 학습한 것을 토대로 작성하였습니다.\n\n\n\n\n\n계층적 구조로 인해 중간에 에러가 발생하면 다음 단계에도 에러가 계속 전파됩니다.\n학습 데이터의 미세한 변동에도 최종 결과에 크게 영향을 끼칠 수 있습니다.\n적은 개수의 노이즈에도 크게 영향을 받습니다.\n나무의 최종노드 개수를 늘리면 과적합(Overfitting) 위험이 발생합니다.\n\\(\\rightarrow\\) 해결방법 : 랜덤포레스트\n\n\n\n\n\n여러 Base모델(의사결정나무)들의 예측을 다수결법칙 or 평균을 이용해 통합하여 예측 정확성을 향상시키는 모델\n랜덤포레스트 구조를 시각화하면 다음과 같습니다.\n\n\n\n\nRandom_Forest_Structure.jpg\n\n\n\n여기서 Base모델은 서로 독립이고, 무작위 예측보다 성능이 좋아야한다는 전제조건이 필요합니다.\n랜덤포레스트의 중요한 두가지 키워드는 Diversity와 Random subspace입니다.\n\n\n\n\n\n여러 Training data를 생성하여 각 데이터마다 개별 의사결정나무 모델로 구축\n그럼 개별 의사결정나무에 어떻게 데이터를 할당해주지? \\(\\rightarrow\\) Bagging 이용\n\n\n\n\n각 모델은 서로 다른 학습 데이터 셋을 이용합니다.\n각 모델에 쓰이는 데이터는 원본데이터의 복원추출로 생성됩니다.\n각 데이터 셋은 원본 데이터의 개수만큼 복원추출을 시행합니다.\n\\(\\rightarrow\\) 원본 데이터의 개수가 n개라면 n번의 복원추출 시행\n위에 랜덤포레스트 구조를 시각화한 것이 이제는 이해가 가시나요?\n\n\n\n\n\n각 개별 트리가 노드를 뻗어나갈 때 필요한 변수는 무작위로 선택됩니다.\n\n\n\n\n\n\n학습된 각각의 의사결정나무들로 어떻게 결과를 도출할까요? \\(\\rightarrow\\) Voting이용\n다음 소개되는 Voting 방법들의 예시는 모두 Classification입니다.\nRegression : \\(f(x) = \\displaystyle\\sum_{m=1}^{M}c_mI(x \\in R_m)\\)\nClassification : \\(f(x) = \\displaystyle\\sum_{m=1}^{M} k(m)I\\left\\{(x_1,x_2)\\in R_m\\right\\}\\)\n\n\n\n\n\\(Ensemble(\\hat{y}) = \\underset{i}\\arg\\max \\left(\\displaystyle\\sum_{i=1}^{n}I(\\hat{y} \\in i), i \\in \\left\\{0,1\\right\\} \\right)\\)\nEX) \\(\\displaystyle\\sum_{i=1}^{n}I(\\hat{y} \\in 0)=4\\) , \\(\\displaystyle\\sum_{i=1}^{n}I(\\hat{y} \\in 1)=6 \\;\\Rightarrow\\; Ensemble(\\hat{y})=1\\)\n\n1이라고 예측한 모델이 6개, 0이라고 예측한 모델이 4개 즉, 다수결 투표로 1이 예측값이 되었습니다.(Hard Voting)\n\n\n\n\n\n\\(Ensemble(\\hat{y}) = \\underset{i}\\arg\\max \\Bigg(\\displaystyle\\frac{\\sum_{j=1}^{n}(TrainAcc_{j})I(\\hat{y}=1)}{\\sum_{j=1}^{n}(TrainAcc_{j})}, i \\in \\left\\{0,1 \\right\\}\\Bigg)\\)\n\nEX) \\(\\displaystyle\\frac{\\sum_{j=1}^{n}(TrainAcc_{j})I(\\hat{y}=0)}{\\sum_{j=1}^{n}(TrainAcc_{j})} = 0.4\\) , \\(\\displaystyle\\frac{\\sum_{j=1}^{n}(TrainAcc_{j})I(\\hat{y}=1)}{\\sum_{j=1}^{n}(TrainAcc_{j})} = 0.6\\)\n\n\\(\\Rightarrow\\; Ensemble(\\hat{y}) = 1\\) * 각 모델의 Accuracy를 바탕으로 가중치가 부여됩니다. 높은 Accuracy를 가진 모델은 더 많은 가중치를 갖게 됩니다.(가중치 평균을 이용한 Voting)\n\n\n\n\n\\(Ensemble(\\hat{y}) = \\underset{i}\\arg\\max\\big(\\displaystyle\\frac1n \\sum_{j=1}^{n}P(\\hat{y}=i), i \\in \\left\\{0,1 \\right\\}\\big)\\)\n\nEX) \\(\\displaystyle\\frac1n \\sum_{j=1}^{n}P(\\hat{y}=0)=0.3\\) , \\(\\displaystyle\\frac1n \\sum_{j=1}^{n}P(\\hat{y}=1)=0.7 \\;\\Rightarrow\\; Ensemble(\\hat{y}) = 1\\)\n\n각 모델들이 예측할 확률들의 평균을 구한 후에, 다수결을 이용하여 최조 선택하는 방식입니다.\n0이 나올 확률들의 평균값은 0.3 , 1이 나올 확률들의 평균값은 0.7이므로 1이 예측값이 되었습니다.(Soft Voting)\n\n\n\n실제로 Voting은 랜덤포레스트에서만 사용되는 앙상블 기법은 아닙니다. 랜덤포레스트에서 Voting은 동일한 모델(의사결정나무)들에서 Voting을 하는 반면에, 좀 더 일반적인 Voting은 다양한 모델에서 투표를 하는 방식으로 이루어집니다.\n\n\n\n\n\n(추후 업로드 예정)"
  },
  {
    "objectID": "posts/의사결정나무.html",
    "href": "posts/의사결정나무.html",
    "title": "의사결정나무",
    "section": "",
    "text": "유튜브 김성범 교수님 강의 , 전북대학교 최규빈 교수님 강의노트로 학습한 것을 토대로 작성하였습니다.\n\n- 파이썬 코드로 간단한 예시를 들어보겠습니다.\n\n# import\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.animation\nimport IPython\nimport sklearn.tree\n#---#\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\n# visualization\nnp.random.seed(43052)\ntemp = pd.read_csv('https://raw.githubusercontent.com/guebin/DV2022/master/posts/temp.csv').iloc[:,3].to_numpy()[:100]\ntemp.sort()\neps = np.random.randn(100)*3 # 오차\nicecream_sales = 20 + temp * 2.5 + eps \ndf_train = pd.DataFrame({'temp':temp,'sales':icecream_sales})\nX = df_train[['temp']]\ny = df_train['sales']\nplt.plot(X,y,'o')\nplt.xlabel('temp')\nplt.ylabel('sales')\nplt.show()\n\n\n\n\n\n\n\n\n\n그림과 같이 온도에 따른 아이스크림 판매량의 그래프가 있습니다.\n여기서 특정 온도에 대한 아이스크림 판매량을 의사결정나무로 예측해보겠습니다.\n\n\npdtr = sklearn.tree.DecisionTreeRegressor(max_depth=1)\npdtr.fit(X,y)\n\nDecisionTreeRegressor(max_depth=1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DecisionTreeRegressorDecisionTreeRegressor(max_depth=1)\n\n\n\nsklearn.tree.plot_tree(\n    pdtr,\n    feature_names=X.columns\n                      );\nfig1 = plt.gcf()\nfig1.suptitle(\"Tree1\")\n\nText(0.5, 0.98, 'Tree1')\n\n\n\n\n\n\n\n\n\n\n그림과 같이 변수 temp를 특정 기준(5.05)으로 나누어서 분류해줍니다.\n지금은 최대깊이가(max_depth) 1로 지정한 트리입니다.\n최대깊이(max_depth)를 늘린 트리는 다음과 같습니다.\n\n\npdtr2 = sklearn.tree.DecisionTreeRegressor(max_depth=3)\npdtr2.fit(X,y)\n\nDecisionTreeRegressor(max_depth=3)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DecisionTreeRegressorDecisionTreeRegressor(max_depth=3)\n\n\n\nsklearn.tree.plot_tree(\n    pdtr2,\n    max_depth=3,\n    feature_names=X.columns\n                      );\nfig2 = plt.gcf()\nfig2.suptitle(\"Tree2\")\n\nText(0.5, 0.98, 'Tree2')\n\n\n\n\n\n\n\n\n\n\n이처럼 의사결정나무는 특정 변수에 대해 기준을 나누어 가지를 뻗어나가는 메커니즘입니다.\n나무를 뒤짚어놓은 모양처럼 생겨서 의사결정나무라고 불립니다.\n각각의 박스들을 노드라고 부릅니다.\n맨 위의 노드를 뿌리노드(Root node), 맨 아리의 노드를 끝노드(Terminal node)라고 부릅니다.\n\n- Tree1의 좌표평면에 대한 시각화는 다음과 같습니다.\n\nplt.plot(X,y,'o',alpha=0.5,label='True')\nplt.plot(X,pdtr.predict(X),'--.',label='Predicted')\nplt.legend()\n\n\n\n\n\n\n\n\n- Tree2의 좌표평면에 대한 시각화는 다음과 같습니다.\n\nplt.plot(X,y,'o',alpha=0.5,label='True')\nplt.plot(X,pdtr2.predict(X),'.--',label='Predicted')\nplt.legend()\n\n\n\n\n\n\n\n\n\nTree1에 비해서 Tree2가 더 잘맞출 것 같습니다.\n하지만 max depth가 너무 많이 커진다면 과적합(Overfittig)의 위험이 생깁니다.\nmax depth에 따른 좌포평면 시각화 애니메이션은 다음과 같습니다.\n\n\n\n\n## step1 \nX = df_train[['temp']]\ny = df_train['sales']\n## step2\npredictrs = [sklearn.tree.DecisionTreeRegressor(max_depth=k) for k in range(1,11)]\n## step3 \nfor k in range(10):\n    predictrs[k].fit(X,y)\n## step4 -- pass\nfig = plt.figure()\n\n&lt;Figure size 432x288 with 0 Axes&gt;\n\n\n\ndef func(frame):\n    ax = fig.gca()\n    ax.clear()\n    ax.plot(X,y,'o',alpha=0.5) \n    ax.plot(X,predictrs[frame].predict(X),'.--') \n    ax.set_title(f'max_depth={predictrs[frame].max_depth}')\n\n\nani = matplotlib.animation.FuncAnimation(\n    fig,\n    func,\n    frames=10\n)\n\n\ndisplay(IPython.display.HTML(ani.to_jshtml()))\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n- 시각화처럼 max depth가 너무 커지면 과적합(Overfitting)의 위험이 증가합니다.\n\n\n\n(추후 업로드 예정)"
  },
  {
    "objectID": "posts/의사결정나무.html#의사결정나무",
    "href": "posts/의사결정나무.html#의사결정나무",
    "title": "의사결정나무",
    "section": "",
    "text": "유튜브 김성범 교수님 강의 , 전북대학교 최규빈 교수님 강의노트로 학습한 것을 토대로 작성하였습니다.\n\n- 파이썬 코드로 간단한 예시를 들어보겠습니다.\n\n# import\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.animation\nimport IPython\nimport sklearn.tree\n#---#\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\n# visualization\nnp.random.seed(43052)\ntemp = pd.read_csv('https://raw.githubusercontent.com/guebin/DV2022/master/posts/temp.csv').iloc[:,3].to_numpy()[:100]\ntemp.sort()\neps = np.random.randn(100)*3 # 오차\nicecream_sales = 20 + temp * 2.5 + eps \ndf_train = pd.DataFrame({'temp':temp,'sales':icecream_sales})\nX = df_train[['temp']]\ny = df_train['sales']\nplt.plot(X,y,'o')\nplt.xlabel('temp')\nplt.ylabel('sales')\nplt.show()\n\n\n\n\n\n\n\n\n\n그림과 같이 온도에 따른 아이스크림 판매량의 그래프가 있습니다.\n여기서 특정 온도에 대한 아이스크림 판매량을 의사결정나무로 예측해보겠습니다.\n\n\npdtr = sklearn.tree.DecisionTreeRegressor(max_depth=1)\npdtr.fit(X,y)\n\nDecisionTreeRegressor(max_depth=1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DecisionTreeRegressorDecisionTreeRegressor(max_depth=1)\n\n\n\nsklearn.tree.plot_tree(\n    pdtr,\n    feature_names=X.columns\n                      );\nfig1 = plt.gcf()\nfig1.suptitle(\"Tree1\")\n\nText(0.5, 0.98, 'Tree1')\n\n\n\n\n\n\n\n\n\n\n그림과 같이 변수 temp를 특정 기준(5.05)으로 나누어서 분류해줍니다.\n지금은 최대깊이가(max_depth) 1로 지정한 트리입니다.\n최대깊이(max_depth)를 늘린 트리는 다음과 같습니다.\n\n\npdtr2 = sklearn.tree.DecisionTreeRegressor(max_depth=3)\npdtr2.fit(X,y)\n\nDecisionTreeRegressor(max_depth=3)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DecisionTreeRegressorDecisionTreeRegressor(max_depth=3)\n\n\n\nsklearn.tree.plot_tree(\n    pdtr2,\n    max_depth=3,\n    feature_names=X.columns\n                      );\nfig2 = plt.gcf()\nfig2.suptitle(\"Tree2\")\n\nText(0.5, 0.98, 'Tree2')\n\n\n\n\n\n\n\n\n\n\n이처럼 의사결정나무는 특정 변수에 대해 기준을 나누어 가지를 뻗어나가는 메커니즘입니다.\n나무를 뒤짚어놓은 모양처럼 생겨서 의사결정나무라고 불립니다.\n각각의 박스들을 노드라고 부릅니다.\n맨 위의 노드를 뿌리노드(Root node), 맨 아리의 노드를 끝노드(Terminal node)라고 부릅니다.\n\n- Tree1의 좌표평면에 대한 시각화는 다음과 같습니다.\n\nplt.plot(X,y,'o',alpha=0.5,label='True')\nplt.plot(X,pdtr.predict(X),'--.',label='Predicted')\nplt.legend()\n\n\n\n\n\n\n\n\n- Tree2의 좌표평면에 대한 시각화는 다음과 같습니다.\n\nplt.plot(X,y,'o',alpha=0.5,label='True')\nplt.plot(X,pdtr2.predict(X),'.--',label='Predicted')\nplt.legend()\n\n\n\n\n\n\n\n\n\nTree1에 비해서 Tree2가 더 잘맞출 것 같습니다.\n하지만 max depth가 너무 많이 커진다면 과적합(Overfittig)의 위험이 생깁니다.\nmax depth에 따른 좌포평면 시각화 애니메이션은 다음과 같습니다.\n\n\n\n\n## step1 \nX = df_train[['temp']]\ny = df_train['sales']\n## step2\npredictrs = [sklearn.tree.DecisionTreeRegressor(max_depth=k) for k in range(1,11)]\n## step3 \nfor k in range(10):\n    predictrs[k].fit(X,y)\n## step4 -- pass\nfig = plt.figure()\n\n&lt;Figure size 432x288 with 0 Axes&gt;\n\n\n\ndef func(frame):\n    ax = fig.gca()\n    ax.clear()\n    ax.plot(X,y,'o',alpha=0.5) \n    ax.plot(X,predictrs[frame].predict(X),'.--') \n    ax.set_title(f'max_depth={predictrs[frame].max_depth}')\n\n\nani = matplotlib.animation.FuncAnimation(\n    fig,\n    func,\n    frames=10\n)\n\n\ndisplay(IPython.display.HTML(ani.to_jshtml()))\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n- 시각화처럼 max depth가 너무 커지면 과적합(Overfitting)의 위험이 증가합니다.\n\n\n\n(추후 업로드 예정)"
  },
  {
    "objectID": "posts/GBM.html",
    "href": "posts/GBM.html",
    "title": "GBM",
    "section": "",
    "text": "- 다음은 wikipedia의 Gradient Boosting Algorithm입니다. 다음의 수식을 먼저 읽고 최대한 이해하려고 노력해봅시다.\n\n\n\n\n\n\n\n\n1. initialize model with a constant value:\n\\(F_0(x) = \\underset{\\gamma}{\\arg\\min}\\sum_{i=1}^n L(y_i, \\gamma)\\)\n\n\n2. For m = 1 to M : 1. Compute so-called pseudo-residuals\n\\(r_{im} = - \\Bigg[\\displaystyle\\frac{\\partial L(y_i, F(x_i))}{\\partial F(x_i)}\\Bigg]_{F(x) = F_{m-1}(x)}\\) for \\(i = 1, ... ,n\\) 2. Fit a base learner (or weak learner, e.g. tree) closed under scaling \\(h_m(x)\\) to pseuo-residuals, i.e. train it using the training set \\(\\{(x_i, r_{im})\\}_{i=1}^{n}\\) 3. Compute multiplier \\(\\gamma_{m}\\) by solving the following one-dimensional optimization problem:\n\\(\\gamma_m = \\underset{\\gamma}{\\arg\\min}\\sum_{i=1}^n L(y_i, F_{m-1}(x_i) + \\gamma h_m(x_i))\\) 4. Update the model:\n\\(F_m(x) = F_{m-1}(x) + \\gamma_m h_m(x)\\)\n\n\n3. Output \\(\\hat{f(x)} = F_M(x)\\)\n\n\n\n수식으로만 완전히 이해하기가 어렵기 때문에 간단한 예시와 pandas dataframe을 이용하여 설명해보겠습니다.\n다음과 같은 데이터가 있다고 가정해보겠습니다.\n\nimport pandas as pd\nidx = ['학생1', '학생2', '학생3', '학생4']\ndata = pd.DataFrame({'공부시간(m)':[150, 120, 60, 80], '성별':['여','남','남', '여'], '수학점수':[90, 65, 55, '???']},index=idx)\ndata\n\n\n\n\n\n\n\n\n공부시간(m)\n성별\n수학점수\n\n\n\n\n학생1\n150\n여\n90\n\n\n학생2\n120\n남\n65\n\n\n학생3\n60\n남\n55\n\n\n학생4\n80\n여\n???\n\n\n\n\n\n\n\n다음과 같은 데이터가 있다고 할 때, 저희는 학생4의 수학점수를 공부시간(m)을 통하여 예측하고 싶습니다.\n\n\n\n\n\n1. initialize model with a constant value:\\\n\n\n\\(F_0(x) = \\underset{\\gamma}{\\arg\\min}\\sum_{i=1}^n L(y_i, \\gamma)\\)\n\n\n\n\n초기모델을 상수로 정의합니다. 여기서 상수는 \\(y_i\\)와 \\(\\gamma\\)입니다.\n\\(y_i\\)는 수학점수를 의미하고 \\(\\gamma\\)는 초기 예측값입니다.\n\n\\(y_1\\) = 90, \\(y_2\\) = 65, \\(y_3\\) = 60\n\n여기서의 Loss Function \\(L(y_i, \\gamma) = \\displaystyle\\frac1n \\sum_{i=1}^{n} (y_i - \\gamma)^2\\)입니다. (잘 모르시는 분들은 MSE 학습하시면 좋을 것 같습니다.)\n예시의 수치를 대입하면\n\\(L(y_i, \\gamma) = \\displaystyle\\frac13 \\sum_{i=1}^{3} (y_i - \\gamma)^2 =  \\displaystyle\\frac13(90 - \\gamma)^2 + \\displaystyle\\frac13(65 - \\gamma)^2 + \\displaystyle\\frac13(55 - \\gamma)^2\\)입니다.\n\\(L(y_i, \\gamma)\\)가 최소가 되는 \\(\\gamma\\)를 찾아야 하므로 \\(\\displaystyle\\frac{\\partial L}{\\partial \\gamma} = \\displaystyle\\frac23(\\gamma - 90) + \\displaystyle\\frac23(\\gamma - 65) + \\displaystyle\\frac23(\\gamma - 55) = \\displaystyle\\frac23(3\\gamma - 210)\\)입니다.\n\\(\\displaystyle\\frac{\\partial L}{\\partial \\gamma} = 0\\)이 되어야 하므로 \\(\\gamma\\) = 70입니다. 따라서 초기 예측값 \\(\\gamma\\)는 70이 됩니다.\n초기 예측값을 이용한 첫번째 잔차는 각각 20, -5, -15가 됩니다.\n\n\nimport pandas as pd\nidx = ['Student1', 'Student2', 'Student3']\ndata = pd.DataFrame({'StudyTime(m)':[150, 120, 60], 'Sex':['female','male','male'], \n                     'MathScore':[90, 65, 55], 'F_0':[70 for i in range(3)],'r1': [20, -5, -15]},index=idx)\ndata\n\n\n\n\n\n\n\n\nStudyTime(m)\nSex\nMathScore\nF_0\nr1\n\n\n\n\nStudent1\n150\nfemale\n90\n70\n20\n\n\nStudent2\n120\nmale\n65\n70\n-5\n\n\nStudent3\n60\nmale\n55\n70\n-15\n\n\n\n\n\n\n\n- 위의 데이터를 의사결정나무로 시각화를 하면 다음과 같습니다.\n\nimport sklearn.tree\n## step1\nx = data[['StudyTime(m)']]\ny = data['MathScore']\n\n## step2\npredictr = sklearn.tree.DecisionTreeRegressor(max_depth=1)\n\n## step3\npredictr.fit(x,y)\n\n## visualization\nsklearn.tree.plot_tree(predictr,feature_names=x.columns);\n\n\n\n\n\n\n\n\n\n\n\n\nFor \\(m\\) = 1 to \\(M\\):\n\nCompute so-called pseudo-residuals \\(r_{im} = - \\Bigg[\\displaystyle\\frac{\\partial L(y_i, F(x_i))}{\\partial F(x_i)}\\Bigg]_{F(x) = F_{m-1}(x)}\\) for \\(i = 1, ... ,n\\)\nFit a base learner (or weak learner, e.g. tree) closed under scaling \\(h_m(x)\\) to pseuo-residuals, i.e. train it using the training set \\(\\{(x_i, r_{im})\\}_{i=1}^{n}\\)\nCompute multiplier \\(\\gamma_{m}\\) by solving the following one-dimensional optimization problem:\n\\(\\gamma_m = \\underset{\\gamma}{\\arg\\min}\\sum_{i=1}^n L(y_i, F_{m-1}(x_i) + \\gamma h_m(x_i))\\)\nUpdate the model:\n\\(F_m(x) = F_{m-1}(x) + \\gamma_m h_m(x)\\)\n\nA단계부터 D단계를 총 M번 반복하는 단계입니다.\n여기서 \\(M\\)은 예측기의 개수, \\(n\\)은 데이터의 개수입니다.\n예측기에 번호를 붙여 \\(m\\)=1인 경우는 1번 예측기 \\(m=M\\)인 경우는 M번 예측기라고 부르겠습니다.\n1번 예측기(\\(m=1\\))가 생성 되는 과정을 살펴보겠습니다. \\(L(y_i,F(x_i)) = \\displaystyle\\frac12(y_i - F(x_i))^2\\)이므로\n\n\\(r_{i1} = - \\Bigg[\\displaystyle\\frac{\\partial L(y_i, F(x_i))}{\\partial F(x_i)}\\Bigg]_{F(x) = F_{0}(x)} = y_i - F_0(x_i)\\)입니다. (잔차와 식이 동일합니다.)\n0번 예측기 \\(F_0(x)\\)는 1단계의 초기모델에서 생성되었습니다. 따라서 \\(F_0(x)=70\\)입니다.(1단계 참고)\n따라서 \\(r_{11}=20\\), \\(r_{21}=-5\\), \\(r_{31}=-15\\)입니다. 1단계 표에 있는 잔차1과 같습니다.\n함수\\(h_1(x)\\)는 앞서 계산한 잔차를 트리 계열로 학습합니다.(의사 결정 나무에 대해서는 추후에 업로드 하겠습니다.)\n모든 데이터(\\(n=3\\))에 대해서 \\(\\sum_{i=1}^{3} L(y_i, F_0(x_i) + \\gamma h_m(x_i)\\) 식이 최소가 되는 \\(\\gamma\\)(learning rate)를 찾습니다.(\\(\\gamma = \\gamma_{1}\\))\n모델을 업데이트 합니다 : \\(F_1(x) = F_0(x) + \\gamma_1 h_1(x)\\)\n\n4번의 과정을 M번 반복합니다.\n\n- 추가설명\n\n\n\n\n\n\n3. Output \\(\\hat{f(x)} = F_M(x)\\)\n\n\n\n\n최종 모델을 산출합니다.\n\n\n\n\n\n(추후 업로드 예정)"
  },
  {
    "objectID": "posts/GBM.html#algorithm",
    "href": "posts/GBM.html#algorithm",
    "title": "GBM",
    "section": "",
    "text": "1. initialize model with a constant value:\n\\(F_0(x) = \\underset{\\gamma}{\\arg\\min}\\sum_{i=1}^n L(y_i, \\gamma)\\)\n\n\n2. For m = 1 to M : 1. Compute so-called pseudo-residuals\n\\(r_{im} = - \\Bigg[\\displaystyle\\frac{\\partial L(y_i, F(x_i))}{\\partial F(x_i)}\\Bigg]_{F(x) = F_{m-1}(x)}\\) for \\(i = 1, ... ,n\\) 2. Fit a base learner (or weak learner, e.g. tree) closed under scaling \\(h_m(x)\\) to pseuo-residuals, i.e. train it using the training set \\(\\{(x_i, r_{im})\\}_{i=1}^{n}\\) 3. Compute multiplier \\(\\gamma_{m}\\) by solving the following one-dimensional optimization problem:\n\\(\\gamma_m = \\underset{\\gamma}{\\arg\\min}\\sum_{i=1}^n L(y_i, F_{m-1}(x_i) + \\gamma h_m(x_i))\\) 4. Update the model:\n\\(F_m(x) = F_{m-1}(x) + \\gamma_m h_m(x)\\)\n\n\n3. Output \\(\\hat{f(x)} = F_M(x)\\)\n\n\n\n수식으로만 완전히 이해하기가 어렵기 때문에 간단한 예시와 pandas dataframe을 이용하여 설명해보겠습니다.\n다음과 같은 데이터가 있다고 가정해보겠습니다.\n\nimport pandas as pd\nidx = ['학생1', '학생2', '학생3', '학생4']\ndata = pd.DataFrame({'공부시간(m)':[150, 120, 60, 80], '성별':['여','남','남', '여'], '수학점수':[90, 65, 55, '???']},index=idx)\ndata\n\n\n\n\n\n\n\n\n공부시간(m)\n성별\n수학점수\n\n\n\n\n학생1\n150\n여\n90\n\n\n학생2\n120\n남\n65\n\n\n학생3\n60\n남\n55\n\n\n학생4\n80\n여\n???\n\n\n\n\n\n\n\n다음과 같은 데이터가 있다고 할 때, 저희는 학생4의 수학점수를 공부시간(m)을 통하여 예측하고 싶습니다.\n\n\n\n\n\n1. initialize model with a constant value:\\\n\n\n\\(F_0(x) = \\underset{\\gamma}{\\arg\\min}\\sum_{i=1}^n L(y_i, \\gamma)\\)\n\n\n\n\n초기모델을 상수로 정의합니다. 여기서 상수는 \\(y_i\\)와 \\(\\gamma\\)입니다.\n\\(y_i\\)는 수학점수를 의미하고 \\(\\gamma\\)는 초기 예측값입니다.\n\n\\(y_1\\) = 90, \\(y_2\\) = 65, \\(y_3\\) = 60\n\n여기서의 Loss Function \\(L(y_i, \\gamma) = \\displaystyle\\frac1n \\sum_{i=1}^{n} (y_i - \\gamma)^2\\)입니다. (잘 모르시는 분들은 MSE 학습하시면 좋을 것 같습니다.)\n예시의 수치를 대입하면\n\\(L(y_i, \\gamma) = \\displaystyle\\frac13 \\sum_{i=1}^{3} (y_i - \\gamma)^2 =  \\displaystyle\\frac13(90 - \\gamma)^2 + \\displaystyle\\frac13(65 - \\gamma)^2 + \\displaystyle\\frac13(55 - \\gamma)^2\\)입니다.\n\\(L(y_i, \\gamma)\\)가 최소가 되는 \\(\\gamma\\)를 찾아야 하므로 \\(\\displaystyle\\frac{\\partial L}{\\partial \\gamma} = \\displaystyle\\frac23(\\gamma - 90) + \\displaystyle\\frac23(\\gamma - 65) + \\displaystyle\\frac23(\\gamma - 55) = \\displaystyle\\frac23(3\\gamma - 210)\\)입니다.\n\\(\\displaystyle\\frac{\\partial L}{\\partial \\gamma} = 0\\)이 되어야 하므로 \\(\\gamma\\) = 70입니다. 따라서 초기 예측값 \\(\\gamma\\)는 70이 됩니다.\n초기 예측값을 이용한 첫번째 잔차는 각각 20, -5, -15가 됩니다.\n\n\nimport pandas as pd\nidx = ['Student1', 'Student2', 'Student3']\ndata = pd.DataFrame({'StudyTime(m)':[150, 120, 60], 'Sex':['female','male','male'], \n                     'MathScore':[90, 65, 55], 'F_0':[70 for i in range(3)],'r1': [20, -5, -15]},index=idx)\ndata\n\n\n\n\n\n\n\n\nStudyTime(m)\nSex\nMathScore\nF_0\nr1\n\n\n\n\nStudent1\n150\nfemale\n90\n70\n20\n\n\nStudent2\n120\nmale\n65\n70\n-5\n\n\nStudent3\n60\nmale\n55\n70\n-15\n\n\n\n\n\n\n\n- 위의 데이터를 의사결정나무로 시각화를 하면 다음과 같습니다.\n\nimport sklearn.tree\n## step1\nx = data[['StudyTime(m)']]\ny = data['MathScore']\n\n## step2\npredictr = sklearn.tree.DecisionTreeRegressor(max_depth=1)\n\n## step3\npredictr.fit(x,y)\n\n## visualization\nsklearn.tree.plot_tree(predictr,feature_names=x.columns);\n\n\n\n\n\n\n\n\n\n\n\n\nFor \\(m\\) = 1 to \\(M\\):\n\nCompute so-called pseudo-residuals \\(r_{im} = - \\Bigg[\\displaystyle\\frac{\\partial L(y_i, F(x_i))}{\\partial F(x_i)}\\Bigg]_{F(x) = F_{m-1}(x)}\\) for \\(i = 1, ... ,n\\)\nFit a base learner (or weak learner, e.g. tree) closed under scaling \\(h_m(x)\\) to pseuo-residuals, i.e. train it using the training set \\(\\{(x_i, r_{im})\\}_{i=1}^{n}\\)\nCompute multiplier \\(\\gamma_{m}\\) by solving the following one-dimensional optimization problem:\n\\(\\gamma_m = \\underset{\\gamma}{\\arg\\min}\\sum_{i=1}^n L(y_i, F_{m-1}(x_i) + \\gamma h_m(x_i))\\)\nUpdate the model:\n\\(F_m(x) = F_{m-1}(x) + \\gamma_m h_m(x)\\)\n\nA단계부터 D단계를 총 M번 반복하는 단계입니다.\n여기서 \\(M\\)은 예측기의 개수, \\(n\\)은 데이터의 개수입니다.\n예측기에 번호를 붙여 \\(m\\)=1인 경우는 1번 예측기 \\(m=M\\)인 경우는 M번 예측기라고 부르겠습니다.\n1번 예측기(\\(m=1\\))가 생성 되는 과정을 살펴보겠습니다. \\(L(y_i,F(x_i)) = \\displaystyle\\frac12(y_i - F(x_i))^2\\)이므로\n\n\\(r_{i1} = - \\Bigg[\\displaystyle\\frac{\\partial L(y_i, F(x_i))}{\\partial F(x_i)}\\Bigg]_{F(x) = F_{0}(x)} = y_i - F_0(x_i)\\)입니다. (잔차와 식이 동일합니다.)\n0번 예측기 \\(F_0(x)\\)는 1단계의 초기모델에서 생성되었습니다. 따라서 \\(F_0(x)=70\\)입니다.(1단계 참고)\n따라서 \\(r_{11}=20\\), \\(r_{21}=-5\\), \\(r_{31}=-15\\)입니다. 1단계 표에 있는 잔차1과 같습니다.\n함수\\(h_1(x)\\)는 앞서 계산한 잔차를 트리 계열로 학습합니다.(의사 결정 나무에 대해서는 추후에 업로드 하겠습니다.)\n모든 데이터(\\(n=3\\))에 대해서 \\(\\sum_{i=1}^{3} L(y_i, F_0(x_i) + \\gamma h_m(x_i)\\) 식이 최소가 되는 \\(\\gamma\\)(learning rate)를 찾습니다.(\\(\\gamma = \\gamma_{1}\\))\n모델을 업데이트 합니다 : \\(F_1(x) = F_0(x) + \\gamma_1 h_1(x)\\)\n\n4번의 과정을 M번 반복합니다.\n\n- 추가설명\n\n\n\n\n\n\n3. Output \\(\\hat{f(x)} = F_M(x)\\)\n\n\n\n\n최종 모델을 산출합니다."
  },
  {
    "objectID": "posts/GBM.html#코드-구현",
    "href": "posts/GBM.html#코드-구현",
    "title": "GBM",
    "section": "",
    "text": "(추후 업로드 예정)"
  },
  {
    "objectID": "posts/JBIG코드실습_풍속예측(2주차).html",
    "href": "posts/JBIG코드실습_풍속예측(2주차).html",
    "title": "JBIG코드실습_타이타닉(4주차)",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport random\nimport os\n\n\ntrain = pd.read_csv('train_3.csv')\ntest = pd.read_csv('test_3.csv')\n\n\ndef seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n\nseed_everything(42) # Seed 고정\n\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.tree import DecisionTreeRegressor\n\n# 독립변수(X), 종속변수(y)로 나누기\ntrain_x = train.drop(columns=['ID', '풍속 (m/s)'], axis = 1)\ntrain_y = train['풍속 (m/s)']\n\ntest_x = test.drop(columns=['ID'])\n\n# 데이터 인코딩\nle = LabelEncoder()\nle = le.fit(train_x['측정 시간대'])\ntrain_x['측정 시간대'] = le.transform(train_x['측정 시간대'])\ntest_x['측정 시간대'] = le.transform(test_x['측정 시간대'])\n\n# 회귀 모델 정의\ndt_clf = DecisionTreeRegressor(random_state=11) \n\n# 모델 학습\ndt_clf.fit(train_x , train_y) \n\n# 예측\ndt_pred = dt_clf.predict(test_x) \n\n\ntrain.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 36581 entries, 0 to 36580\nData columns (total 16 columns):\n #   Column          Non-Null Count  Dtype  \n---  ------          --------------  -----  \n 0   ID              36581 non-null  object \n 1   월               36581 non-null  int64  \n 2   일               36581 non-null  int64  \n 3   측정 시간대          36581 non-null  object \n 4   섭씨 온도(°⁣C)      36581 non-null  float64\n 5   절대 온도(K)        36581 non-null  float64\n 6   이슬점 온도(°C)      36581 non-null  float64\n 7   상대 습도 (%)       36581 non-null  float64\n 8   대기압(mbar)       36581 non-null  float64\n 9   포화 증기압(mbar)    36581 non-null  float64\n 10  실제 증기압(mbar)    36581 non-null  float64\n 11  증기압 부족량(mbar)   36581 non-null  float64\n 12  수증기 함량 (g/kg)   36581 non-null  float64\n 13  공기 밀도 (g/m**3)  36581 non-null  float64\n 14  풍향 (deg)        36581 non-null  float64\n 15  풍속 (m/s)        36581 non-null  float64\ndtypes: float64(12), int64(2), object(2)\nmemory usage: 4.5+ MB\n\n\n공기 밀도 (g/m**3)\n풍향 (deg)\n\ntrain.columns\n\nIndex(['ID', '월', '일', '측정 시간대', '섭씨 온도(°⁣C)', '절대 온도(K)', '이슬점 온도(°C)',\n       '상대 습도 (%)', '대기압(mbar)', '포화 증기압(mbar)', '실제 증기압(mbar)',\n       '증기압 부족량(mbar)', '수증기 함량 (g/kg)', '공기 밀도 (g/m**3)', '풍향 (deg)',\n       '풍속 (m/s)'],\n      dtype='object')\n\n\n\nimport matplotlib.pyplot as plt\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# train 데이터프레임이 적절히 정의되어 있다고 가정\n# train = pd.read_csv(\"train.csv\")\n\n# 첫 번째, 두 번째, 세 번째 열을 제외한 열들을 선택\nselected_columns = train.columns[3:]\n\n# 선택된 열들을 가진 새로운 데이터프레임 생성\nselected_data = train[selected_columns]\n\n# 박스 플롯 그리기\nplt.figure(figsize=(10, 6))  # 그래프의 크기 설정\nselected_data.boxplot()  # 박스 플롯 그리기\nplt.title('Boxplot of Selected Columns')  # 그래프 제목 설정\nplt.xlabel('Columns')  # x축 레이블 설정\nplt.ylabel('Values')  # y축 레이블 설정\nplt.xticks(rotation=45)  # x축 레이블 회전\nplt.grid(True)  # 그리드 추가\nplt.tight_layout()  # 그래프 레이아웃 조정\nplt.show()  # 그래프 출력\n\n\n\n\n\n\n\n\n\n# Normalization 혹은 Standardrization이 필요한 칼럼을 확인한 후 전처리를 진행해보세요.\n\nfrom sklearn.preprocessing import StandardScaler\n\n# '공기 밀도 (g/m**3)'열과 '풍향 (deg)'열만 선택\nselected_columns = ['공기 밀도 (g/m**3)', '풍향 (deg)']\n\n# 선택된 열들에 대한 데이터 추출\nselected_data = train[selected_columns]\n\n# StandardScaler를 사용하여 선택된 열들을 표준화\nscaler = StandardScaler()\nscaled_data = scaler.fit_transform(selected_data)\n\n# 표준화된 데이터를 데이터프레임으로 변환\nscaled_df = pd.DataFrame(scaled_data, columns=selected_columns)\n\n# 기존 데이터프레임에 표준화된 데이터 열을 추가\ntrain[selected_columns] = scaled_df\n\n\ntrain.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 36581 entries, 0 to 36580\nData columns (total 16 columns):\n #   Column          Non-Null Count  Dtype  \n---  ------          --------------  -----  \n 0   ID              36581 non-null  object \n 1   월               36581 non-null  int64  \n 2   일               36581 non-null  int64  \n 3   측정 시간대          36581 non-null  object \n 4   섭씨 온도(°⁣C)      36581 non-null  float64\n 5   절대 온도(K)        36581 non-null  float64\n 6   이슬점 온도(°C)      36581 non-null  float64\n 7   상대 습도 (%)       36581 non-null  float64\n 8   대기압(mbar)       36581 non-null  float64\n 9   포화 증기압(mbar)    36581 non-null  float64\n 10  실제 증기압(mbar)    36581 non-null  float64\n 11  증기압 부족량(mbar)   36581 non-null  float64\n 12  수증기 함량 (g/kg)   36581 non-null  float64\n 13  공기 밀도 (g/m**3)  36581 non-null  float64\n 14  풍향 (deg)        36581 non-null  float64\n 15  풍속 (m/s)        36581 non-null  float64\ndtypes: float64(12), int64(2), object(2)\nmemory usage: 4.5+ MB\n\n\n\n# train과 test셋을 분리하여 rmse로 에러를 평가해보세요.\n\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nimport numpy as np\n\n# 예시로 사용할 데이터를 생성합니다.\n# 아래 코드는 실제 데이터를 사용하는 것이 아니라 예시를 위한 가상의 데이터입니다.\n# 실제 데이터를 사용할 경우 해당 데이터를 불러와야 합니다.\n# 여기서는 특징과 타겟을 각각 X와 y에 할당하는 예시를 제공합니다.\nX = train.drop(['ID','월','일','측정 시간대'], axis=1)  # 특징들\ny = train['풍속 (m/s)']  # 타겟\n\n# train과 test 데이터로 분리합니다.\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# 선형 회귀 모델을 정의하고 학습시킵니다.\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\n\n# 테스트 데이터에 대한 예측을 수행합니다.\npredictions = model.predict(X_test)\n\n# RMSE를 계산합니다.\nrmse = np.sqrt(mean_squared_error(y_test, predictions))\nprint(\"RMSE:\", rmse)\n\nRMSE: 1.1127962148868694e-15\n\n\n\n# PyCaret에서 성능이 좋은 모델을 비교해서 선택해보세요.\n\n\nfrom pycaret.regression import *\nimport pandas as pd\n\n# 데이터 불러오기\n\n# PyCaret의 setup 함수를 사용하여 환경 설정\n# target 파라미터에는 타겟 변수의 이름을 입력합니다.\nreg_setup = setup(train, target='풍속 (m/s)')\n\n# PyCaret의 compare_models 함수를 사용하여 여러 모델을 비교\nbest_model = compare_models(fold=5, sort='RMSE')\n\n# 선택된 최적의 모델을 확인\nprint(best_model)\n\n# 최적의 모델을 튜닝 (옵션)\n\n# 튜닝된 모델을 사용하여 예측\n# 예측할 데이터가 있다면 predict_model 함수를 사용하여 예측할 수 있습니다.\n\n\n\n\n\n\n \nDescription\nValue\n\n\n\n\n0\nSession id\n1974\n\n\n1\nTarget\n풍속 (m/s)\n\n\n2\nTarget type\nRegression\n\n\n3\nOriginal data shape\n(36581, 16)\n\n\n4\nTransformed data shape\n(36581, 19)\n\n\n5\nTransformed train set shape\n(25606, 19)\n\n\n6\nTransformed test set shape\n(10975, 19)\n\n\n7\nNumeric features\n13\n\n\n8\nCategorical features\n2\n\n\n9\nPreprocess\nTrue\n\n\n10\nImputation type\nsimple\n\n\n11\nNumeric imputation\nmean\n\n\n12\nCategorical imputation\nmode\n\n\n13\nMaximum one-hot encoding\n25\n\n\n14\nEncoding method\nNone\n\n\n15\nFold Generator\nKFold\n\n\n16\nFold Number\n10\n\n\n17\nCPU Jobs\n-1\n\n\n18\nUse GPU\nFalse\n\n\n19\nLog Experiment\nFalse\n\n\n20\nExperiment Name\nreg-default-name\n\n\n21\nUSI\nbd4b\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInitiated\n. . . . . . . . . . . . . . . . . .\n20:00:43\n\n\nStatus\n. . . . . . . . . . . . . . . . . .\nFitting 5 Folds\n\n\nEstimator\n. . . . . . . . . . . . . . . . . .\nLight Gradient Boosting Machine\n\n\n\n\n\n\n\n\n\n\n\n\n \nModel\nMAE\nMSE\nRMSE\nR2\nRMSLE\nMAPE\nTT (Sec)\n\n\n\n\nknn\nK Neighbors Regressor\n0.5256\n0.5654\n0.7518\n0.7646\n0.2390\n0.4132\n0.4420\n\n\nen\nElastic Net\n1.0574\n2.0314\n1.4252\n0.1544\n0.4261\n0.9466\n0.3200\n\n\nhuber\nHuber Regressor\n1.0239\n2.0538\n1.4331\n0.1451\n0.4183\n0.8363\n0.3720\n\n\nlasso\nLasso Regression\n1.0698\n2.0834\n1.4433\n0.1328\n0.4308\n0.9661\n0.3300\n\n\nllar\nLasso Least Angle Regression\n1.0698\n2.0835\n1.4433\n0.1328\n0.4308\n0.9661\n0.3280\n\n\nomp\nOrthogonal Matching Pursuit\n1.0849\n2.1566\n1.4684\n0.1024\n0.4363\n0.9747\n0.3240\n\n\nridge\nRidge Regression\n1.1614\n2.4014\n1.5495\n0.0005\n0.4664\n1.0905\n0.3160\n\n\net\nExtra Trees Regressor\n1.1617\n2.4028\n1.5500\n-0.0001\n0.4665\n1.0909\n0.4900\n\n\nlr\nLinear Regression\n1.1618\n2.4031\n1.5501\n-0.0002\n0.4666\n1.0910\n0.6860\n\n\nbr\nBayesian Ridge\n1.1618\n2.4031\n1.5501\n-0.0002\n0.4666\n1.0910\n0.3240\n\n\ndt\nDecision Tree Regressor\n1.1619\n2.4031\n1.5501\n-0.0002\n0.4666\n1.0913\n0.3680\n\n\nrf\nRandom Forest Regressor\n1.1619\n2.4031\n1.5501\n-0.0002\n0.4666\n1.0913\n0.9720\n\n\ngbr\nGradient Boosting Regressor\n1.1625\n2.4030\n1.5501\n-0.0002\n0.4668\n1.0931\n1.9320\n\n\nada\nAdaBoost Regressor\n1.1446\n2.4146\n1.5538\n-0.0050\n0.4607\n1.0333\n0.8040\n\n\npar\nPassive Aggressive Regressor\n1.2716\n2.4564\n1.5637\n-0.0246\n0.4957\n1.3048\n0.3260\n\n\nlar\nLeast Angle Regression\n260.8634\n187716.2930\n417.9840\n-77868.3494\n3.9194\n233.4947\n0.3240\n\n\n\n\n\n\n\n\n\n# PyCaret에서 나온 성능이 좋은 모델을 Optuna를 통해 하이퍼 파라미터 최적활를 진행하세요.\n\n\nimport optuna\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\n\n# # 데이터 불러오기\n# data = pd.read_csv('train.csv')  # 파일 경로를 적절하게 수정해야 합니다.\n\n# # 타겟 변수와 특성 데이터 분리\n# X = data.drop(columns=['target_variable_name'])\n# y = data['target_variable_name']\n\n# # 훈련 세트와 검증 세트로 데이터 분리\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)\n\ndef objective(trial):\n    # K 최근접 이웃 모델에 대한 하이퍼파라미터 검색 범위 설정\n    n_neighbors = trial.suggest_int('n_neighbors', 1, 50)\n    weights = trial.suggest_categorical('weights', ['uniform', 'distance'])\n    \n    # 모델 생성 및 학습\n    model = KNeighborsRegressor(n_neighbors=n_neighbors, weights=weights)\n    model.fit(X_train, y_train)\n    \n    # 검증 세트에 대한 예측\n    y_pred = model.predict(X_valid)\n    \n    # 검증 세트에 대한 RMSE 계산\n    rmse = mean_squared_error(y_valid, y_pred, squared=False)\n    \n    return rmse\n\n# Optuna를 사용하여 하이퍼파라미터 최적화\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=100)\n\n# 최적의 하이퍼파라미터 출력\nprint(\"Best trial:\")\nbest_trial = study.best_trial\nprint(\"  Value: RMSE = {:.4f}\".format(best_trial.value))\nprint(\"  Params: \")\nfor key, value in best_trial.params.items():\n    print(\"    {}: {}\".format(key, value))\n\n[I 2024-03-28 20:27:11,078] A new study created in memory with name: no-name-00344757-fe71-4098-8fd0-c1f8daf146db\n[I 2024-03-28 20:27:11,238] Trial 0 finished with value: 0.5534046253251211 and parameters: {'n_neighbors': 15, 'weights': 'uniform'}. Best is trial 0 with value: 0.5534046253251211.\n[I 2024-03-28 20:27:11,351] Trial 1 finished with value: 0.376508570820968 and parameters: {'n_neighbors': 4, 'weights': 'distance'}. Best is trial 1 with value: 0.376508570820968.\n[I 2024-03-28 20:27:11,458] Trial 2 finished with value: 0.37092827264345585 and parameters: {'n_neighbors': 3, 'weights': 'distance'}. Best is trial 2 with value: 0.37092827264345585.\n[I 2024-03-28 20:27:11,606] Trial 3 finished with value: 0.45225704654877136 and parameters: {'n_neighbors': 12, 'weights': 'distance'}. Best is trial 2 with value: 0.37092827264345585.\n[I 2024-03-28 20:27:11,858] Trial 4 finished with value: 0.645698179220662 and parameters: {'n_neighbors': 47, 'weights': 'distance'}. Best is trial 2 with value: 0.37092827264345585.\n[I 2024-03-28 20:27:12,088] Trial 5 finished with value: 0.6125879798356881 and parameters: {'n_neighbors': 39, 'weights': 'distance'}. Best is trial 2 with value: 0.37092827264345585.\n[I 2024-03-28 20:27:12,297] Trial 6 finished with value: 0.5750838859088238 and parameters: {'n_neighbors': 31, 'weights': 'distance'}. Best is trial 2 with value: 0.37092827264345585.\n[I 2024-03-28 20:27:12,482] Trial 7 finished with value: 0.5305082277530734 and parameters: {'n_neighbors': 23, 'weights': 'distance'}. Best is trial 2 with value: 0.37092827264345585.\n[I 2024-03-28 20:27:12,673] Trial 8 finished with value: 0.6480808811501448 and parameters: {'n_neighbors': 26, 'weights': 'uniform'}. Best is trial 2 with value: 0.37092827264345585.\n[I 2024-03-28 20:27:12,845] Trial 9 finished with value: 0.5043894132866685 and parameters: {'n_neighbors': 19, 'weights': 'distance'}. Best is trial 2 with value: 0.37092827264345585.\n[I 2024-03-28 20:27:12,939] Trial 10 finished with value: 0.41802991418887503 and parameters: {'n_neighbors': 1, 'weights': 'uniform'}. Best is trial 2 with value: 0.37092827264345585.\n[I 2024-03-28 20:27:13,033] Trial 11 finished with value: 0.41802991418887503 and parameters: {'n_neighbors': 1, 'weights': 'distance'}. Best is trial 2 with value: 0.37092827264345585.\n[I 2024-03-28 20:27:13,173] Trial 12 finished with value: 0.4276459225522527 and parameters: {'n_neighbors': 9, 'weights': 'distance'}. Best is trial 2 with value: 0.37092827264345585.\n[I 2024-03-28 20:27:13,310] Trial 13 finished with value: 0.4177623784452642 and parameters: {'n_neighbors': 8, 'weights': 'distance'}. Best is trial 2 with value: 0.37092827264345585.\n[I 2024-03-28 20:27:13,432] Trial 14 finished with value: 0.42787035137259644 and parameters: {'n_neighbors': 5, 'weights': 'uniform'}. Best is trial 2 with value: 0.37092827264345585.\n[I 2024-03-28 20:27:13,599] Trial 15 finished with value: 0.48230117545845846 and parameters: {'n_neighbors': 16, 'weights': 'distance'}. Best is trial 2 with value: 0.37092827264345585.\n[I 2024-03-28 20:27:13,810] Trial 16 finished with value: 0.5750838859088238 and parameters: {'n_neighbors': 31, 'weights': 'distance'}. Best is trial 2 with value: 0.37092827264345585.\n[I 2024-03-28 20:27:13,939] Trial 17 finished with value: 0.398295014900313 and parameters: {'n_neighbors': 6, 'weights': 'distance'}. Best is trial 2 with value: 0.37092827264345585.\n[I 2024-03-28 20:27:14,088] Trial 18 finished with value: 0.5108120375493856 and parameters: {'n_neighbors': 11, 'weights': 'uniform'}. Best is trial 2 with value: 0.37092827264345585.\n[I 2024-03-28 20:27:14,349] Trial 19 finished with value: 0.6576641553723276 and parameters: {'n_neighbors': 50, 'weights': 'distance'}. Best is trial 2 with value: 0.37092827264345585.\n[I 2024-03-28 20:27:14,534] Trial 20 finished with value: 0.5179897093617714 and parameters: {'n_neighbors': 21, 'weights': 'distance'}. Best is trial 2 with value: 0.37092827264345585.\n[I 2024-03-28 20:27:14,656] Trial 21 finished with value: 0.3876907530267905 and parameters: {'n_neighbors': 5, 'weights': 'distance'}. Best is trial 2 with value: 0.37092827264345585.\n[I 2024-03-28 20:27:14,774] Trial 22 finished with value: 0.376508570820968 and parameters: {'n_neighbors': 4, 'weights': 'distance'}. Best is trial 2 with value: 0.37092827264345585.\n[I 2024-03-28 20:27:14,869] Trial 23 finished with value: 0.41802991418887503 and parameters: {'n_neighbors': 1, 'weights': 'distance'}. Best is trial 2 with value: 0.37092827264345585.\n[I 2024-03-28 20:27:15,031] Trial 24 finished with value: 0.4676808745031302 and parameters: {'n_neighbors': 14, 'weights': 'distance'}. Best is trial 2 with value: 0.37092827264345585.\n[I 2024-03-28 20:27:15,149] Trial 25 finished with value: 0.376508570820968 and parameters: {'n_neighbors': 4, 'weights': 'distance'}. Best is trial 2 with value: 0.37092827264345585.\n[I 2024-03-28 20:27:15,286] Trial 26 finished with value: 0.4731711725743855 and parameters: {'n_neighbors': 8, 'weights': 'uniform'}. Best is trial 2 with value: 0.37092827264345585.\n[I 2024-03-28 20:27:15,440] Trial 27 finished with value: 0.45225704654877136 and parameters: {'n_neighbors': 12, 'weights': 'distance'}. Best is trial 2 with value: 0.37092827264345585.\n[I 2024-03-28 20:27:15,613] Trial 28 finished with value: 0.4968196911906673 and parameters: {'n_neighbors': 18, 'weights': 'distance'}. Best is trial 2 with value: 0.37092827264345585.\n[I 2024-03-28 20:27:15,781] Trial 29 finished with value: 0.5629637795075085 and parameters: {'n_neighbors': 16, 'weights': 'uniform'}. Best is trial 2 with value: 0.37092827264345585.\n[I 2024-03-28 20:27:15,899] Trial 30 finished with value: 0.376508570820968 and parameters: {'n_neighbors': 4, 'weights': 'distance'}. Best is trial 2 with value: 0.37092827264345585.\n[I 2024-03-28 20:27:16,011] Trial 31 finished with value: 0.37092827264345585 and parameters: {'n_neighbors': 3, 'weights': 'distance'}. Best is trial 2 with value: 0.37092827264345585.\n[I 2024-03-28 20:27:16,157] Trial 32 finished with value: 0.4367506602388166 and parameters: {'n_neighbors': 10, 'weights': 'distance'}. Best is trial 2 with value: 0.37092827264345585.\n[I 2024-03-28 20:27:16,270] Trial 33 finished with value: 0.37092827264345585 and parameters: {'n_neighbors': 3, 'weights': 'distance'}. Best is trial 2 with value: 0.37092827264345585.\n[I 2024-03-28 20:27:16,366] Trial 34 finished with value: 0.41802991418887503 and parameters: {'n_neighbors': 1, 'weights': 'distance'}. Best is trial 2 with value: 0.37092827264345585.\n[I 2024-03-28 20:27:16,499] Trial 35 finished with value: 0.40826738007337976 and parameters: {'n_neighbors': 7, 'weights': 'distance'}. Best is trial 2 with value: 0.37092827264345585.\n[I 2024-03-28 20:27:16,655] Trial 36 finished with value: 0.4602494040064771 and parameters: {'n_neighbors': 13, 'weights': 'distance'}. Best is trial 2 with value: 0.37092827264345585.\n[I 2024-03-28 20:27:16,882] Trial 37 finished with value: 0.599299576538495 and parameters: {'n_neighbors': 36, 'weights': 'distance'}. Best is trial 2 with value: 0.37092827264345585.\n[I 2024-03-28 20:27:16,992] Trial 38 finished with value: 0.37092827264345585 and parameters: {'n_neighbors': 3, 'weights': 'distance'}. Best is trial 2 with value: 0.37092827264345585.\n[I 2024-03-28 20:27:17,240] Trial 39 finished with value: 0.6339572875404834 and parameters: {'n_neighbors': 44, 'weights': 'distance'}. Best is trial 2 with value: 0.37092827264345585.\n[I 2024-03-28 20:27:17,386] Trial 40 finished with value: 0.5005218364744485 and parameters: {'n_neighbors': 10, 'weights': 'uniform'}. Best is trial 2 with value: 0.37092827264345585.\n[I 2024-03-28 20:27:17,498] Trial 41 finished with value: 0.37092827264345585 and parameters: {'n_neighbors': 3, 'weights': 'distance'}. Best is trial 2 with value: 0.37092827264345585.\n[I 2024-03-28 20:27:17,611] Trial 42 finished with value: 0.37092827264345585 and parameters: {'n_neighbors': 3, 'weights': 'distance'}. Best is trial 2 with value: 0.37092827264345585.\n[I 2024-03-28 20:27:17,722] Trial 43 finished with value: 0.37092827264345585 and parameters: {'n_neighbors': 3, 'weights': 'distance'}. Best is trial 2 with value: 0.37092827264345585.\n[I 2024-03-28 20:27:17,857] Trial 44 finished with value: 0.40826738007337976 and parameters: {'n_neighbors': 7, 'weights': 'distance'}. Best is trial 2 with value: 0.37092827264345585.\n[I 2024-03-28 20:27:17,952] Trial 45 finished with value: 0.41802991418887503 and parameters: {'n_neighbors': 1, 'weights': 'distance'}. Best is trial 2 with value: 0.37092827264345585.\n[I 2024-03-28 20:27:18,148] Trial 46 finished with value: 0.5425939076222115 and parameters: {'n_neighbors': 25, 'weights': 'distance'}. Best is trial 2 with value: 0.37092827264345585.\n[I 2024-03-28 20:27:18,281] Trial 47 finished with value: 0.40826738007337976 and parameters: {'n_neighbors': 7, 'weights': 'distance'}. Best is trial 2 with value: 0.37092827264345585.\n[I 2024-03-28 20:27:18,422] Trial 48 finished with value: 0.4276459225522527 and parameters: {'n_neighbors': 9, 'weights': 'distance'}. Best is trial 2 with value: 0.37092827264345585.\n[I 2024-03-28 20:27:18,533] Trial 49 finished with value: 0.39707143394764194 and parameters: {'n_neighbors': 3, 'weights': 'uniform'}. Best is trial 2 with value: 0.37092827264345585.\n[I 2024-03-28 20:27:18,661] Trial 50 finished with value: 0.398295014900313 and parameters: {'n_neighbors': 6, 'weights': 'distance'}. Best is trial 2 with value: 0.37092827264345585.\n[I 2024-03-28 20:27:18,765] Trial 51 finished with value: 0.37275693473197474 and parameters: {'n_neighbors': 2, 'weights': 'distance'}. Best is trial 2 with value: 0.37092827264345585.\n[I 2024-03-28 20:27:18,888] Trial 52 finished with value: 0.3876907530267905 and parameters: {'n_neighbors': 5, 'weights': 'distance'}. Best is trial 2 with value: 0.37092827264345585.\n[I 2024-03-28 20:27:19,000] Trial 53 finished with value: 0.37092827264345585 and parameters: {'n_neighbors': 3, 'weights': 'distance'}. Best is trial 2 with value: 0.37092827264345585.\n[I 2024-03-28 20:27:19,212] Trial 54 finished with value: 0.5750838859088238 and parameters: {'n_neighbors': 31, 'weights': 'distance'}. Best is trial 2 with value: 0.37092827264345585.\n[I 2024-03-28 20:27:19,362] Trial 55 finished with value: 0.4442124320806514 and parameters: {'n_neighbors': 11, 'weights': 'distance'}. Best is trial 2 with value: 0.37092827264345585.\n[I 2024-03-28 20:27:19,484] Trial 56 finished with value: 0.3876907530267905 and parameters: {'n_neighbors': 5, 'weights': 'distance'}. Best is trial 2 with value: 0.37092827264345585.\n[I 2024-03-28 20:27:19,621] Trial 57 finished with value: 0.4177623784452642 and parameters: {'n_neighbors': 8, 'weights': 'distance'}. Best is trial 2 with value: 0.37092827264345585.\n[I 2024-03-28 20:27:19,716] Trial 58 finished with value: 0.41802991418887503 and parameters: {'n_neighbors': 1, 'weights': 'distance'}. Best is trial 2 with value: 0.37092827264345585.\n[I 2024-03-28 20:27:19,843] Trial 59 finished with value: 0.44441531654243593 and parameters: {'n_neighbors': 6, 'weights': 'uniform'}. Best is trial 2 with value: 0.37092827264345585.\n[I 2024-03-28 20:27:19,955] Trial 60 finished with value: 0.37092827264345585 and parameters: {'n_neighbors': 3, 'weights': 'distance'}. Best is trial 2 with value: 0.37092827264345585.\n[I 2024-03-28 20:27:20,068] Trial 61 finished with value: 0.37092827264345585 and parameters: {'n_neighbors': 3, 'weights': 'distance'}. Best is trial 2 with value: 0.37092827264345585.\n[I 2024-03-28 20:27:20,179] Trial 62 finished with value: 0.37092827264345585 and parameters: {'n_neighbors': 3, 'weights': 'distance'}. Best is trial 2 with value: 0.37092827264345585.\n[I 2024-03-28 20:27:20,320] Trial 63 finished with value: 0.4276459225522527 and parameters: {'n_neighbors': 9, 'weights': 'distance'}. Best is trial 2 with value: 0.37092827264345585.\n[I 2024-03-28 20:27:20,444] Trial 64 finished with value: 0.3876907530267905 and parameters: {'n_neighbors': 5, 'weights': 'distance'}. Best is trial 2 with value: 0.37092827264345585.\n[I 2024-03-28 20:27:20,548] Trial 65 finished with value: 0.37275693473197474 and parameters: {'n_neighbors': 2, 'weights': 'distance'}. Best is trial 2 with value: 0.37092827264345585.\n[I 2024-03-28 20:27:20,681] Trial 66 finished with value: 0.40826738007337976 and parameters: {'n_neighbors': 7, 'weights': 'distance'}. Best is trial 2 with value: 0.37092827264345585.\n[I 2024-03-28 20:27:20,805] Trial 67 finished with value: 0.3876907530267905 and parameters: {'n_neighbors': 5, 'weights': 'distance'}. Best is trial 2 with value: 0.37092827264345585.\n[I 2024-03-28 20:27:20,959] Trial 68 finished with value: 0.45225704654877136 and parameters: {'n_neighbors': 12, 'weights': 'distance'}. Best is trial 2 with value: 0.37092827264345585.\n[I 2024-03-28 20:27:21,063] Trial 69 finished with value: 0.37275693473197474 and parameters: {'n_neighbors': 2, 'weights': 'distance'}. Best is trial 2 with value: 0.37092827264345585.\n[I 2024-03-28 20:27:21,224] Trial 70 finished with value: 0.542981615632149 and parameters: {'n_neighbors': 14, 'weights': 'uniform'}. Best is trial 2 with value: 0.37092827264345585.\n[I 2024-03-28 20:27:21,342] Trial 71 finished with value: 0.376508570820968 and parameters: {'n_neighbors': 4, 'weights': 'distance'}. Best is trial 2 with value: 0.37092827264345585.\n[I 2024-03-28 20:27:21,454] Trial 72 finished with value: 0.37092827264345585 and parameters: {'n_neighbors': 3, 'weights': 'distance'}. Best is trial 2 with value: 0.37092827264345585.\n[I 2024-03-28 20:27:21,549] Trial 73 finished with value: 0.41802991418887503 and parameters: {'n_neighbors': 1, 'weights': 'distance'}. Best is trial 2 with value: 0.37092827264345585.\n[I 2024-03-28 20:27:21,676] Trial 74 finished with value: 0.398295014900313 and parameters: {'n_neighbors': 6, 'weights': 'distance'}. Best is trial 2 with value: 0.37092827264345585.\n[I 2024-03-28 20:27:21,813] Trial 75 finished with value: 0.4177623784452642 and parameters: {'n_neighbors': 8, 'weights': 'distance'}. Best is trial 2 with value: 0.37092827264345585.\n[I 2024-03-28 20:27:21,931] Trial 76 finished with value: 0.376508570820968 and parameters: {'n_neighbors': 4, 'weights': 'distance'}. Best is trial 2 with value: 0.37092827264345585.\n[I 2024-03-28 20:27:22,077] Trial 77 finished with value: 0.4367506602388166 and parameters: {'n_neighbors': 10, 'weights': 'distance'}. Best is trial 2 with value: 0.37092827264345585.\n[I 2024-03-28 20:27:22,188] Trial 78 finished with value: 0.37092827264345585 and parameters: {'n_neighbors': 3, 'weights': 'distance'}. Best is trial 2 with value: 0.37092827264345585.\n[I 2024-03-28 20:27:22,283] Trial 79 finished with value: 0.41802991418887503 and parameters: {'n_neighbors': 1, 'weights': 'distance'}. Best is trial 2 with value: 0.37092827264345585.\n[I 2024-03-28 20:27:22,412] Trial 80 finished with value: 0.398295014900313 and parameters: {'n_neighbors': 6, 'weights': 'distance'}. Best is trial 2 with value: 0.37092827264345585.\n[I 2024-03-28 20:27:22,523] Trial 81 finished with value: 0.37092827264345585 and parameters: {'n_neighbors': 3, 'weights': 'distance'}. Best is trial 2 with value: 0.37092827264345585.\n[I 2024-03-28 20:27:22,748] Trial 82 finished with value: 0.599299576538495 and parameters: {'n_neighbors': 36, 'weights': 'distance'}. Best is trial 2 with value: 0.37092827264345585.\n[I 2024-03-28 20:27:22,853] Trial 83 finished with value: 0.37275693473197474 and parameters: {'n_neighbors': 2, 'weights': 'distance'}. Best is trial 2 with value: 0.37092827264345585.\n[I 2024-03-28 20:27:22,972] Trial 84 finished with value: 0.376508570820968 and parameters: {'n_neighbors': 4, 'weights': 'distance'}. Best is trial 2 with value: 0.37092827264345585.\n[I 2024-03-28 20:27:23,106] Trial 85 finished with value: 0.40826738007337976 and parameters: {'n_neighbors': 7, 'weights': 'distance'}. Best is trial 2 with value: 0.37092827264345585.\n[I 2024-03-28 20:27:23,224] Trial 86 finished with value: 0.376508570820968 and parameters: {'n_neighbors': 4, 'weights': 'distance'}. Best is trial 2 with value: 0.37092827264345585.\n[I 2024-03-28 20:27:23,330] Trial 87 finished with value: 0.388867774568847 and parameters: {'n_neighbors': 2, 'weights': 'uniform'}. Best is trial 2 with value: 0.37092827264345585.\n[I 2024-03-28 20:27:23,474] Trial 88 finished with value: 0.4276459225522527 and parameters: {'n_neighbors': 9, 'weights': 'distance'}. Best is trial 2 with value: 0.37092827264345585.\n[I 2024-03-28 20:27:23,667] Trial 89 finished with value: 0.5305082277530734 and parameters: {'n_neighbors': 23, 'weights': 'distance'}. Best is trial 2 with value: 0.37092827264345585.\n[I 2024-03-28 20:27:23,874] Trial 90 finished with value: 0.5598097954704512 and parameters: {'n_neighbors': 28, 'weights': 'distance'}. Best is trial 2 with value: 0.37092827264345585.\n[I 2024-03-28 20:27:23,989] Trial 91 finished with value: 0.37092827264345585 and parameters: {'n_neighbors': 3, 'weights': 'distance'}. Best is trial 2 with value: 0.37092827264345585.\n[I 2024-03-28 20:27:24,124] Trial 92 finished with value: 0.398295014900313 and parameters: {'n_neighbors': 6, 'weights': 'distance'}. Best is trial 2 with value: 0.37092827264345585.\n[I 2024-03-28 20:27:24,248] Trial 93 finished with value: 0.3876907530267905 and parameters: {'n_neighbors': 5, 'weights': 'distance'}. Best is trial 2 with value: 0.37092827264345585.\n[I 2024-03-28 20:27:24,344] Trial 94 finished with value: 0.41802991418887503 and parameters: {'n_neighbors': 1, 'weights': 'distance'}. Best is trial 2 with value: 0.37092827264345585.\n[I 2024-03-28 20:27:24,457] Trial 95 finished with value: 0.37092827264345585 and parameters: {'n_neighbors': 3, 'weights': 'distance'}. Best is trial 2 with value: 0.37092827264345585.\n[I 2024-03-28 20:27:24,562] Trial 96 finished with value: 0.37275693473197474 and parameters: {'n_neighbors': 2, 'weights': 'distance'}. Best is trial 2 with value: 0.37092827264345585.\n[I 2024-03-28 20:27:24,686] Trial 97 finished with value: 0.3876907530267905 and parameters: {'n_neighbors': 5, 'weights': 'distance'}. Best is trial 2 with value: 0.37092827264345585.\n[I 2024-03-28 20:27:24,932] Trial 98 finished with value: 0.6339572875404834 and parameters: {'n_neighbors': 44, 'weights': 'distance'}. Best is trial 2 with value: 0.37092827264345585.\n[I 2024-03-28 20:27:25,066] Trial 99 finished with value: 0.45935756087512386 and parameters: {'n_neighbors': 7, 'weights': 'uniform'}. Best is trial 2 with value: 0.37092827264345585.\n\n\nBest trial:\n  Value: RMSE = 0.3709\n  Params: \n    n_neighbors: 3\n    weights: distance\n\n\n\n### 앙상블 기법 중 부스팅을 활용해서 성능을 올려보세요.(AdaBoostRegressor)\n\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\n\n# 데이터 불러오기\n\n# 모델 생성 및 학습\nmodel = AdaBoostRegressor()\nmodel.fit(X_train, y_train)\n\n# 검증 세트에 대한 예측\ny_pred = model.predict(X_valid)\n\n# 검증 세트에 대한 RMSE 계산\nrmse = mean_squared_error(y_valid, y_pred, squared=False)\nprint(\"Validation RMSE:\", rmse)\n\nValidation RMSE: 0.14080009612347158"
  },
  {
    "objectID": "posts/ANN(1) - ANN's Structure.html",
    "href": "posts/ANN(1) - ANN's Structure.html",
    "title": "ANN(1) - ANN의 구조",
    "section": "",
    "text": "test\n\n\n\nInput Layer, Hidden Layer, Output Layer 3 단계로 나눌 수 있습니다.\nInput Layer : n개의 입력값을 받는 층(layer)입니다.\nHidden Layer : 입력과 출력사이의 복잡한 관계를 학습하는 층(layer)입니다.\nOutput Layer : 네트워크의 최종 출력을 생성하는 층(layer)입니다.\nn개의 변수(\\(x_1, x_2, ... ,x_n\\))가 입력되어 m개의 값(\\(y_1, y_2, ... y_m\\))출력되는 함수라고 생각해도 좋을 것 같습니다.\\(f(x_1, x_2, ... ,x_n) = (y_1, y_2, ... ,y_m)\\)\n\n그림에는 Output이 n개지만 꼭 입력값과 같을 필요는 없습니다. (실제로는 \\(P(n=m)\\)의 값이 매우 낮습니다.)\n\n\n\n\n\n\n\n\n노드\n\n\n\n노드 구조는 다음과 같습니다.\n\n\n\\(x_k(k=0,1,2)\\) : 이전 층(혹은 노드)에서 나온 출력값입니다.\n\\(w_k(k=0,1,2)\\) : x_k의 가중치입니다.\n\\(b\\) : 노드에 사용되는 bias(편향치)입니다.\n\\(y\\) : 노드의 출력값입니다.\n\n\n계산 식은 다음과 같습니다 : \\(\\displaystyle\\sum\\limits_{i=0}^{2}(x_0w_0) + b = y\\)\n노드가 여러개 모이면 한개의 층(layer)을 형성합니다.\n\n\n\n\n\n\n\nFully Connected Layer\n\n\n\n직역 그대로 완전히 연결되어 있는 층(layer)이라고 생각하시면 됩니다.\n그림과 같이 각 층(layer)에 있는 노드가 다음 층에 있는 노드와 빠짐없이 전부 연결되어 있습니다.\n\\(i\\)번째 층(layer)에 있는 노드가 \\(p\\)개 \\(i+1\\)번째 층(layer)에 있는 노드가 \\(q\\)개라면, \\(i\\)번째 층(layer)과 \\(i+1\\)번째 층(layer)을 잇는 선은 총 \\(p^q\\)개입니다.\n\n\n\n\n\n\n\nforward propagation.jpg\n\n\n※아무거나 사진 퍼오기가 무서워서 공부할겸 직접 그려봤습니다..\n\n편의상 \\(i\\)번째 층의 각각의 노드에 들어가는 입력값을 \\(x_1, x_2, ... x_n\\)이라고 하겠습니다.\n입력값들은 노드를 지나칠 때마다 가중치를 곱하고 편향(bias)를 더합니다.\n\nk번째 노드에서의 계산은 다음과 같습니다. \\(x_kw_k+b_k\\)\n\nFully Connected Layer에 의해서 \\(i+1\\)번째 층(layer)에서 각각의 노드는 입력값을 벡터로 받게 됩니다 : \\(\\displaystyle\\overrightarrow{a^1} = \\displaystyle\\left(a^1_1 a^1_2 ... a^1_n \\right)\\)\n\\(i+1\\)번째 층(layer)의 \\(k\\)번째 노드에서의 연산은 다음과 같습니다.\n\n\\(\\displaystyle\\overrightarrow{a_k}(\\overrightarrow{w_k})^T + b^1_k\\)\n\n각 노드에서는 가중치(weight)곱과 편향(bias)합 연산이 이루어지고, Activation Function을 거치게 됩니다.\n\n\n\n\n\n\n\nHidden Layer에서 여러 가중치(weight)와 편향(bias)의 연산을 마치고 난 값들은 Output Layer의 입력값으로 들어가게 됩니다.\nOutput Layer에서도 여러 연산이 있지만 Loss Function을 통해서 예측값과 정답의 차이를 계산하게 됩니다.\n이때 예측값과 정답의 차이 즉, Loss를 줄여나가야 효과적인 딥러닝 모델이라고 볼 수 있겠죠?\nLoss를 줄이는 방법으로 미분이 사용됩니다.\n\n\n\n\n역전파1.jpg\n\n\n\n아이디어는 다음과 같습니다.\n\n\n쉬운 예로 Loss Function을 다음과 같이 정의합니다 : \\(f(x)=2x^2\\)\n그림과 같이 Loss Function을 미분하여 기울기를 얻으면, -(기울기)가 곧 \\(f(x)\\)의 최소값으로 향하는 방향이 됩니다.\n\\(f'(1)=4\\)이므로 -4는 \\(f(x)\\)가 최소로 되는 방향을 가르킵니다. 실제로 \\(1-f'(1)\\)을 하게 되면 최소가 있는 \\(x=0\\)방향으로 이동합니다.\n따라서 \\(x\\)를 다음과 같이 업데이트 할 수 있습니다 : \\(x \\to x-f'(x)\\)\n\n\n\n\n하지만 \\(x \\to x-f'(x)\\)로 업데이트 할 경우 다음과 같은 문제가 있습니다. \\(x \\to x-f'(x) = -3x\\)\n\n\\(x=1)\\)\n\\(1 \\to -3\\)\n\n\\(x=-3)\\)\n$ -3 (-3)(-3)=9\\(\\\n\\\n\\)x=9)$\n\\(9 \\to (-3)9 = -27\\)\n즉 \\(x\\)값은 \\(f(x)\\)가 최소인 \\(x\\)로 수렴하지 못하고 발산하게 됩니다.\n이는 learning rate(학습률)로 해결할 수 있습니다.\n\n\n\n\n역전파2.jpg\n\n\n\\(x \\to x - \\alpha f'(x)(\\alpha = 0.1)\\)\n\\(x \\to x - 0.1f'(x)\\) \\(x \\to 0.6x\\)\n\n\\(x=1)\\)\n\\(1 \\to 0.6\\)\n\n\\(x=0.6)\\)\n\\(0.6 \\to 0.36\\)\n- 이렇게 \\(\\alpha\\)(learning rate)를 곱해주면 앞서 발산하는 문제를 해결할 수 있습니다.\n- 경사를 따라 내려온다고 하여 경사하강법으로도 불립니다.(Gradient descent)\n\n\n\noutput layer.jpg\n\n\n- Output Layer에 있는 Loss Function을 시각화한 그림입니다.\n\n실제 Loss function은 앞선 2차식처럼 단순하지 않습니다.\nHidden Layer의 Layer 수와 각 Layer의 노드 수, 그리고 입력값에 따라 영향을 받습니다.\n따라서 Loss Function을 가중치(weight)와 편향(bias)에 대해 편미분을 해줘야 합니다.\n그렇게 여러 차례 미분해서 Loss Function을 최소화 시켜줍니다.\n\n- Forward Propagation이 Input Layer에서 Output Layer의 방향으로 계산된다면,\n- Back Propagation은 Output Layer에서 Input Layer방향으로 계산됩니다.\n\n\n\n\n\nLeCun Initialization\n\\(w \\sim U(- \\sqrt{\\displaystyle\\frac{3}{N_{in}}}, \\sqrt{\\displaystyle\\frac{3}{N_{in}}})\\) or \\(w \\sim N(0, \\displaystyle\\frac{1}{N_{in}})\\) $\nXavier(Sigmoid/tanh 사용하는 신경망)\n\\(w \\sim U\\left(-\\sqrt{\\displaystyle\\frac{3}{N_{\\text{in}} + N_{\\text{out}}}}, \\sqrt{\\displaystyle\\frac{3}{N_{\\text{in}} + N_{\\text{out}}}}\\right)\\) or \\(w \\sim U\\left(0, \\displaystyle\\frac{2}{N_{\\text{in}} + N_{\\text{out}}}\\right)\\)\nHe(ReLU 사용하는 신경망)\n\\(w \\sim U\\left(-\\sqrt{\\displaystyle\\frac{6}{N_{\\text{in}}}}, \\sqrt{\\displaystyle\\frac{6}{N_{\\text{in}}}}\\right)\\) or \\(w \\sim \\left(0, \\displaystyle\\frac{2}{N_{\\text{in}}}\\right)\\)\n\n\n\\(N_{\\text{in}}\\) : 이전 layer 노드의 수\n\\(N_{\\text{out}}\\) : 이후 layer 노드의 수\n\n\n\n\n- 노드는 Affine Function과 Activation Function으로 이루어져있습니다.\n\n\n\nNode.jpg\n\n\n- Affine Function * 입력값을 \\(x\\), Affine Function을 \\(f(x)\\)라고 정의할 때, 다음과 같습니다.\n\\(f(x) = xw + b = z\\)\n- Activation Function * 계산된 \\(z\\)값을 Activation Function \\(g(z)\\)에 넣어줍니다 : \\(g(z) = a\\) * Activation Function의 종류는 크게 세가지로 나눌 수 있습니다.\n\\(\\Rightarrow\\) Sigmoid, tanh, ReLu\n\n\n\\(Sigmoid(z) = \\displaystyle\\frac{1}{1+e^{-z}}\\)\n\n\n\nsigmoid\n\n\n\n\n\n\\(tanh(z) = \\displaystyle\\frac{e^{z}-e^{-z}}{e^{z}+e^{-z}}\\)\n\n\n\n스크린샷 2024-03-22 160148.png\n\n\n- Activation Function을 사용해야 하는 이유 * 기본적으로 Affine Function은 lienar function이므로 Affine Function만 계속해서 늘려간다면, Hidden Layer가 아무리 깊어도 학습효과를 누리지 못함."
  },
  {
    "objectID": "posts/ANN(1) - ANN's Structure.html#ann1---ann의-구조",
    "href": "posts/ANN(1) - ANN's Structure.html#ann1---ann의-구조",
    "title": "ANN(1) - ANN의 구조",
    "section": "",
    "text": "test\n\n\n\nInput Layer, Hidden Layer, Output Layer 3 단계로 나눌 수 있습니다.\nInput Layer : n개의 입력값을 받는 층(layer)입니다.\nHidden Layer : 입력과 출력사이의 복잡한 관계를 학습하는 층(layer)입니다.\nOutput Layer : 네트워크의 최종 출력을 생성하는 층(layer)입니다.\nn개의 변수(\\(x_1, x_2, ... ,x_n\\))가 입력되어 m개의 값(\\(y_1, y_2, ... y_m\\))출력되는 함수라고 생각해도 좋을 것 같습니다.\\(f(x_1, x_2, ... ,x_n) = (y_1, y_2, ... ,y_m)\\)\n\n그림에는 Output이 n개지만 꼭 입력값과 같을 필요는 없습니다. (실제로는 \\(P(n=m)\\)의 값이 매우 낮습니다.)\n\n\n\n\n\n\n\n\n노드\n\n\n\n노드 구조는 다음과 같습니다.\n\n\n\\(x_k(k=0,1,2)\\) : 이전 층(혹은 노드)에서 나온 출력값입니다.\n\\(w_k(k=0,1,2)\\) : x_k의 가중치입니다.\n\\(b\\) : 노드에 사용되는 bias(편향치)입니다.\n\\(y\\) : 노드의 출력값입니다.\n\n\n계산 식은 다음과 같습니다 : \\(\\displaystyle\\sum\\limits_{i=0}^{2}(x_0w_0) + b = y\\)\n노드가 여러개 모이면 한개의 층(layer)을 형성합니다.\n\n\n\n\n\n\n\nFully Connected Layer\n\n\n\n직역 그대로 완전히 연결되어 있는 층(layer)이라고 생각하시면 됩니다.\n그림과 같이 각 층(layer)에 있는 노드가 다음 층에 있는 노드와 빠짐없이 전부 연결되어 있습니다.\n\\(i\\)번째 층(layer)에 있는 노드가 \\(p\\)개 \\(i+1\\)번째 층(layer)에 있는 노드가 \\(q\\)개라면, \\(i\\)번째 층(layer)과 \\(i+1\\)번째 층(layer)을 잇는 선은 총 \\(p^q\\)개입니다.\n\n\n\n\n\n\n\nforward propagation.jpg\n\n\n※아무거나 사진 퍼오기가 무서워서 공부할겸 직접 그려봤습니다..\n\n편의상 \\(i\\)번째 층의 각각의 노드에 들어가는 입력값을 \\(x_1, x_2, ... x_n\\)이라고 하겠습니다.\n입력값들은 노드를 지나칠 때마다 가중치를 곱하고 편향(bias)를 더합니다.\n\nk번째 노드에서의 계산은 다음과 같습니다. \\(x_kw_k+b_k\\)\n\nFully Connected Layer에 의해서 \\(i+1\\)번째 층(layer)에서 각각의 노드는 입력값을 벡터로 받게 됩니다 : \\(\\displaystyle\\overrightarrow{a^1} = \\displaystyle\\left(a^1_1 a^1_2 ... a^1_n \\right)\\)\n\\(i+1\\)번째 층(layer)의 \\(k\\)번째 노드에서의 연산은 다음과 같습니다.\n\n\\(\\displaystyle\\overrightarrow{a_k}(\\overrightarrow{w_k})^T + b^1_k\\)\n\n각 노드에서는 가중치(weight)곱과 편향(bias)합 연산이 이루어지고, Activation Function을 거치게 됩니다.\n\n\n\n\n\n\n\nHidden Layer에서 여러 가중치(weight)와 편향(bias)의 연산을 마치고 난 값들은 Output Layer의 입력값으로 들어가게 됩니다.\nOutput Layer에서도 여러 연산이 있지만 Loss Function을 통해서 예측값과 정답의 차이를 계산하게 됩니다.\n이때 예측값과 정답의 차이 즉, Loss를 줄여나가야 효과적인 딥러닝 모델이라고 볼 수 있겠죠?\nLoss를 줄이는 방법으로 미분이 사용됩니다.\n\n\n\n\n역전파1.jpg\n\n\n\n아이디어는 다음과 같습니다.\n\n\n쉬운 예로 Loss Function을 다음과 같이 정의합니다 : \\(f(x)=2x^2\\)\n그림과 같이 Loss Function을 미분하여 기울기를 얻으면, -(기울기)가 곧 \\(f(x)\\)의 최소값으로 향하는 방향이 됩니다.\n\\(f'(1)=4\\)이므로 -4는 \\(f(x)\\)가 최소로 되는 방향을 가르킵니다. 실제로 \\(1-f'(1)\\)을 하게 되면 최소가 있는 \\(x=0\\)방향으로 이동합니다.\n따라서 \\(x\\)를 다음과 같이 업데이트 할 수 있습니다 : \\(x \\to x-f'(x)\\)\n\n\n\n\n하지만 \\(x \\to x-f'(x)\\)로 업데이트 할 경우 다음과 같은 문제가 있습니다. \\(x \\to x-f'(x) = -3x\\)\n\n\\(x=1)\\)\n\\(1 \\to -3\\)\n\n\\(x=-3)\\)\n$ -3 (-3)(-3)=9\\(\\\n\\\n\\)x=9)$\n\\(9 \\to (-3)9 = -27\\)\n즉 \\(x\\)값은 \\(f(x)\\)가 최소인 \\(x\\)로 수렴하지 못하고 발산하게 됩니다.\n이는 learning rate(학습률)로 해결할 수 있습니다.\n\n\n\n\n역전파2.jpg\n\n\n\\(x \\to x - \\alpha f'(x)(\\alpha = 0.1)\\)\n\\(x \\to x - 0.1f'(x)\\) \\(x \\to 0.6x\\)\n\n\\(x=1)\\)\n\\(1 \\to 0.6\\)\n\n\\(x=0.6)\\)\n\\(0.6 \\to 0.36\\)\n- 이렇게 \\(\\alpha\\)(learning rate)를 곱해주면 앞서 발산하는 문제를 해결할 수 있습니다.\n- 경사를 따라 내려온다고 하여 경사하강법으로도 불립니다.(Gradient descent)\n\n\n\noutput layer.jpg\n\n\n- Output Layer에 있는 Loss Function을 시각화한 그림입니다.\n\n실제 Loss function은 앞선 2차식처럼 단순하지 않습니다.\nHidden Layer의 Layer 수와 각 Layer의 노드 수, 그리고 입력값에 따라 영향을 받습니다.\n따라서 Loss Function을 가중치(weight)와 편향(bias)에 대해 편미분을 해줘야 합니다.\n그렇게 여러 차례 미분해서 Loss Function을 최소화 시켜줍니다.\n\n- Forward Propagation이 Input Layer에서 Output Layer의 방향으로 계산된다면,\n- Back Propagation은 Output Layer에서 Input Layer방향으로 계산됩니다.\n\n\n\n\n\nLeCun Initialization\n\\(w \\sim U(- \\sqrt{\\displaystyle\\frac{3}{N_{in}}}, \\sqrt{\\displaystyle\\frac{3}{N_{in}}})\\) or \\(w \\sim N(0, \\displaystyle\\frac{1}{N_{in}})\\) $\nXavier(Sigmoid/tanh 사용하는 신경망)\n\\(w \\sim U\\left(-\\sqrt{\\displaystyle\\frac{3}{N_{\\text{in}} + N_{\\text{out}}}}, \\sqrt{\\displaystyle\\frac{3}{N_{\\text{in}} + N_{\\text{out}}}}\\right)\\) or \\(w \\sim U\\left(0, \\displaystyle\\frac{2}{N_{\\text{in}} + N_{\\text{out}}}\\right)\\)\nHe(ReLU 사용하는 신경망)\n\\(w \\sim U\\left(-\\sqrt{\\displaystyle\\frac{6}{N_{\\text{in}}}}, \\sqrt{\\displaystyle\\frac{6}{N_{\\text{in}}}}\\right)\\) or \\(w \\sim \\left(0, \\displaystyle\\frac{2}{N_{\\text{in}}}\\right)\\)\n\n\n\\(N_{\\text{in}}\\) : 이전 layer 노드의 수\n\\(N_{\\text{out}}\\) : 이후 layer 노드의 수\n\n\n\n\n- 노드는 Affine Function과 Activation Function으로 이루어져있습니다.\n\n\n\nNode.jpg\n\n\n- Affine Function * 입력값을 \\(x\\), Affine Function을 \\(f(x)\\)라고 정의할 때, 다음과 같습니다.\n\\(f(x) = xw + b = z\\)\n- Activation Function * 계산된 \\(z\\)값을 Activation Function \\(g(z)\\)에 넣어줍니다 : \\(g(z) = a\\) * Activation Function의 종류는 크게 세가지로 나눌 수 있습니다.\n\\(\\Rightarrow\\) Sigmoid, tanh, ReLu\n\n\n\\(Sigmoid(z) = \\displaystyle\\frac{1}{1+e^{-z}}\\)\n\n\n\nsigmoid\n\n\n\n\n\n\\(tanh(z) = \\displaystyle\\frac{e^{z}-e^{-z}}{e^{z}+e^{-z}}\\)\n\n\n\n스크린샷 2024-03-22 160148.png\n\n\n- Activation Function을 사용해야 하는 이유 * 기본적으로 Affine Function은 lienar function이므로 Affine Function만 계속해서 늘려간다면, Hidden Layer가 아무리 깊어도 학습효과를 누리지 못함."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "AI_theory",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nMay 3, 2024\n\n\nCNN(Convolutional Neural Network)\n\n\n이정재 \n\n\n\n\nApr 5, 2024\n\n\nJBIG코드실습_타이타닉(4주차)\n\n\n이정재 \n\n\n\n\nApr 5, 2024\n\n\nANN(3) - Overfitting\n\n\n이정재 \n\n\n\n\nMar 29, 2024\n\n\nANN(2) - Vanishing Gradient & Optimization\n\n\n이정재 \n\n\n\n\nMar 24, 2024\n\n\nJBIG코드실습_타이타닉(2주차)\n\n\n이정재 \n\n\n\n\nMar 24, 2024\n\n\n랜덤포레스트(배깅,보팅)\n\n\n이정재 \n\n\n\n\nMar 24, 2024\n\n\n로지스틱회귀\n\n\n이정재 \n\n\n\n\nMar 24, 2024\n\n\n의사결정나무\n\n\n이정재 \n\n\n\n\nMar 24, 2024\n\n\nJBIG코드실습_타이타닉(1주차)\n\n\n이정재 \n\n\n\n\nMar 22, 2024\n\n\nANN(1) - ANN의 구조\n\n\n이정재 \n\n\n\n\nMar 16, 2024\n\n\nGBM\n\n\n이정재 \n\n\n\n\n\nNo matching items"
  }
]