[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/GBM.html",
    "href": "posts/GBM.html",
    "title": "GBM",
    "section": "",
    "text": "- 다음은 wikipedia의 Gradient Boosting Algorithm입니다. 다음의 수식을 먼저 읽고 최대한 이해하려고 노력해봅시다.\n\n\n\n\n\n\n\n\n1. initialize model with a constant value:\n\\(F_0(x) = \\underset{\\gamma}{\\arg\\min}\\sum_{i=1}^n L(y_i, \\gamma)\\)\n\n\n2. For m = 1 to M : 1. Compute so-called pseudo-residuals\n\\(r_{im} = - \\Bigg[\\displaystyle\\frac{\\partial L(y_i, F(x_i))}{\\partial F(x_i)}\\Bigg]_{F(x) = F_{m-1}(x)}\\) for \\(i = 1, ... ,n\\) 2. Fit a base learner (or weak learner, e.g. tree) closed under scaling \\(h_m(x)\\) to pseuo-residuals, i.e. train it using the training set \\(\\{(x_i, r_{im})\\}_{i=1}^{n}\\) 3. Compute multiplier \\(\\gamma_{m}\\) by solving the following one-dimensional optimization problem:\n\\(\\gamma_m = \\underset{\\gamma}{\\arg\\min}\\sum_{i=1}^n L(y_i, F_{m-1}(x_i) + \\gamma h_m(x_i))\\) 4. Update the model:\n\\(F_m(x) = F_{m-1}(x) + \\gamma_m h_m(x)\\)\n\n\n3. Output \\(\\hat{f(x)} = F_M(x)\\)\n\n\n\n수식으로만 완전히 이해하기가 어렵기 때문에 간단한 예시와 pandas dataframe을 이용하여 설명해보겠습니다.\n다음과 같은 데이터가 있다고 가정해보겠습니다.\n\nimport pandas as pd\nidx = ['학생1', '학생2', '학생3', '학생4']\ndata = pd.DataFrame({'공부시간(m)':[150, 120, 60, 80], '성별':['여','남','남', '여'], '수학점수':[90, 65, 55, '???']},index=idx)\ndata\n\n\n\n\n\n\n\n\n공부시간(m)\n성별\n수학점수\n\n\n\n\n학생1\n150\n여\n90\n\n\n학생2\n120\n남\n65\n\n\n학생3\n60\n남\n55\n\n\n학생4\n80\n여\n???\n\n\n\n\n\n\n\n다음과 같은 데이터가 있다고 할 때, 저희는 학생4의 수학점수를 공부시간(m)을 통하여 예측하고 싶습니다.\n\n\n\n\n\n1. initialize model with a constant value:\\\n\n\n\\(F_0(x) = \\underset{\\gamma}{\\arg\\min}\\sum_{i=1}^n L(y_i, \\gamma)\\)\n\n\n\n\n초기모델을 상수로 정의합니다. 여기서 상수는 \\(y_i\\)와 \\(\\gamma\\)입니다.\n\\(y_i\\)는 수학점수를 의미하고 \\(\\gamma\\)는 초기 예측값입니다.\n\n\\(y_1\\) = 90, \\(y_2\\) = 65, \\(y_3\\) = 60\n\n여기서의 Loss Function \\(L(y_i, \\gamma) = \\displaystyle\\frac1n \\sum_{i=1}^{n} (y_i - \\gamma)^2\\)입니다. (잘 모르시는 분들은 MSE 학습하시면 좋을 것 같습니다.)\n예시의 수치를 대입하면\n\\(L(y_i, \\gamma) = \\displaystyle\\frac13 \\sum_{i=1}^{3} (y_i - \\gamma)^2 =  \\displaystyle\\frac13(90 - \\gamma)^2 + \\displaystyle\\frac13(65 - \\gamma)^2 + \\displaystyle\\frac13(55 - \\gamma)^2\\)입니다.\n\\(L(y_i, \\gamma)\\)가 최소가 되는 \\(\\gamma\\)를 찾아야 하므로 \\(\\displaystyle\\frac{\\partial L}{\\partial \\gamma} = \\displaystyle\\frac23(\\gamma - 90) + \\displaystyle\\frac23(\\gamma - 65) + \\displaystyle\\frac23(\\gamma - 55) = \\displaystyle\\frac23(3\\gamma - 210)\\)입니다.\n\\(\\displaystyle\\frac{\\partial L}{\\partial \\gamma} = 0\\)이 되어야 하므로 \\(\\gamma\\) = 70입니다. 따라서 초기 예측값 \\(\\gamma\\)는 70이 됩니다.\n초기 예측값을 이용한 첫번째 잔차는 각각 20, -5, -15가 됩니다.\n\n\nimport pandas as pd\nidx = ['Student1', 'Student2', 'Student3']\ndata = pd.DataFrame({'StudyTime(m)':[150, 120, 60], 'Sex':['female','male','male'], \n                     'MathScore':[90, 65, 55], 'F_0':[70 for i in range(3)],'r1': [20, -5, -15]},index=idx)\ndata\n\n\n\n\n\n\n\n\nStudyTime(m)\nSex\nMathScore\nF_0\nr1\n\n\n\n\nStudent1\n150\nfemale\n90\n70\n20\n\n\nStudent2\n120\nmale\n65\n70\n-5\n\n\nStudent3\n60\nmale\n55\n70\n-15\n\n\n\n\n\n\n\n- 위의 데이터를 의사결정나무로 시각화를 하면 다음과 같습니다.\n\nimport sklearn.tree\n## step1\nx = data[['StudyTime(m)']]\ny = data['MathScore']\n\n## step2\npredictr = sklearn.tree.DecisionTreeRegressor(max_depth=1)\n\n## step3\npredictr.fit(x,y)\n\n## visualization\nsklearn.tree.plot_tree(predictr,feature_names=x.columns);\n\n\n\n\n\n\n\n\n\n\n\n\nFor \\(m\\) = 1 to \\(M\\):\n\nCompute so-called pseudo-residuals \\(r_{im} = - \\Bigg[\\displaystyle\\frac{\\partial L(y_i, F(x_i))}{\\partial F(x_i)}\\Bigg]_{F(x) = F_{m-1}(x)}\\) for \\(i = 1, ... ,n\\)\nFit a base learner (or weak learner, e.g. tree) closed under scaling \\(h_m(x)\\) to pseuo-residuals, i.e. train it using the training set \\(\\{(x_i, r_{im})\\}_{i=1}^{n}\\)\nCompute multiplier \\(\\gamma_{m}\\) by solving the following one-dimensional optimization problem:\n\\(\\gamma_m = \\underset{\\gamma}{\\arg\\min}\\sum_{i=1}^n L(y_i, F_{m-1}(x_i) + \\gamma h_m(x_i))\\)\nUpdate the model:\n\\(F_m(x) = F_{m-1}(x) + \\gamma_m h_m(x)\\)\n\nA단계부터 D단계를 총 M번 반복하는 단계입니다.\n여기서 \\(M\\)은 예측기의 개수, \\(n\\)은 데이터의 개수입니다.\n예측기에 번호를 붙여 \\(m\\)=1인 경우는 1번 예측기 \\(m=M\\)인 경우는 M번 예측기라고 부르겠습니다.\n1번 예측기(\\(m=1\\))가 생성 되는 과정을 살펴보겠습니다. \\(L(y_i,F(x_i)) = \\displaystyle\\frac12(y_i - F(x_i))^2\\)이므로\n\n\\(r_{i1} = - \\Bigg[\\displaystyle\\frac{\\partial L(y_i, F(x_i))}{\\partial F(x_i)}\\Bigg]_{F(x) = F_{0}(x)} = y_i - F_0(x_i)\\)입니다. (잔차와 식이 동일합니다.)\n0번 예측기 \\(F_0(x)\\)는 1단계의 초기모델에서 생성되었습니다. 따라서 \\(F_0(x)=70\\)입니다.(1단계 참고)\n따라서 \\(r_{11}=20\\), \\(r_{21}=-5\\), \\(r_{31}=-15\\)입니다. 1단계 표에 있는 잔차1과 같습니다.\n함수\\(h_1(x)\\)는 앞서 계산한 잔차를 트리 계열로 학습합니다.(의사 결정 나무에 대해서는 추후에 업로드 하겠습니다.)\n모든 데이터(\\(n=3\\))에 대해서 \\(\\sum_{i=1}^{3} L(y_i, F_0(x_i) + \\gamma h_m(x_i)\\) 식이 최소가 되는 \\(\\gamma\\)(learning rate)를 찾습니다.(\\(\\gamma = \\gamma_{1}\\))\n모델을 업데이트 합니다 : \\(F_1(x) = F_0(x) + \\gamma_1 h_1(x)\\)\n\n4번의 과정을 M번 반복합니다.\n\n- 추가설명\n\n\n\n\n\n\n3. Output \\(\\hat{f(x)} = F_M(x)\\)\n\n\n\n\n최종 모델을 산출합니다."
  },
  {
    "objectID": "posts/GBM.html#algorithm",
    "href": "posts/GBM.html#algorithm",
    "title": "GBM",
    "section": "",
    "text": "1. initialize model with a constant value:\n\\(F_0(x) = \\underset{\\gamma}{\\arg\\min}\\sum_{i=1}^n L(y_i, \\gamma)\\)\n\n\n2. For m = 1 to M : 1. Compute so-called pseudo-residuals\n\\(r_{im} = - \\Bigg[\\displaystyle\\frac{\\partial L(y_i, F(x_i))}{\\partial F(x_i)}\\Bigg]_{F(x) = F_{m-1}(x)}\\) for \\(i = 1, ... ,n\\) 2. Fit a base learner (or weak learner, e.g. tree) closed under scaling \\(h_m(x)\\) to pseuo-residuals, i.e. train it using the training set \\(\\{(x_i, r_{im})\\}_{i=1}^{n}\\) 3. Compute multiplier \\(\\gamma_{m}\\) by solving the following one-dimensional optimization problem:\n\\(\\gamma_m = \\underset{\\gamma}{\\arg\\min}\\sum_{i=1}^n L(y_i, F_{m-1}(x_i) + \\gamma h_m(x_i))\\) 4. Update the model:\n\\(F_m(x) = F_{m-1}(x) + \\gamma_m h_m(x)\\)\n\n\n3. Output \\(\\hat{f(x)} = F_M(x)\\)\n\n\n\n수식으로만 완전히 이해하기가 어렵기 때문에 간단한 예시와 pandas dataframe을 이용하여 설명해보겠습니다.\n다음과 같은 데이터가 있다고 가정해보겠습니다.\n\nimport pandas as pd\nidx = ['학생1', '학생2', '학생3', '학생4']\ndata = pd.DataFrame({'공부시간(m)':[150, 120, 60, 80], '성별':['여','남','남', '여'], '수학점수':[90, 65, 55, '???']},index=idx)\ndata\n\n\n\n\n\n\n\n\n공부시간(m)\n성별\n수학점수\n\n\n\n\n학생1\n150\n여\n90\n\n\n학생2\n120\n남\n65\n\n\n학생3\n60\n남\n55\n\n\n학생4\n80\n여\n???\n\n\n\n\n\n\n\n다음과 같은 데이터가 있다고 할 때, 저희는 학생4의 수학점수를 공부시간(m)을 통하여 예측하고 싶습니다.\n\n\n\n\n\n1. initialize model with a constant value:\\\n\n\n\\(F_0(x) = \\underset{\\gamma}{\\arg\\min}\\sum_{i=1}^n L(y_i, \\gamma)\\)\n\n\n\n\n초기모델을 상수로 정의합니다. 여기서 상수는 \\(y_i\\)와 \\(\\gamma\\)입니다.\n\\(y_i\\)는 수학점수를 의미하고 \\(\\gamma\\)는 초기 예측값입니다.\n\n\\(y_1\\) = 90, \\(y_2\\) = 65, \\(y_3\\) = 60\n\n여기서의 Loss Function \\(L(y_i, \\gamma) = \\displaystyle\\frac1n \\sum_{i=1}^{n} (y_i - \\gamma)^2\\)입니다. (잘 모르시는 분들은 MSE 학습하시면 좋을 것 같습니다.)\n예시의 수치를 대입하면\n\\(L(y_i, \\gamma) = \\displaystyle\\frac13 \\sum_{i=1}^{3} (y_i - \\gamma)^2 =  \\displaystyle\\frac13(90 - \\gamma)^2 + \\displaystyle\\frac13(65 - \\gamma)^2 + \\displaystyle\\frac13(55 - \\gamma)^2\\)입니다.\n\\(L(y_i, \\gamma)\\)가 최소가 되는 \\(\\gamma\\)를 찾아야 하므로 \\(\\displaystyle\\frac{\\partial L}{\\partial \\gamma} = \\displaystyle\\frac23(\\gamma - 90) + \\displaystyle\\frac23(\\gamma - 65) + \\displaystyle\\frac23(\\gamma - 55) = \\displaystyle\\frac23(3\\gamma - 210)\\)입니다.\n\\(\\displaystyle\\frac{\\partial L}{\\partial \\gamma} = 0\\)이 되어야 하므로 \\(\\gamma\\) = 70입니다. 따라서 초기 예측값 \\(\\gamma\\)는 70이 됩니다.\n초기 예측값을 이용한 첫번째 잔차는 각각 20, -5, -15가 됩니다.\n\n\nimport pandas as pd\nidx = ['Student1', 'Student2', 'Student3']\ndata = pd.DataFrame({'StudyTime(m)':[150, 120, 60], 'Sex':['female','male','male'], \n                     'MathScore':[90, 65, 55], 'F_0':[70 for i in range(3)],'r1': [20, -5, -15]},index=idx)\ndata\n\n\n\n\n\n\n\n\nStudyTime(m)\nSex\nMathScore\nF_0\nr1\n\n\n\n\nStudent1\n150\nfemale\n90\n70\n20\n\n\nStudent2\n120\nmale\n65\n70\n-5\n\n\nStudent3\n60\nmale\n55\n70\n-15\n\n\n\n\n\n\n\n- 위의 데이터를 의사결정나무로 시각화를 하면 다음과 같습니다.\n\nimport sklearn.tree\n## step1\nx = data[['StudyTime(m)']]\ny = data['MathScore']\n\n## step2\npredictr = sklearn.tree.DecisionTreeRegressor(max_depth=1)\n\n## step3\npredictr.fit(x,y)\n\n## visualization\nsklearn.tree.plot_tree(predictr,feature_names=x.columns);\n\n\n\n\n\n\n\n\n\n\n\n\nFor \\(m\\) = 1 to \\(M\\):\n\nCompute so-called pseudo-residuals \\(r_{im} = - \\Bigg[\\displaystyle\\frac{\\partial L(y_i, F(x_i))}{\\partial F(x_i)}\\Bigg]_{F(x) = F_{m-1}(x)}\\) for \\(i = 1, ... ,n\\)\nFit a base learner (or weak learner, e.g. tree) closed under scaling \\(h_m(x)\\) to pseuo-residuals, i.e. train it using the training set \\(\\{(x_i, r_{im})\\}_{i=1}^{n}\\)\nCompute multiplier \\(\\gamma_{m}\\) by solving the following one-dimensional optimization problem:\n\\(\\gamma_m = \\underset{\\gamma}{\\arg\\min}\\sum_{i=1}^n L(y_i, F_{m-1}(x_i) + \\gamma h_m(x_i))\\)\nUpdate the model:\n\\(F_m(x) = F_{m-1}(x) + \\gamma_m h_m(x)\\)\n\nA단계부터 D단계를 총 M번 반복하는 단계입니다.\n여기서 \\(M\\)은 예측기의 개수, \\(n\\)은 데이터의 개수입니다.\n예측기에 번호를 붙여 \\(m\\)=1인 경우는 1번 예측기 \\(m=M\\)인 경우는 M번 예측기라고 부르겠습니다.\n1번 예측기(\\(m=1\\))가 생성 되는 과정을 살펴보겠습니다. \\(L(y_i,F(x_i)) = \\displaystyle\\frac12(y_i - F(x_i))^2\\)이므로\n\n\\(r_{i1} = - \\Bigg[\\displaystyle\\frac{\\partial L(y_i, F(x_i))}{\\partial F(x_i)}\\Bigg]_{F(x) = F_{0}(x)} = y_i - F_0(x_i)\\)입니다. (잔차와 식이 동일합니다.)\n0번 예측기 \\(F_0(x)\\)는 1단계의 초기모델에서 생성되었습니다. 따라서 \\(F_0(x)=70\\)입니다.(1단계 참고)\n따라서 \\(r_{11}=20\\), \\(r_{21}=-5\\), \\(r_{31}=-15\\)입니다. 1단계 표에 있는 잔차1과 같습니다.\n함수\\(h_1(x)\\)는 앞서 계산한 잔차를 트리 계열로 학습합니다.(의사 결정 나무에 대해서는 추후에 업로드 하겠습니다.)\n모든 데이터(\\(n=3\\))에 대해서 \\(\\sum_{i=1}^{3} L(y_i, F_0(x_i) + \\gamma h_m(x_i)\\) 식이 최소가 되는 \\(\\gamma\\)(learning rate)를 찾습니다.(\\(\\gamma = \\gamma_{1}\\))\n모델을 업데이트 합니다 : \\(F_1(x) = F_0(x) + \\gamma_1 h_1(x)\\)\n\n4번의 과정을 M번 반복합니다.\n\n- 추가설명\n\n\n\n\n\n\n3. Output \\(\\hat{f(x)} = F_M(x)\\)\n\n\n\n\n최종 모델을 산출합니다."
  },
  {
    "objectID": "posts/ANN(1) - ANN의 구조.html",
    "href": "posts/ANN(1) - ANN의 구조.html",
    "title": "ANN(1) - ANN의 구조",
    "section": "",
    "text": "test\n\n\n\nInput Layer, Hidden Layer, Output Layer 3 단계로 나눌 수 있습니다.\nInput Layer : n개의 입력값을 받는 층(layer)입니다.\nHidden Layer : 입력과 출력사이의 복잡한 관계를 학습하는 층(layer)입니다.\nOutput Layer : 네트워크의 최종 출력을 생성하는 층(layer)입니다.\nn개의 변수(\\(x_1, x_2, ... ,x_n\\))가 입력되어 m개의 값(\\(y_1, y_2, ... y_m\\))출력되는 함수라고 생각해도 좋을 것 같습니다.\\(f(x_1, x_2, ... ,x_n) = (y_1, y_2, ... ,y_m)\\)\n\n그림에는 Output이 n개지만 꼭 입력값과 같을 필요는 없습니다. (실제로는 \\(P(n=m)\\)의 값이 매우 낮습니다.)\n\n\n\n\n\n\n\n\n노드\n\n\n\n노드 구조는 다음과 같습니다.\n\n\n\\(x_k(k=0,1,2)\\) : 이전 층(혹은 노드)에서 나온 출력값입니다.\n\\(w_k(k=0,1,2)\\) : x_k의 가중치입니다.\n\\(b\\) : 노드에 사용되는 bias(편향치)입니다.\n\\(y\\) : 노드의 출력값입니다.\n\n\n계산 식은 다음과 같습니다 : \\(\\displaystyle\\sum\\limits_{i=0}^{2}(x_0w_0) + b = y\\)\n노드가 여러개 모이면 한개의 층(layer)을 형성합니다.\n\n\n\n\n\n\n\nFully Connected Layer\n\n\n\n직역 그대로 완전히 연결되어 있는 층(layer)이라고 생각하시면 됩니다.\n그림과 같이 각 층(layer)에 있는 노드가 다음 층에 있는 노드와 빠짐없이 전부 연결되어 있습니다.\n\\(i\\)번째 층(layer)에 있는 노드가 \\(p\\)개 \\(i+1\\)번째 층(layer)에 있는 노드가 \\(q\\)개라면, \\(i\\)번째 층(layer)과 \\(i+1\\)번째 층(layer)을 잇는 선은 총 \\(p^q\\)개입니다.\n\n\n\n\n\n\n\nforward propagation.jpg\n\n\n※아무거나 사진 퍼오기가 무서워서 공부할겸 직접 그려봤습니다..\n\n편의상 \\(i\\)번째 층의 각각의 노드에 들어가는 입력값을 \\(x_1, x_2, ... x_n\\)이라고 하겠습니다.\n입력값들은 노드를 지나칠 때마다 가중치를 곱하고 편향(bias)를 더합니다.\n\nk번째 노드에서의 계산은 다음과 같습니다. \\(x_kw_k+b_k\\)\n\nFully Connected Layer에 의해서 \\(i+1\\)번째 층(layer)에서 각각의 노드는 입력값을 벡터로 받게 됩니다 : $ = (a^1_1 a^1_2 … a^1_n)$4\n\\(i+1\\)번째 층(layer)의 \\(k\\)번째 노드에서의 연산은 다음과 같습니다.\n\n\\(\\displaystyle(\\overrightarrow{a_k})^T\\overrightarrow{w^k} + b^1_k\\)\n\n각 노드에서는 가중치(weight)곱과 편향(bias)합 연산이 이루어지고, Activation Function을 거치게 됩니다.\n\n\n\n\n\n\n\nHidden Layer에서 여러 가중치(weight)와 편향(bias)의 연산을 마치고 난 값들은 Output Layer의 입력값으로 들어가게 됩니다.\nOutput Layer에서도 여러 연산이 있지만 Loss Function을 통해서 예측값과 정답의 차이를 계산하게 됩니다.\n이때 예측값과 정답의 차이 즉, Loss를 줄여나가야 효과적인 딥러닝 모델이라고 볼 수 있겠죠?\nLoss를 줄이는 방법으로 미분이 사용됩니다.\n\n\n\n\n역전파1.jpg\n\n\n\n아이디어는 다음과 같습니다.\n\n\n쉬운 예로 Loss Function을 다음과 같이 정의합니다 : \\(f(x)=2x^2\\)\n그림과 같이 Loss Function을 미분하여 기울기를 얻으면, -(기울기)가 곧 \\(f(x)\\)의 최소값으로 향하는 방향이 됩니다.\n\\(f'(1)=4\\)이므로 -4는 \\(f(x)\\)가 최소로 되는 방향을 가르킵니다. 실제로 \\(1-f'(1)\\)을 하게 되면 최소가 있는 \\(x=0\\)방향으로 이동합니다.\n따라서 \\(x\\)를 다음과 같이 업데이트 할 수 있습니다 : \\(x \\to x-f'(x)\\)\n\n\n\n\n하지만 \\(x \\to x-f'(x)\\)로 업데이트 할 경우 다음과 같은 문제가 있습니다. \\(x \\to x-f'(x) = -3x\\)\n\\(x=1)\\)\n\\(1 \\to -3\\)\n\n\\(x=-3)\\)\n$ -3 (-3)(-3)=9\\(\\\n\\\n\\)x=9)\\(\\\n\\) 9 (-3)9 = -27$\n즉 \\(x\\)값은 \\(f(x)\\)가 최소인 \\(x\\)로 수렴하지 못하고 발산하게 됩니다.\n이는 learning rate(학습률)로 해결할 수 있습니다.\n\n\n\n\n역전파2.jpg\n\n\n\\(x \\to x - \\alpha f'(x)(\\alpha = 0.1)\\)\n\\(x \\to x - 0.1f'(x)\\) \\(x \\to 0.6x\\)\n\n\\(x=1)\\)\n\\(1 \\to 0.6\\)\n\n\\(x=0.6)\\)\n\\(0.6 \\to 0.36\\)\n- 이렇게 \\(\\alpha\\)(learning rate)를 곱해주면 앞서 발산하는 문제를 해결할 수 있습니다.\n- 경사를 따라 내려온다고 하여 경사하강법으로도 불립니다.(Gradient descent)\n\n\n\noutput layer.jpg\n\n\n- Output Layer에 있는 Loss Function을 시각화한 그림입니다.\n\n실제 Loss function은 앞선 2차식처럼 단순하지 않습니다.\nHidden Layer의 Layer 수와 각 Layer의 노드 수, 그리고 입력값에 따라 영향을 받습니다.\n따라서 Loss Function을 가중치(weight)와 편향(bias)에 대해 편미분을 해줘야 합니다.\n그렇게 여러 차례 미분해서 Loss Function을 최소화 시켜줍니다.\n\n- Forward Propagation이 Input Layer에서 Output Layer의 방향으로 계산된다면,\n- Back Propagation은 Output Layer에서 Input Layer방향으로 계산됩니다.\n\n\n\n\n\nLeCun Initialization\n\\(w \\sim U(- \\sqrt{\\displaystyle\\frac{3}{N_{in}}}, \\sqrt{\\displaystyle\\frac{3}{N_{in}}})\\) or \\(w \\sim N(0, \\displaystyle\\frac{1}{N_{in}})\\) $\nXavier(Sigmoid/tanh 사용하는 신경망)\n\\(w \\sim U\\left(-\\sqrt{\\displaystyle\\frac{3}{N_{\\text{in}} + N_{\\text{out}}}}, \\sqrt{\\displaystyle\\frac{3}{N_{\\text{in}} + N_{\\text{out}}}}\\right)\\) or \\(w \\sim U\\left(0, \\displaystyle\\frac{2}{N_{\\text{in}} + N_{\\text{out}}}\\right)\\)\nHe(ReLU 사용하는 신경망)\n\\(w \\sim U\\left(-\\sqrt{\\displaystyle\\frac{6}{N_{\\text{in}}}}, \\sqrt{\\displaystyle\\frac{6}{N_{\\text{in}}}}\\right)\\) or \\(w \\sim \\left(0, \\displaystyle\\frac{2}{N_{\\text{in}}}\\right)\\)\n\n\n\\(N_{\\text{in}}\\) : 이전 layer 노드의 수\n\\(N_{\\text{out}}\\) : 이후 layer 노드의 수\n\n\n\n\n- 노드는 Affine Function과 Activation Function으로 이루어져있습니다.\n\n\n\nNode.jpg\n\n\n- Affine Function * 입력값을 \\(x\\), Affine Function을 \\(f(x)\\)라고 정의할 때, 다음과 같습니다.\n\\(f(x) = xw + b = z\\)\n- Activation Function * 계산된 \\(z\\)값을 Activation Function \\(g(z)\\)에 넣어줍니다 : \\(g(z) = a\\) * Activation Function의 종류는 크게 세가지로 나눌 수 있습니다.\n\\(\\Rightarrow\\) Sigmoid, tanh, ReLu\n\n\n\\(Sigmoid(z) = \\displaystyle\\frac{1}{1+e^{-z}}\\)\n\n\n\nsigmoid\n\n\n\n\n\n\\(tanh(z) = \\displaystyle\\frac{e^{z}-e^{-z}}{e^{z}+e^{-z}}\\)\n\n\n\n스크린샷 2024-03-22 160148.png"
  },
  {
    "objectID": "posts/ANN(1) - ANN의 구조.html#ann1---ann의-구조",
    "href": "posts/ANN(1) - ANN의 구조.html#ann1---ann의-구조",
    "title": "ANN(1) - ANN의 구조",
    "section": "",
    "text": "test\n\n\n\nInput Layer, Hidden Layer, Output Layer 3 단계로 나눌 수 있습니다.\nInput Layer : n개의 입력값을 받는 층(layer)입니다.\nHidden Layer : 입력과 출력사이의 복잡한 관계를 학습하는 층(layer)입니다.\nOutput Layer : 네트워크의 최종 출력을 생성하는 층(layer)입니다.\nn개의 변수(\\(x_1, x_2, ... ,x_n\\))가 입력되어 m개의 값(\\(y_1, y_2, ... y_m\\))출력되는 함수라고 생각해도 좋을 것 같습니다.\\(f(x_1, x_2, ... ,x_n) = (y_1, y_2, ... ,y_m)\\)\n\n그림에는 Output이 n개지만 꼭 입력값과 같을 필요는 없습니다. (실제로는 \\(P(n=m)\\)의 값이 매우 낮습니다.)\n\n\n\n\n\n\n\n\n노드\n\n\n\n노드 구조는 다음과 같습니다.\n\n\n\\(x_k(k=0,1,2)\\) : 이전 층(혹은 노드)에서 나온 출력값입니다.\n\\(w_k(k=0,1,2)\\) : x_k의 가중치입니다.\n\\(b\\) : 노드에 사용되는 bias(편향치)입니다.\n\\(y\\) : 노드의 출력값입니다.\n\n\n계산 식은 다음과 같습니다 : \\(\\displaystyle\\sum\\limits_{i=0}^{2}(x_0w_0) + b = y\\)\n노드가 여러개 모이면 한개의 층(layer)을 형성합니다.\n\n\n\n\n\n\n\nFully Connected Layer\n\n\n\n직역 그대로 완전히 연결되어 있는 층(layer)이라고 생각하시면 됩니다.\n그림과 같이 각 층(layer)에 있는 노드가 다음 층에 있는 노드와 빠짐없이 전부 연결되어 있습니다.\n\\(i\\)번째 층(layer)에 있는 노드가 \\(p\\)개 \\(i+1\\)번째 층(layer)에 있는 노드가 \\(q\\)개라면, \\(i\\)번째 층(layer)과 \\(i+1\\)번째 층(layer)을 잇는 선은 총 \\(p^q\\)개입니다.\n\n\n\n\n\n\n\nforward propagation.jpg\n\n\n※아무거나 사진 퍼오기가 무서워서 공부할겸 직접 그려봤습니다..\n\n편의상 \\(i\\)번째 층의 각각의 노드에 들어가는 입력값을 \\(x_1, x_2, ... x_n\\)이라고 하겠습니다.\n입력값들은 노드를 지나칠 때마다 가중치를 곱하고 편향(bias)를 더합니다.\n\nk번째 노드에서의 계산은 다음과 같습니다. \\(x_kw_k+b_k\\)\n\nFully Connected Layer에 의해서 \\(i+1\\)번째 층(layer)에서 각각의 노드는 입력값을 벡터로 받게 됩니다 : $ = (a^1_1 a^1_2 … a^1_n)$4\n\\(i+1\\)번째 층(layer)의 \\(k\\)번째 노드에서의 연산은 다음과 같습니다.\n\n\\(\\displaystyle(\\overrightarrow{a_k})^T\\overrightarrow{w^k} + b^1_k\\)\n\n각 노드에서는 가중치(weight)곱과 편향(bias)합 연산이 이루어지고, Activation Function을 거치게 됩니다.\n\n\n\n\n\n\n\nHidden Layer에서 여러 가중치(weight)와 편향(bias)의 연산을 마치고 난 값들은 Output Layer의 입력값으로 들어가게 됩니다.\nOutput Layer에서도 여러 연산이 있지만 Loss Function을 통해서 예측값과 정답의 차이를 계산하게 됩니다.\n이때 예측값과 정답의 차이 즉, Loss를 줄여나가야 효과적인 딥러닝 모델이라고 볼 수 있겠죠?\nLoss를 줄이는 방법으로 미분이 사용됩니다.\n\n\n\n\n역전파1.jpg\n\n\n\n아이디어는 다음과 같습니다.\n\n\n쉬운 예로 Loss Function을 다음과 같이 정의합니다 : \\(f(x)=2x^2\\)\n그림과 같이 Loss Function을 미분하여 기울기를 얻으면, -(기울기)가 곧 \\(f(x)\\)의 최소값으로 향하는 방향이 됩니다.\n\\(f'(1)=4\\)이므로 -4는 \\(f(x)\\)가 최소로 되는 방향을 가르킵니다. 실제로 \\(1-f'(1)\\)을 하게 되면 최소가 있는 \\(x=0\\)방향으로 이동합니다.\n따라서 \\(x\\)를 다음과 같이 업데이트 할 수 있습니다 : \\(x \\to x-f'(x)\\)\n\n\n\n\n하지만 \\(x \\to x-f'(x)\\)로 업데이트 할 경우 다음과 같은 문제가 있습니다. \\(x \\to x-f'(x) = -3x\\)\n\\(x=1)\\)\n\\(1 \\to -3\\)\n\n\\(x=-3)\\)\n$ -3 (-3)(-3)=9\\(\\\n\\\n\\)x=9)\\(\\\n\\) 9 (-3)9 = -27$\n즉 \\(x\\)값은 \\(f(x)\\)가 최소인 \\(x\\)로 수렴하지 못하고 발산하게 됩니다.\n이는 learning rate(학습률)로 해결할 수 있습니다.\n\n\n\n\n역전파2.jpg\n\n\n\\(x \\to x - \\alpha f'(x)(\\alpha = 0.1)\\)\n\\(x \\to x - 0.1f'(x)\\) \\(x \\to 0.6x\\)\n\n\\(x=1)\\)\n\\(1 \\to 0.6\\)\n\n\\(x=0.6)\\)\n\\(0.6 \\to 0.36\\)\n- 이렇게 \\(\\alpha\\)(learning rate)를 곱해주면 앞서 발산하는 문제를 해결할 수 있습니다.\n- 경사를 따라 내려온다고 하여 경사하강법으로도 불립니다.(Gradient descent)\n\n\n\noutput layer.jpg\n\n\n- Output Layer에 있는 Loss Function을 시각화한 그림입니다.\n\n실제 Loss function은 앞선 2차식처럼 단순하지 않습니다.\nHidden Layer의 Layer 수와 각 Layer의 노드 수, 그리고 입력값에 따라 영향을 받습니다.\n따라서 Loss Function을 가중치(weight)와 편향(bias)에 대해 편미분을 해줘야 합니다.\n그렇게 여러 차례 미분해서 Loss Function을 최소화 시켜줍니다.\n\n- Forward Propagation이 Input Layer에서 Output Layer의 방향으로 계산된다면,\n- Back Propagation은 Output Layer에서 Input Layer방향으로 계산됩니다.\n\n\n\n\n\nLeCun Initialization\n\\(w \\sim U(- \\sqrt{\\displaystyle\\frac{3}{N_{in}}}, \\sqrt{\\displaystyle\\frac{3}{N_{in}}})\\) or \\(w \\sim N(0, \\displaystyle\\frac{1}{N_{in}})\\) $\nXavier(Sigmoid/tanh 사용하는 신경망)\n\\(w \\sim U\\left(-\\sqrt{\\displaystyle\\frac{3}{N_{\\text{in}} + N_{\\text{out}}}}, \\sqrt{\\displaystyle\\frac{3}{N_{\\text{in}} + N_{\\text{out}}}}\\right)\\) or \\(w \\sim U\\left(0, \\displaystyle\\frac{2}{N_{\\text{in}} + N_{\\text{out}}}\\right)\\)\nHe(ReLU 사용하는 신경망)\n\\(w \\sim U\\left(-\\sqrt{\\displaystyle\\frac{6}{N_{\\text{in}}}}, \\sqrt{\\displaystyle\\frac{6}{N_{\\text{in}}}}\\right)\\) or \\(w \\sim \\left(0, \\displaystyle\\frac{2}{N_{\\text{in}}}\\right)\\)\n\n\n\\(N_{\\text{in}}\\) : 이전 layer 노드의 수\n\\(N_{\\text{out}}\\) : 이후 layer 노드의 수\n\n\n\n\n- 노드는 Affine Function과 Activation Function으로 이루어져있습니다.\n\n\n\nNode.jpg\n\n\n- Affine Function * 입력값을 \\(x\\), Affine Function을 \\(f(x)\\)라고 정의할 때, 다음과 같습니다.\n\\(f(x) = xw + b = z\\)\n- Activation Function * 계산된 \\(z\\)값을 Activation Function \\(g(z)\\)에 넣어줍니다 : \\(g(z) = a\\) * Activation Function의 종류는 크게 세가지로 나눌 수 있습니다.\n\\(\\Rightarrow\\) Sigmoid, tanh, ReLu\n\n\n\\(Sigmoid(z) = \\displaystyle\\frac{1}{1+e^{-z}}\\)\n\n\n\nsigmoid\n\n\n\n\n\n\\(tanh(z) = \\displaystyle\\frac{e^{z}-e^{-z}}{e^{z}+e^{-z}}\\)\n\n\n\n스크린샷 2024-03-22 160148.png"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "AI_theory",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nMar 22, 2024\n\n\nANN(1) - ANN의 구조\n\n\n이정재 \n\n\n\n\nMar 16, 2024\n\n\nGBM\n\n\n이정재 \n\n\n\n\n\nNo matching items"
  }
]