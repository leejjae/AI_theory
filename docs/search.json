[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/ANN(1) - ANN's Structure.html",
    "href": "posts/ANN(1) - ANN's Structure.html",
    "title": "ANN(1) - ANN의 구조",
    "section": "",
    "text": "test\n\n\n\nInput Layer, Hidden Layer, Output Layer 3 단계로 나눌 수 있습니다.\nInput Layer : n개의 입력값을 받는 층(layer)입니다.\nHidden Layer : 입력과 출력사이의 복잡한 관계를 학습하는 층(layer)입니다.\nOutput Layer : 네트워크의 최종 출력을 생성하는 층(layer)입니다.\nn개의 변수(\\(x_1, x_2, ... ,x_n\\))가 입력되어 m개의 값(\\(y_1, y_2, ... y_m\\))출력되는 함수라고 생각해도 좋을 것 같습니다.\\(f(x_1, x_2, ... ,x_n) = (y_1, y_2, ... ,y_m)\\)\n\n그림에는 Output이 n개지만 꼭 입력값과 같을 필요는 없습니다. (실제로는 \\(P(n=m)\\)의 값이 매우 낮습니다.)\n\n\n\n\n\n\n\n\n노드\n\n\n\n노드 구조는 다음과 같습니다.\n\n\n\\(x_k(k=0,1,2)\\) : 이전 층(혹은 노드)에서 나온 출력값입니다.\n\\(w_k(k=0,1,2)\\) : x_k의 가중치입니다.\n\\(b\\) : 노드에 사용되는 bias(편향치)입니다.\n\\(y\\) : 노드의 출력값입니다.\n\n\n계산 식은 다음과 같습니다 : \\(\\displaystyle\\sum\\limits_{i=0}^{2}(x_0w_0) + b = y\\)\n노드가 여러개 모이면 한개의 층(layer)을 형성합니다.\n\n\n\n\n\n\n\nFully Connected Layer\n\n\n\n직역 그대로 완전히 연결되어 있는 층(layer)이라고 생각하시면 됩니다.\n그림과 같이 각 층(layer)에 있는 노드가 다음 층에 있는 노드와 빠짐없이 전부 연결되어 있습니다.\n\\(i\\)번째 층(layer)에 있는 노드가 \\(p\\)개 \\(i+1\\)번째 층(layer)에 있는 노드가 \\(q\\)개라면, \\(i\\)번째 층(layer)과 \\(i+1\\)번째 층(layer)을 잇는 선은 총 \\(p^q\\)개입니다.\n\n\n\n\n\n\n\nforward propagation.jpg\n\n\n※아무거나 사진 퍼오기가 무서워서 공부할겸 직접 그려봤습니다..\n\n편의상 \\(i\\)번째 층의 각각의 노드에 들어가는 입력값을 \\(x_1, x_2, ... x_n\\)이라고 하겠습니다.\n입력값들은 노드를 지나칠 때마다 가중치를 곱하고 편향(bias)를 더합니다.\n\nk번째 노드에서의 계산은 다음과 같습니다. \\(x_kw_k+b_k\\)\n\nFully Connected Layer에 의해서 \\(i+1\\)번째 층(layer)에서 각각의 노드는 입력값을 벡터로 받게 됩니다 : \\(\\displaystyle\\overrightarrow{a^1} = \\displaystyle\\left(a^1_1 a^1_2 ... a^1_n \\right)\\)\n\\(i+1\\)번째 층(layer)의 \\(k\\)번째 노드에서의 연산은 다음과 같습니다.\n\n\\(\\displaystyle\\overrightarrow{a_k}(\\overrightarrow{w_k})^T + b^1_k\\)\n\n각 노드에서는 가중치(weight)곱과 편향(bias)합 연산이 이루어지고, Activation Function을 거치게 됩니다.\n\n\n\n\n\n\n\nHidden Layer에서 여러 가중치(weight)와 편향(bias)의 연산을 마치고 난 값들은 Output Layer의 입력값으로 들어가게 됩니다.\nOutput Layer에서도 여러 연산이 있지만 Loss Function을 통해서 예측값과 정답의 차이를 계산하게 됩니다.\n이때 예측값과 정답의 차이 즉, Loss를 줄여나가야 효과적인 딥러닝 모델이라고 볼 수 있겠죠?\nLoss를 줄이는 방법으로 미분이 사용됩니다.\n\n\n\n\n역전파1.jpg\n\n\n\n아이디어는 다음과 같습니다.\n\n\n쉬운 예로 Loss Function을 다음과 같이 정의합니다 : \\(f(x)=2x^2\\)\n그림과 같이 Loss Function을 미분하여 기울기를 얻으면, -(기울기)가 곧 \\(f(x)\\)의 최소값으로 향하는 방향이 됩니다.\n\\(f'(1)=4\\)이므로 -4는 \\(f(x)\\)가 최소로 되는 방향을 가르킵니다. 실제로 \\(1-f'(1)\\)을 하게 되면 최소가 있는 \\(x=0\\)방향으로 이동합니다.\n따라서 \\(x\\)를 다음과 같이 업데이트 할 수 있습니다 : \\(x \\to x-f'(x)\\)\n\n\n\n\n하지만 \\(x \\to x-f'(x)\\)로 업데이트 할 경우 다음과 같은 문제가 있습니다. \\(x \\to x-f'(x) = -3x\\)\n\n\\(x=1)\\)\n\\(1 \\to -3\\)\n\n\\(x=-3)\\)\n$ -3 (-3)(-3)=9\\(\\\n\\\n\\)x=9)$\n\\(9 \\to (-3)9 = -27\\)\n즉 \\(x\\)값은 \\(f(x)\\)가 최소인 \\(x\\)로 수렴하지 못하고 발산하게 됩니다.\n이는 learning rate(학습률)로 해결할 수 있습니다.\n\n\n\n\n역전파2.jpg\n\n\n\\(x \\to x - \\alpha f'(x)(\\alpha = 0.1)\\)\n\\(x \\to x - 0.1f'(x)\\) \\(x \\to 0.6x\\)\n\n\\(x=1)\\)\n\\(1 \\to 0.6\\)\n\n\\(x=0.6)\\)\n\\(0.6 \\to 0.36\\)\n- 이렇게 \\(\\alpha\\)(learning rate)를 곱해주면 앞서 발산하는 문제를 해결할 수 있습니다.\n- 경사를 따라 내려온다고 하여 경사하강법으로도 불립니다.(Gradient descent)\n\n\n\noutput layer.jpg\n\n\n- Output Layer에 있는 Loss Function을 시각화한 그림입니다.\n\n실제 Loss function은 앞선 2차식처럼 단순하지 않습니다.\nHidden Layer의 Layer 수와 각 Layer의 노드 수, 그리고 입력값에 따라 영향을 받습니다.\n따라서 Loss Function을 가중치(weight)와 편향(bias)에 대해 편미분을 해줘야 합니다.\n그렇게 여러 차례 미분해서 Loss Function을 최소화 시켜줍니다.\n\n- Forward Propagation이 Input Layer에서 Output Layer의 방향으로 계산된다면,\n- Back Propagation은 Output Layer에서 Input Layer방향으로 계산됩니다.\n\n\n\n\n\nLeCun Initialization\n\\(w \\sim U(- \\sqrt{\\displaystyle\\frac{3}{N_{in}}}, \\sqrt{\\displaystyle\\frac{3}{N_{in}}})\\) or \\(w \\sim N(0, \\displaystyle\\frac{1}{N_{in}})\\) $\nXavier(Sigmoid/tanh 사용하는 신경망)\n\\(w \\sim U\\left(-\\sqrt{\\displaystyle\\frac{3}{N_{\\text{in}} + N_{\\text{out}}}}, \\sqrt{\\displaystyle\\frac{3}{N_{\\text{in}} + N_{\\text{out}}}}\\right)\\) or \\(w \\sim U\\left(0, \\displaystyle\\frac{2}{N_{\\text{in}} + N_{\\text{out}}}\\right)\\)\nHe(ReLU 사용하는 신경망)\n\\(w \\sim U\\left(-\\sqrt{\\displaystyle\\frac{6}{N_{\\text{in}}}}, \\sqrt{\\displaystyle\\frac{6}{N_{\\text{in}}}}\\right)\\) or \\(w \\sim \\left(0, \\displaystyle\\frac{2}{N_{\\text{in}}}\\right)\\)\n\n\n\\(N_{\\text{in}}\\) : 이전 layer 노드의 수\n\\(N_{\\text{out}}\\) : 이후 layer 노드의 수\n\n\n\n\n- 노드는 Affine Function과 Activation Function으로 이루어져있습니다.\n\n\n\nNode.jpg\n\n\n- Affine Function * 입력값을 \\(x\\), Affine Function을 \\(f(x)\\)라고 정의할 때, 다음과 같습니다.\n\\(f(x) = xw + b = z\\)\n- Activation Function * 계산된 \\(z\\)값을 Activation Function \\(g(z)\\)에 넣어줍니다 : \\(g(z) = a\\) * Activation Function의 종류는 크게 세가지로 나눌 수 있습니다.\n\\(\\Rightarrow\\) Sigmoid, tanh, ReLu\n\n\n\\(Sigmoid(z) = \\displaystyle\\frac{1}{1+e^{-z}}\\)\n\n\n\nsigmoid\n\n\n\n\n\n\\(tanh(z) = \\displaystyle\\frac{e^{z}-e^{-z}}{e^{z}+e^{-z}}\\)\n\n\n\n스크린샷 2024-03-22 160148.png\n\n\n- Activation Function을 사용해야 하는 이유 * 기본적으로 Affine Function은 lienar function이므로 Affine Function만 계속해서 늘려간다면, Hidden Layer가 아무리 깊어도 학습효과를 누리지 못함."
  },
  {
    "objectID": "posts/ANN(1) - ANN's Structure.html#ann1---ann의-구조",
    "href": "posts/ANN(1) - ANN's Structure.html#ann1---ann의-구조",
    "title": "ANN(1) - ANN의 구조",
    "section": "",
    "text": "test\n\n\n\nInput Layer, Hidden Layer, Output Layer 3 단계로 나눌 수 있습니다.\nInput Layer : n개의 입력값을 받는 층(layer)입니다.\nHidden Layer : 입력과 출력사이의 복잡한 관계를 학습하는 층(layer)입니다.\nOutput Layer : 네트워크의 최종 출력을 생성하는 층(layer)입니다.\nn개의 변수(\\(x_1, x_2, ... ,x_n\\))가 입력되어 m개의 값(\\(y_1, y_2, ... y_m\\))출력되는 함수라고 생각해도 좋을 것 같습니다.\\(f(x_1, x_2, ... ,x_n) = (y_1, y_2, ... ,y_m)\\)\n\n그림에는 Output이 n개지만 꼭 입력값과 같을 필요는 없습니다. (실제로는 \\(P(n=m)\\)의 값이 매우 낮습니다.)\n\n\n\n\n\n\n\n\n노드\n\n\n\n노드 구조는 다음과 같습니다.\n\n\n\\(x_k(k=0,1,2)\\) : 이전 층(혹은 노드)에서 나온 출력값입니다.\n\\(w_k(k=0,1,2)\\) : x_k의 가중치입니다.\n\\(b\\) : 노드에 사용되는 bias(편향치)입니다.\n\\(y\\) : 노드의 출력값입니다.\n\n\n계산 식은 다음과 같습니다 : \\(\\displaystyle\\sum\\limits_{i=0}^{2}(x_0w_0) + b = y\\)\n노드가 여러개 모이면 한개의 층(layer)을 형성합니다.\n\n\n\n\n\n\n\nFully Connected Layer\n\n\n\n직역 그대로 완전히 연결되어 있는 층(layer)이라고 생각하시면 됩니다.\n그림과 같이 각 층(layer)에 있는 노드가 다음 층에 있는 노드와 빠짐없이 전부 연결되어 있습니다.\n\\(i\\)번째 층(layer)에 있는 노드가 \\(p\\)개 \\(i+1\\)번째 층(layer)에 있는 노드가 \\(q\\)개라면, \\(i\\)번째 층(layer)과 \\(i+1\\)번째 층(layer)을 잇는 선은 총 \\(p^q\\)개입니다.\n\n\n\n\n\n\n\nforward propagation.jpg\n\n\n※아무거나 사진 퍼오기가 무서워서 공부할겸 직접 그려봤습니다..\n\n편의상 \\(i\\)번째 층의 각각의 노드에 들어가는 입력값을 \\(x_1, x_2, ... x_n\\)이라고 하겠습니다.\n입력값들은 노드를 지나칠 때마다 가중치를 곱하고 편향(bias)를 더합니다.\n\nk번째 노드에서의 계산은 다음과 같습니다. \\(x_kw_k+b_k\\)\n\nFully Connected Layer에 의해서 \\(i+1\\)번째 층(layer)에서 각각의 노드는 입력값을 벡터로 받게 됩니다 : \\(\\displaystyle\\overrightarrow{a^1} = \\displaystyle\\left(a^1_1 a^1_2 ... a^1_n \\right)\\)\n\\(i+1\\)번째 층(layer)의 \\(k\\)번째 노드에서의 연산은 다음과 같습니다.\n\n\\(\\displaystyle\\overrightarrow{a_k}(\\overrightarrow{w_k})^T + b^1_k\\)\n\n각 노드에서는 가중치(weight)곱과 편향(bias)합 연산이 이루어지고, Activation Function을 거치게 됩니다.\n\n\n\n\n\n\n\nHidden Layer에서 여러 가중치(weight)와 편향(bias)의 연산을 마치고 난 값들은 Output Layer의 입력값으로 들어가게 됩니다.\nOutput Layer에서도 여러 연산이 있지만 Loss Function을 통해서 예측값과 정답의 차이를 계산하게 됩니다.\n이때 예측값과 정답의 차이 즉, Loss를 줄여나가야 효과적인 딥러닝 모델이라고 볼 수 있겠죠?\nLoss를 줄이는 방법으로 미분이 사용됩니다.\n\n\n\n\n역전파1.jpg\n\n\n\n아이디어는 다음과 같습니다.\n\n\n쉬운 예로 Loss Function을 다음과 같이 정의합니다 : \\(f(x)=2x^2\\)\n그림과 같이 Loss Function을 미분하여 기울기를 얻으면, -(기울기)가 곧 \\(f(x)\\)의 최소값으로 향하는 방향이 됩니다.\n\\(f'(1)=4\\)이므로 -4는 \\(f(x)\\)가 최소로 되는 방향을 가르킵니다. 실제로 \\(1-f'(1)\\)을 하게 되면 최소가 있는 \\(x=0\\)방향으로 이동합니다.\n따라서 \\(x\\)를 다음과 같이 업데이트 할 수 있습니다 : \\(x \\to x-f'(x)\\)\n\n\n\n\n하지만 \\(x \\to x-f'(x)\\)로 업데이트 할 경우 다음과 같은 문제가 있습니다. \\(x \\to x-f'(x) = -3x\\)\n\n\\(x=1)\\)\n\\(1 \\to -3\\)\n\n\\(x=-3)\\)\n$ -3 (-3)(-3)=9\\(\\\n\\\n\\)x=9)$\n\\(9 \\to (-3)9 = -27\\)\n즉 \\(x\\)값은 \\(f(x)\\)가 최소인 \\(x\\)로 수렴하지 못하고 발산하게 됩니다.\n이는 learning rate(학습률)로 해결할 수 있습니다.\n\n\n\n\n역전파2.jpg\n\n\n\\(x \\to x - \\alpha f'(x)(\\alpha = 0.1)\\)\n\\(x \\to x - 0.1f'(x)\\) \\(x \\to 0.6x\\)\n\n\\(x=1)\\)\n\\(1 \\to 0.6\\)\n\n\\(x=0.6)\\)\n\\(0.6 \\to 0.36\\)\n- 이렇게 \\(\\alpha\\)(learning rate)를 곱해주면 앞서 발산하는 문제를 해결할 수 있습니다.\n- 경사를 따라 내려온다고 하여 경사하강법으로도 불립니다.(Gradient descent)\n\n\n\noutput layer.jpg\n\n\n- Output Layer에 있는 Loss Function을 시각화한 그림입니다.\n\n실제 Loss function은 앞선 2차식처럼 단순하지 않습니다.\nHidden Layer의 Layer 수와 각 Layer의 노드 수, 그리고 입력값에 따라 영향을 받습니다.\n따라서 Loss Function을 가중치(weight)와 편향(bias)에 대해 편미분을 해줘야 합니다.\n그렇게 여러 차례 미분해서 Loss Function을 최소화 시켜줍니다.\n\n- Forward Propagation이 Input Layer에서 Output Layer의 방향으로 계산된다면,\n- Back Propagation은 Output Layer에서 Input Layer방향으로 계산됩니다.\n\n\n\n\n\nLeCun Initialization\n\\(w \\sim U(- \\sqrt{\\displaystyle\\frac{3}{N_{in}}}, \\sqrt{\\displaystyle\\frac{3}{N_{in}}})\\) or \\(w \\sim N(0, \\displaystyle\\frac{1}{N_{in}})\\) $\nXavier(Sigmoid/tanh 사용하는 신경망)\n\\(w \\sim U\\left(-\\sqrt{\\displaystyle\\frac{3}{N_{\\text{in}} + N_{\\text{out}}}}, \\sqrt{\\displaystyle\\frac{3}{N_{\\text{in}} + N_{\\text{out}}}}\\right)\\) or \\(w \\sim U\\left(0, \\displaystyle\\frac{2}{N_{\\text{in}} + N_{\\text{out}}}\\right)\\)\nHe(ReLU 사용하는 신경망)\n\\(w \\sim U\\left(-\\sqrt{\\displaystyle\\frac{6}{N_{\\text{in}}}}, \\sqrt{\\displaystyle\\frac{6}{N_{\\text{in}}}}\\right)\\) or \\(w \\sim \\left(0, \\displaystyle\\frac{2}{N_{\\text{in}}}\\right)\\)\n\n\n\\(N_{\\text{in}}\\) : 이전 layer 노드의 수\n\\(N_{\\text{out}}\\) : 이후 layer 노드의 수\n\n\n\n\n- 노드는 Affine Function과 Activation Function으로 이루어져있습니다.\n\n\n\nNode.jpg\n\n\n- Affine Function * 입력값을 \\(x\\), Affine Function을 \\(f(x)\\)라고 정의할 때, 다음과 같습니다.\n\\(f(x) = xw + b = z\\)\n- Activation Function * 계산된 \\(z\\)값을 Activation Function \\(g(z)\\)에 넣어줍니다 : \\(g(z) = a\\) * Activation Function의 종류는 크게 세가지로 나눌 수 있습니다.\n\\(\\Rightarrow\\) Sigmoid, tanh, ReLu\n\n\n\\(Sigmoid(z) = \\displaystyle\\frac{1}{1+e^{-z}}\\)\n\n\n\nsigmoid\n\n\n\n\n\n\\(tanh(z) = \\displaystyle\\frac{e^{z}-e^{-z}}{e^{z}+e^{-z}}\\)\n\n\n\n스크린샷 2024-03-22 160148.png\n\n\n- Activation Function을 사용해야 하는 이유 * 기본적으로 Affine Function은 lienar function이므로 Affine Function만 계속해서 늘려간다면, Hidden Layer가 아무리 깊어도 학습효과를 누리지 못함."
  },
  {
    "objectID": "posts/JBIG코드실습_타이타닉1.html",
    "href": "posts/JBIG코드실습_타이타닉1.html",
    "title": "JBIG코드실습_타이타닉(1주차)",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\n\n\ntrain = pd.read_csv('./train_1.csv')\ntrain.head()\nset(train['Pclass'])\n\n{1, 2, 3}\n\n\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier\nimport lightgbm as lgb\nfrom sklearn.metrics import accuracy_score\n\n# 데이터 불러오기\ntrain = pd.read_csv('./train_1.csv')\n\n# 전처리 (필요시 전처리 추가할 것)#########################################################################\ntrain['Age'] = train['Age'].fillna(29)\ntrain['Sex'] = train['Sex'].map({'male': 1, 'female': 0})\n# 위 5개 칼럼만 사용할 것\ntrain = train[[ 'Pclass', 'SibSp', 'Age', 'Sex', 'Survived']] \n\n# X값과 y값 구하기\nX = train.drop(columns=['Survived'])\ny = train['Survived']\n\n# 데이터 세트 분리\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n\n# 랜덤포레스트를 정의 및 학습하고 Accuracy로 평가해보세요\n\n# 랜덤 포레스트 정의\nrf_classifier = RandomForestClassifier(random_state=42)\n\n# 랜덤 포레스트 학습\nrf_classifier.fit(X_train, y_train)\n\n# 테스트 데이터에 대한 예측\ny_pred_rf = rf_classifier.predict(X_test)\n\n# 정확도 평가\naccuracy_rf = accuracy_score(y_test, y_pred_rf)\nprint(\"Random Forest의 정확도:\", accuracy_rf)\n\nRandom Forest의 정확도: 0.8268156424581006\n\n\n\n# LightGBM를 정의 및 학습하고 Accuracy로 평가해보세요\n\n# LightGBM 모델 정의\nlgb_classifier = lgb.LGBMClassifier(random_state=42)\n\n# LightGBM 모델 학습\nlgb_classifier.fit(X_train, y_train)\n\n# 테스트 데이터에 대한 예측\ny_pred_lgb = lgb_classifier.predict(X_test)\n\n# 정확도 평가\naccuracy_lgb = accuracy_score(y_test, y_pred_lgb)\nprint(\"LightGBM의 정확도:\", accuracy_lgb)\n\nLightGBM의 정확도: 0.8100558659217877\n\n\n\n# 둘 중 더 높은 Accuracy가 나온 모델의 하이퍼 파라미터 default값을 '구글링'을 통해 확인하고 변경해보세요(Grid_Search말고 직접 바꿀것)\n\n\n# 랜덤 포레스트 정의\nrf_classifier = RandomForestClassifier(\n    n_estimators=13,\n    min_samples_split=5,\n    min_samples_leaf=1,\n    max_features='sqrt',\n    max_depth=7,\n    max_leaf_nodes=10,\n    random_state=42)\n\n# 랜덤 포레스트 학습\nrf_classifier.fit(X_train, y_train)\n\n# 테스트 데이터에 대한 예측\ny_pred_rf = rf_classifier.predict(X_test)\n\n# 정확도 평가\naccuracy_rf = accuracy_score(y_test, y_pred_rf)\nprint(\"Random Forest의 정확도:\", accuracy_rf)\n\nRandom Forest의 정확도: 0.8044692737430168\n\n\n\n# LightGBM를 정의 및 학습하고 Accuracy로 평가해보세요\n\n# LightGBM 모델 정의\nlgb_classifier = lgb.LGBMClassifier(\n    num_iterations=130,\n    learning_rate=0.09,\n    max_depth=6,\n    min_data_in_leaf=30,\n    num_leaves=40,\n    boosting='gbdt',\n    bagging_fraction=1.0,\n    feature_fraction=1.0,\n    random_state=42)\n\n# LightGBM 모델 학습\nlgb_classifier.fit(X_train, y_train)\n\n# 테스트 데이터에 대한 예측\ny_pred_lgb = lgb_classifier.predict(X_test)\n\n# 정확도 평가\naccuracy_lgb = accuracy_score(y_test, y_pred_lgb)\nprint(\"LightGBM의 정확도:\", accuracy_lgb)\n\n/root/anaconda3/envs/ag/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n\n\n[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\nLightGBM의 정확도: 0.8268156424581006\n\n\n\n# VotingClassifier를 통해서 앙상블;보팅 한후 Accuracy를 계산해보세요\n\nfrom sklearn.ensemble import VotingClassifier\n\n# VotingClassifier 정의\nvoting_classifier = VotingClassifier(estimators=[\n    ('random_forest', rf_classifier),\n    ('lightgbm', lgb_classifier)\n])\n\n# VotingClassifier 학습\nvoting_classifier.fit(X_train, y_train)\n\n# 테스트 데이터에 대한 예측\ny_pred_voting = voting_classifier.predict(X_test)\n\n# 정확도 평가\naccuracy_voting = accuracy_score(y_test, y_pred_voting)\nprint(\"VotingClassifier의 정확도:\", accuracy_voting)\n\n/root/anaconda3/envs/ag/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n\n\n[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\nVotingClassifier의 정확도: 0.8100558659217877"
  },
  {
    "objectID": "posts/로지스틱회귀.html",
    "href": "posts/로지스틱회귀.html",
    "title": "로지스틱회귀",
    "section": "",
    "text": "- 베르누이 확률밀도함수(PDF)는 다음과 같습니다. * \\(f(x|p) = p^x(1-p)^{1-x} (x=1,0)\\) * 즉 \\(x=1\\)일 확률이 \\(p\\), \\(x=0\\)일 확률이 \\(1-p\\)입니다.\n\n\n\n- 승산비(\\(odds\\,ratio\\)) * 베르누이 분포에서 1이 나올 확률 \\(p\\)와 0이 나올 확률 \\(1-p\\)의 비율입니다. * \\(odds\\, ratio = \\displaystyle\\frac{p}{1-p}    \\left(0\\leq odds\\,ratio\\leq \\infty \\right) = \\displaystyle\\frac{\\text{1일확률}}{\\text{0일확률}}\\) * 확률 \\(p\\)를 \\(odds\\,ratio\\)로 변환하면 0부터 무한대까지의 값을 가질 수 있다. * 승산비(\\(odds\\,ratio\\))를 로그변환하면 다음과 같다. * \\(z = logit\\left(\\displaystyle\\frac{p}{1-p} \\right) = log\\left(\\displaystyle\\frac{p}{1-p}\\right)\\)\n- 식을 다음과 같이 변환해보자\n\\(log\\left(\\displaystyle\\frac{p}{1-p}\\right) = z\\)\n\n\\(\\Leftrightarrow \\displaystyle\\frac{p}{1-p} = e^z\\)\n\n\\(\\Leftrightarrow p = e^z(1-p)\\)\n\n\\(\\Leftrightarrow p = e^z - pe^k\\)\n\n\\(\\Leftrightarrow p(1+e^z) = e^z\\)\n\n\\(\\Leftrightarrow p = \\displaystyle\\frac{e^z}{1+e^z} = \\displaystyle\\frac{1}{1+e^{-z}}\\)\n\n* 이처럼 \\(logit\\,function\\)의 역함수가 \\(logistic\\, function\\)이다. 따라서 * \\(logistic(z) = \\mu(z) = \\displaystyle\\frac{1}{1+e^{-z}}\\)\n- \\(logistic\\,function\\)의 그래프는 다음과 같다.(출처)\n\n\n\nsigmoid\n\n\n\n\n\n- 그러다면 \\(logistic function\\)을 왜 사용하는 것일까?\n\n\n\n사진\n\n\n\n나이에 따른 질병 유무에 대한 분류문제를 해결해야한다고 가정해보자.사진출처 : ratsgo’s blog\n그림과 같이 분류 문제를 선형 회귀로 해결하려고 하면 그림이 많이 이상해집니다.\n따라서 앞서 언급한 \\(logistic\\,function\\)을 이용하여 문제를 해결합니다. \\(logistic\\,function(z) = \\displaystyle\\frac{1}{1+e^{-z}} = \\begin{cases}1&z&gt;\\frac12\\\\\\frac12&z=\\frac12\\\\0&z&lt;\\frac12\\end{cases}\\)\n\n\n\n\n(추가 업로드 예정)\n\n\n\n(추가 업로드 예정)"
  },
  {
    "objectID": "posts/로지스틱회귀.html#로지스틱-회귀logistic-regression",
    "href": "posts/로지스틱회귀.html#로지스틱-회귀logistic-regression",
    "title": "로지스틱회귀",
    "section": "",
    "text": "- 베르누이 확률밀도함수(PDF)는 다음과 같습니다. * \\(f(x|p) = p^x(1-p)^{1-x} (x=1,0)\\) * 즉 \\(x=1\\)일 확률이 \\(p\\), \\(x=0\\)일 확률이 \\(1-p\\)입니다.\n\n\n\n- 승산비(\\(odds\\,ratio\\)) * 베르누이 분포에서 1이 나올 확률 \\(p\\)와 0이 나올 확률 \\(1-p\\)의 비율입니다. * \\(odds\\, ratio = \\displaystyle\\frac{p}{1-p}    \\left(0\\leq odds\\,ratio\\leq \\infty \\right) = \\displaystyle\\frac{\\text{1일확률}}{\\text{0일확률}}\\) * 확률 \\(p\\)를 \\(odds\\,ratio\\)로 변환하면 0부터 무한대까지의 값을 가질 수 있다. * 승산비(\\(odds\\,ratio\\))를 로그변환하면 다음과 같다. * \\(z = logit\\left(\\displaystyle\\frac{p}{1-p} \\right) = log\\left(\\displaystyle\\frac{p}{1-p}\\right)\\)\n- 식을 다음과 같이 변환해보자\n\\(log\\left(\\displaystyle\\frac{p}{1-p}\\right) = z\\)\n\n\\(\\Leftrightarrow \\displaystyle\\frac{p}{1-p} = e^z\\)\n\n\\(\\Leftrightarrow p = e^z(1-p)\\)\n\n\\(\\Leftrightarrow p = e^z - pe^k\\)\n\n\\(\\Leftrightarrow p(1+e^z) = e^z\\)\n\n\\(\\Leftrightarrow p = \\displaystyle\\frac{e^z}{1+e^z} = \\displaystyle\\frac{1}{1+e^{-z}}\\)\n\n* 이처럼 \\(logit\\,function\\)의 역함수가 \\(logistic\\, function\\)이다. 따라서 * \\(logistic(z) = \\mu(z) = \\displaystyle\\frac{1}{1+e^{-z}}\\)\n- \\(logistic\\,function\\)의 그래프는 다음과 같다.(출처)\n\n\n\nsigmoid\n\n\n\n\n\n- 그러다면 \\(logistic function\\)을 왜 사용하는 것일까?\n\n\n\n사진\n\n\n\n나이에 따른 질병 유무에 대한 분류문제를 해결해야한다고 가정해보자.사진출처 : ratsgo’s blog\n그림과 같이 분류 문제를 선형 회귀로 해결하려고 하면 그림이 많이 이상해집니다.\n따라서 앞서 언급한 \\(logistic\\,function\\)을 이용하여 문제를 해결합니다. \\(logistic\\,function(z) = \\displaystyle\\frac{1}{1+e^{-z}} = \\begin{cases}1&z&gt;\\frac12\\\\\\frac12&z=\\frac12\\\\0&z&lt;\\frac12\\end{cases}\\)\n\n\n\n\n(추가 업로드 예정)\n\n\n\n(추가 업로드 예정)"
  },
  {
    "objectID": "posts/Regularization.html",
    "href": "posts/Regularization.html",
    "title": "Regularization",
    "section": "",
    "text": "\\(\\begin{align}\n    E(MSE) & =E\\left[(Y-\\hat{Y})^2|X\\right]\\\\\n        & =E\\left(Y^2-2Y\\hat{Y}+\\hat{Y}^2 |X\\right)\\\\\n        & = \\sigma^2  + \\left\\{E(\\hat{Y}) - \\hat{Y} \\right\\}^2\\\\\n        & = \\sigma^2 + \\text{Bias}\\left(\\hat{Y}\\right) + \\text{Var}(\\hat{Y})\n\\end{align}\\)\n\\(\\hat{\\beta}^{\\text{LS}} = \\underset{\\beta}\\arg\\min \\left\\{\\displaystyle\\sum_{i=1}^{n} (y-x_i\\beta)^2\\right\\}\\)\n\\(\\Rightarrow \\hat{\\beta}^{\\text{LS}} : \\text{unbiased estimator}\\)\n\\(\\Rightarrow\\) 아이디어 : Bias가 올라가더라도 Variance를 더 낮출 수 있지 않을까?\n\\[\\text{Notation}\\] \\[\\hat{y} = \\beta_1x_1 + \\beta_2x_2 + ... + \\beta_{p}x_p\\] \\[\\text{or}\\] \\[\\hat{y} = \\beta_1x + \\beta_2x^2 + ... + \\beta_{p}x^p\\]\n\n\n\n\\(L(\\beta)=\\underset{\\beta}\\min \\displaystyle\\sum_{i=1}^{n} (y_i-\\hat{y}_i)^2 + \\lambda \\displaystyle\\sum_{j=1}^{p} \\beta_j^2 (\\lambda : \\text{hyper parameter})\\)\n- if) \\(\\lambda = \\infty\\)\n\\(\\Rightarrow \\beta_k \\approx 0(1 \\leqq k \\leqq p)\\)\n\\(\\Rightarrow \\text{Underfitting}\\)\n- if) \\(\\lambda=0\\)\n\\(\\Rightarrow \\text{Overfitting(LSE)}\\)\n\n\n\n\\(L(\\beta) = \\underset{\\beta}\\arg\\min \\left\\{\\displaystyle\\sum_{i=1}^{n}(y_i-\\hat{y})^2+\\lambda\\displaystyle\\sum_{j=1}^{p}\\vert \\beta_j\\vert\\right\\}\\)\n- if) \\(\\lambda = \\infty\\)\n\\(\\Rightarrow \\beta_k \\approx 0(1 \\leqq k \\leqq p)\\)\n\\(\\Rightarrow \\text{Underfitting}\\)\n- if) \\(\\lambda=0\\)\n\\(\\Rightarrow \\text{Overfitting(LSE)}\\)\n\n\nHyper parameter \\(\\lambda\\)를 구하는 방법은?"
  },
  {
    "objectID": "posts/Regularization.html#preview",
    "href": "posts/Regularization.html#preview",
    "title": "Regularization",
    "section": "",
    "text": "\\(\\begin{align}\n    E(MSE) & =E\\left[(Y-\\hat{Y})^2|X\\right]\\\\\n        & =E\\left(Y^2-2Y\\hat{Y}+\\hat{Y}^2 |X\\right)\\\\\n        & = \\sigma^2  + \\left\\{E(\\hat{Y}) - \\hat{Y} \\right\\}^2\\\\\n        & = \\sigma^2 + \\text{Bias}\\left(\\hat{Y}\\right) + \\text{Var}(\\hat{Y})\n\\end{align}\\)\n\\(\\hat{\\beta}^{\\text{LS}} = \\underset{\\beta}\\arg\\min \\left\\{\\displaystyle\\sum_{i=1}^{n} (y-x_i\\beta)^2\\right\\}\\)\n\\(\\Rightarrow \\hat{\\beta}^{\\text{LS}} : \\text{unbiased estimator}\\)\n\\(\\Rightarrow\\) 아이디어 : Bias가 올라가더라도 Variance를 더 낮출 수 있지 않을까?\n\\[\\text{Notation}\\] \\[\\hat{y} = \\beta_1x_1 + \\beta_2x_2 + ... + \\beta_{p}x_p\\] \\[\\text{or}\\] \\[\\hat{y} = \\beta_1x + \\beta_2x^2 + ... + \\beta_{p}x^p\\]"
  },
  {
    "objectID": "posts/Regularization.html#ridge",
    "href": "posts/Regularization.html#ridge",
    "title": "Regularization",
    "section": "",
    "text": "\\(L(\\beta)=\\underset{\\beta}\\min \\displaystyle\\sum_{i=1}^{n} (y_i-\\hat{y}_i)^2 + \\lambda \\displaystyle\\sum_{j=1}^{p} \\beta_j^2 (\\lambda : \\text{hyper parameter})\\)\n- if) \\(\\lambda = \\infty\\)\n\\(\\Rightarrow \\beta_k \\approx 0(1 \\leqq k \\leqq p)\\)\n\\(\\Rightarrow \\text{Underfitting}\\)\n- if) \\(\\lambda=0\\)\n\\(\\Rightarrow \\text{Overfitting(LSE)}\\)"
  },
  {
    "objectID": "posts/Regularization.html#lasso",
    "href": "posts/Regularization.html#lasso",
    "title": "Regularization",
    "section": "",
    "text": "\\(L(\\beta) = \\underset{\\beta}\\arg\\min \\left\\{\\displaystyle\\sum_{i=1}^{n}(y_i-\\hat{y})^2+\\lambda\\displaystyle\\sum_{j=1}^{p}\\vert \\beta_j\\vert\\right\\}\\)\n- if) \\(\\lambda = \\infty\\)\n\\(\\Rightarrow \\beta_k \\approx 0(1 \\leqq k \\leqq p)\\)\n\\(\\Rightarrow \\text{Underfitting}\\)\n- if) \\(\\lambda=0\\)\n\\(\\Rightarrow \\text{Overfitting(LSE)}\\)\n\n\nHyper parameter \\(\\lambda\\)를 구하는 방법은?"
  },
  {
    "objectID": "posts/JBIG코드실습_타이타닉(2주차).html",
    "href": "posts/JBIG코드실습_타이타닉(2주차).html",
    "title": "JBIG코드실습_타이타닉(2주차)",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\n\n\ntrain = pd.read_csv('./train_2.csv')\ntrain.head()\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n\n\n\n\n\n\n\n\n# Matplolib 으로 Survived의 분포 확인\nimport matplotlib.pyplot as plt\n\nsurvived_counts = train['Survived'].value_counts()\n\n# 막대 그래프 그리기\nplt.bar(survived_counts.index, survived_counts.values)\n\n# 그래프에 제목과 레이블 추가\nplt.title('Survived Distribution')\nplt.xlabel('Survived')\nplt.ylabel('Count')\n\n# x축의 눈금 레이블 설정\nplt.xticks(survived_counts.index, ['Not Survived', 'Survived'])\n\n# 그래프 출력\nplt.show()\n\n\n\n\n\n\n\n\n\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n# 데이터 불러오기\ntrain = pd.read_csv('./train_2.csv')\n\n# 전처리 (필요시 전처리 추가할 것)#########################################################################\ntrain['Age'] = train['Age'].fillna(29)\ntrain['Sex'] = train['Sex'].map({'male': 1, 'female': 0})\n# 위 5개 칼럼만 사용할 것\ntrain = train[[ 'Pclass', 'SibSp', 'Age', 'Sex', 'Survived']] \n\n# X값과 y값 구하기\nX = train.drop(columns=['Survived'])\ny = train['Survived']\n\n# 데이터 세트 분리\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n\n# 스케일링과 데이터 인코딩이 필요한 칼럼을 확인한 후 전처리를 진행하세요\n\n\n# PyCaret으로 가장 좋은 성능을 보이는 모델을 찾으세요.\nfrom pycaret.classification import *\n# PyCaret 설정\nexp1 = setup(train, target='Survived', session_id=123,use_gpu=True) \n\n# 모델 비교 및 평가\nbest_model = compare_models()\n\n[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n[LightGBM] [Info] Number of positive: 1, number of negative: 1\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 0\n[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n[LightGBM] [Info] Number of positive: 1, number of negative: 1\n[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n[LightGBM] [Info] Number of positive: 1, number of negative: 1\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 0\n[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n[LightGBM] [Info] Number of positive: 1, number of negative: 1\n[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n[LightGBM] [Info] Number of positive: 1, number of negative: 1\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 0\n[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n[LightGBM] [Info] Number of positive: 1, number of negative: 1\n[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n[LightGBM] [Info] Number of positive: 1, number of negative: 1\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 0\n[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n[LightGBM] [Info] Number of positive: 1, number of negative: 1\n[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n[LightGBM] [Info] Number of positive: 1, number of negative: 1\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 0\n[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n[LightGBM] [Info] Number of positive: 1, number of negative: 1\n[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n[LightGBM] [Info] Number of positive: 1, number of negative: 1\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 0\n[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n[LightGBM] [Info] Number of positive: 1, number of negative: 1\n[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n[LightGBM] [Info] Number of positive: 1, number of negative: 1\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 0\n[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n[LightGBM] [Info] Number of positive: 1, number of negative: 1\n[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n[LightGBM] [Info] Number of positive: 1, number of negative: 1\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 0\n[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n[LightGBM] [Info] Number of positive: 1, number of negative: 1\n\n\n\n\n\n\n\n \nDescription\nValue\n\n\n\n\n0\nSession id\n123\n\n\n1\nTarget\nSurvived\n\n\n2\nTarget type\nBinary\n\n\n3\nOriginal data shape\n(891, 5)\n\n\n4\nTransformed data shape\n(891, 5)\n\n\n5\nTransformed train set shape\n(623, 5)\n\n\n6\nTransformed test set shape\n(268, 5)\n\n\n7\nNumeric features\n4\n\n\n8\nPreprocess\nTrue\n\n\n9\nImputation type\nsimple\n\n\n10\nNumeric imputation\nmean\n\n\n11\nCategorical imputation\nmode\n\n\n12\nFold Generator\nStratifiedKFold\n\n\n13\nFold Number\n10\n\n\n14\nCPU Jobs\n-1\n\n\n15\nUse GPU\nTrue\n\n\n16\nLog Experiment\nFalse\n\n\n17\nExperiment Name\nclf-default-name\n\n\n18\nUSI\n58d4\n\n\n\n\n\n\n\n\n\n\n\n\n\n \nModel\nAccuracy\nAUC\nRecall\nPrec.\nF1\nKappa\nMCC\nTT (Sec)\n\n\n\n\ngbc\nGradient Boosting Classifier\n0.8073\n0.8457\n0.6946\n0.7800\n0.7322\n0.5831\n0.5874\n0.0800\n\n\nlightgbm\nLight Gradient Boosting Machine\n0.8025\n0.8472\n0.6906\n0.7725\n0.7259\n0.5731\n0.5780\n1.4880\n\n\nada\nAda Boost Classifier\n0.8024\n0.8450\n0.7362\n0.7502\n0.7393\n0.5808\n0.5847\n0.0830\n\n\nridge\nRidge Classifier\n0.8008\n0.0000\n0.6946\n0.7663\n0.7258\n0.5706\n0.5745\n0.0160\n\n\nlda\nLinear Discriminant Analysis\n0.7992\n0.8544\n0.6946\n0.7638\n0.7245\n0.5677\n0.5717\n0.0170\n\n\nrf\nRandom Forest Classifier\n0.7977\n0.8239\n0.6987\n0.7601\n0.7242\n0.5654\n0.5704\n0.2370\n\n\nqda\nQuadratic Discriminant Analysis\n0.7928\n0.8521\n0.7321\n0.7320\n0.7263\n0.5606\n0.5661\n0.0170\n\n\nlr\nLogistic Regression\n0.7897\n0.8549\n0.6989\n0.7420\n0.7148\n0.5494\n0.5545\n0.0210\n\n\net\nExtra Trees Classifier\n0.7881\n0.7965\n0.6777\n0.7481\n0.7084\n0.5430\n0.5470\n0.2040\n\n\ndt\nDecision Tree Classifier\n0.7866\n0.7693\n0.6781\n0.7417\n0.7057\n0.5395\n0.5431\n0.0170\n\n\nknn\nK Neighbors Classifier\n0.7624\n0.8037\n0.6114\n0.7284\n0.6609\n0.4814\n0.4882\n0.0760\n\n\nnb\nNaive Bayes\n0.7577\n0.8426\n0.7071\n0.6797\n0.6874\n0.4907\n0.4970\n0.0170\n\n\nsvm\nSVM - Linear Kernel\n0.6257\n0.0000\n0.8321\n0.6098\n0.6406\n0.3214\n0.3598\n0.0160\n\n\ndummy\nDummy Classifier\n0.6164\n0.5000\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n0.0150\n\n\n\n\n\n\n\n\n\n# PyCaret에서 찾은 모델을 \n# 직접 sklearn의 GridSearch를 통해 하이퍼 파라미터를 튜닝해보세요.\n\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.model_selection import GridSearchCV\n\n# Gradient Boosting Classifier 모델 정의\ngb_classifier = GradientBoostingClassifier()\n\n# 탐색할 하이퍼파라미터 그리드 정의\nparam_grid = {\n    'n_estimators': [50, 100, 150],\n    'learning_rate': [0.05, 0.1, 0.2],\n    'max_depth': [3, 4, 5]\n}\n\n# GridSearchCV를 사용하여 하이퍼파라미터 튜닝\ngrid_search = GridSearchCV(estimator=gb_classifier, param_grid=param_grid, cv=5)\ngrid_search.fit(X, y)\n\n# 최적의 하이퍼파라미터와 점수 출력\nprint(\"Best Parameters:\", grid_search.best_params_)\nprint(\"Best Score:\", grid_search.best_score_)\n\nBest Parameters: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 150}\nBest Score: 0.8327788588286987\n\n\n\n# best params로 교차검증 및 StratifiedKFold를 진행하세요.\n# 최적의 하이퍼파라미터로 교차검증 및 StratifiedKFold 진행\nfrom sklearn.model_selection import cross_val_score, StratifiedKFold\nbest_params = grid_search.best_params_\ngb_classifier = GradientBoostingClassifier(**best_params)\nstratified_kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\ncv_scores = cross_val_score(gb_classifier, X, y, cv=stratified_kfold)\n\n# 교차검증 결과 출력\nprint(\"Cross Validation Scores:\", cv_scores)\nprint(\"Mean CV Score:\", cv_scores.mean())\n\nCross Validation Scores: [0.84916201 0.8258427  0.79775281 0.80898876 0.83707865]\nMean CV Score: 0.8237649865042999\n\n\n\n# 앙상블 기법 중 배깅을 사용해보세요.\n\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\n\n# 데이터를 훈련 세트와 테스트 세트로 나눔\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# 배깅 분류기 정의\nbagging_classifier = BaggingClassifier(gb_classifier, n_estimators=10, random_state=42)\n\n# 배깅 분류기를 훈련 데이터에 적합\ngb_classifier.fit(X, y)\n\n# 테스트 데이터에 대한 예측\ny_pred = gb_classifier.predict(X)\n\n# 정확도 평가\naccuracy = accuracy_score(y, y_pred)\nprint(\"Accuracy:\", accuracy)\n\nAccuracy: 0.8731762065095399"
  },
  {
    "objectID": "posts/랜덤포레스트.html",
    "href": "posts/랜덤포레스트.html",
    "title": "랜덤포레스트(배깅,보팅)",
    "section": "",
    "text": "유튜브 김성범 교수님 강의 , 전북대학교 최규빈 교수님 강의노트로 학습한 것을 토대로 작성하였습니다.\n\n\n\n\n\n계층적 구조로 인해 중간에 에러가 발생하면 다음 단계에도 에러가 계속 전파됩니다.\n학습 데이터의 미세한 변동에도 최종 결과에 크게 영향을 끼칠 수 있습니다.\n적은 개수의 노이즈에도 크게 영향을 받습니다.\n나무의 최종노드 개수를 늘리면 과적합(Overfitting) 위험이 발생합니다.\n\\(\\rightarrow\\) 해결방법 : 랜덤포레스트\n\n\n\n\n\n여러 Base모델(의사결정나무)들의 예측을 다수결법칙 or 평균을 이용해 통합하여 예측 정확성을 향상시키는 모델\n랜덤포레스트 구조를 시각화하면 다음과 같습니다.\n\n\n\n\nRandom_Forest_Structure.jpg\n\n\n\n여기서 Base모델은 서로 독립이고, 무작위 예측보다 성능이 좋아야한다는 전제조건이 필요합니다.\n랜덤포레스트의 중요한 두가지 키워드는 Diversity와 Random subspace입니다.\n\n\n\n\n\n여러 Training data를 생성하여 각 데이터마다 개별 의사결정나무 모델로 구축\n그럼 개별 의사결정나무에 어떻게 데이터를 할당해주지? \\(\\rightarrow\\) Bagging 이용\n\n\n\n\n각 모델은 서로 다른 학습 데이터 셋을 이용합니다.\n각 모델에 쓰이는 데이터는 원본데이터의 복원추출로 생성됩니다.\n각 데이터 셋은 원본 데이터의 개수만큼 복원추출을 시행합니다.\n\\(\\rightarrow\\) 원본 데이터의 개수가 n개라면 n번의 복원추출 시행\n위에 랜덤포레스트 구조를 시각화한 것이 이제는 이해가 가시나요?\n\n\n\n\n\n각 개별 트리가 노드를 뻗어나갈 때 필요한 변수는 무작위로 선택됩니다.\n\n\n\n\n\n\n학습된 각각의 의사결정나무들로 어떻게 결과를 도출할까요? \\(\\rightarrow\\) Voting이용\n다음 소개되는 Voting 방법들의 예시는 모두 Classification입니다.\nRegression : \\(f(x) = \\displaystyle\\sum_{m=1}^{M}c_mI(x \\in R_m)\\)\nClassification : \\(f(x) = \\displaystyle\\sum_{m=1}^{M} k(m)I\\left\\{(x_1,x_2)\\in R_m\\right\\}\\)\n\n\n\n\n\\(Ensemble(\\hat{y}) = \\underset{i}\\arg\\max \\left(\\displaystyle\\sum_{i=1}^{n}I(\\hat{y} \\in i), i \\in \\left\\{0,1\\right\\} \\right)\\)\nEX) \\(\\displaystyle\\sum_{i=1}^{n}I(\\hat{y} \\in 0)=4\\) , \\(\\displaystyle\\sum_{i=1}^{n}I(\\hat{y} \\in 1)=6 \\;\\Rightarrow\\; Ensemble(\\hat{y})=1\\)\n\n1이라고 예측한 모델이 6개, 0이라고 예측한 모델이 4개 즉, 다수결 투표로 1이 예측값이 되었습니다.(Hard Voting)\n\n\n\n\n\n\\(Ensemble(\\hat{y}) = \\underset{i}\\arg\\max \\Bigg(\\displaystyle\\frac{\\sum_{j=1}^{n}(TrainAcc_{j})I(\\hat{y}=1)}{\\sum_{j=1}^{n}(TrainAcc_{j})}, i \\in \\left\\{0,1 \\right\\}\\Bigg)\\)\n\nEX) \\(\\displaystyle\\frac{\\sum_{j=1}^{n}(TrainAcc_{j})I(\\hat{y}=0)}{\\sum_{j=1}^{n}(TrainAcc_{j})} = 0.4\\) , \\(\\displaystyle\\frac{\\sum_{j=1}^{n}(TrainAcc_{j})I(\\hat{y}=1)}{\\sum_{j=1}^{n}(TrainAcc_{j})} = 0.6\\)\n\n\\(\\Rightarrow\\; Ensemble(\\hat{y}) = 1\\) * 각 모델의 Accuracy를 바탕으로 가중치가 부여됩니다. 높은 Accuracy를 가진 모델은 더 많은 가중치를 갖게 됩니다.(가중치 평균을 이용한 Voting)\n\n\n\n\n\\(Ensemble(\\hat{y}) = \\underset{i}\\arg\\max\\big(\\displaystyle\\frac1n \\sum_{j=1}^{n}P(\\hat{y}=i), i \\in \\left\\{0,1 \\right\\}\\big)\\)\n\nEX) \\(\\displaystyle\\frac1n \\sum_{j=1}^{n}P(\\hat{y}=0)=0.3\\) , \\(\\displaystyle\\frac1n \\sum_{j=1}^{n}P(\\hat{y}=1)=0.7 \\;\\Rightarrow\\; Ensemble(\\hat{y}) = 1\\)\n\n각 모델들이 예측할 확률들의 평균을 구한 후에, 다수결을 이용하여 최조 선택하는 방식입니다.\n0이 나올 확률들의 평균값은 0.3 , 1이 나올 확률들의 평균값은 0.7이므로 1이 예측값이 되었습니다.(Soft Voting)\n\n\n\n실제로 Voting은 랜덤포레스트에서만 사용되는 앙상블 기법은 아닙니다. 랜덤포레스트에서 Voting은 동일한 모델(의사결정나무)들에서 Voting을 하는 반면에, 좀 더 일반적인 Voting은 다양한 모델에서 투표를 하는 방식으로 이루어집니다.\n\n\n\n\n\n(추후 업로드 예정)"
  },
  {
    "objectID": "posts/랜덤포레스트.html#랜덤포레스트",
    "href": "posts/랜덤포레스트.html#랜덤포레스트",
    "title": "랜덤포레스트(배깅,보팅)",
    "section": "",
    "text": "유튜브 김성범 교수님 강의 , 전북대학교 최규빈 교수님 강의노트로 학습한 것을 토대로 작성하였습니다.\n\n\n\n\n\n계층적 구조로 인해 중간에 에러가 발생하면 다음 단계에도 에러가 계속 전파됩니다.\n학습 데이터의 미세한 변동에도 최종 결과에 크게 영향을 끼칠 수 있습니다.\n적은 개수의 노이즈에도 크게 영향을 받습니다.\n나무의 최종노드 개수를 늘리면 과적합(Overfitting) 위험이 발생합니다.\n\\(\\rightarrow\\) 해결방법 : 랜덤포레스트\n\n\n\n\n\n여러 Base모델(의사결정나무)들의 예측을 다수결법칙 or 평균을 이용해 통합하여 예측 정확성을 향상시키는 모델\n랜덤포레스트 구조를 시각화하면 다음과 같습니다.\n\n\n\n\nRandom_Forest_Structure.jpg\n\n\n\n여기서 Base모델은 서로 독립이고, 무작위 예측보다 성능이 좋아야한다는 전제조건이 필요합니다.\n랜덤포레스트의 중요한 두가지 키워드는 Diversity와 Random subspace입니다.\n\n\n\n\n\n여러 Training data를 생성하여 각 데이터마다 개별 의사결정나무 모델로 구축\n그럼 개별 의사결정나무에 어떻게 데이터를 할당해주지? \\(\\rightarrow\\) Bagging 이용\n\n\n\n\n각 모델은 서로 다른 학습 데이터 셋을 이용합니다.\n각 모델에 쓰이는 데이터는 원본데이터의 복원추출로 생성됩니다.\n각 데이터 셋은 원본 데이터의 개수만큼 복원추출을 시행합니다.\n\\(\\rightarrow\\) 원본 데이터의 개수가 n개라면 n번의 복원추출 시행\n위에 랜덤포레스트 구조를 시각화한 것이 이제는 이해가 가시나요?\n\n\n\n\n\n각 개별 트리가 노드를 뻗어나갈 때 필요한 변수는 무작위로 선택됩니다.\n\n\n\n\n\n\n학습된 각각의 의사결정나무들로 어떻게 결과를 도출할까요? \\(\\rightarrow\\) Voting이용\n다음 소개되는 Voting 방법들의 예시는 모두 Classification입니다.\nRegression : \\(f(x) = \\displaystyle\\sum_{m=1}^{M}c_mI(x \\in R_m)\\)\nClassification : \\(f(x) = \\displaystyle\\sum_{m=1}^{M} k(m)I\\left\\{(x_1,x_2)\\in R_m\\right\\}\\)\n\n\n\n\n\\(Ensemble(\\hat{y}) = \\underset{i}\\arg\\max \\left(\\displaystyle\\sum_{i=1}^{n}I(\\hat{y} \\in i), i \\in \\left\\{0,1\\right\\} \\right)\\)\nEX) \\(\\displaystyle\\sum_{i=1}^{n}I(\\hat{y} \\in 0)=4\\) , \\(\\displaystyle\\sum_{i=1}^{n}I(\\hat{y} \\in 1)=6 \\;\\Rightarrow\\; Ensemble(\\hat{y})=1\\)\n\n1이라고 예측한 모델이 6개, 0이라고 예측한 모델이 4개 즉, 다수결 투표로 1이 예측값이 되었습니다.(Hard Voting)\n\n\n\n\n\n\\(Ensemble(\\hat{y}) = \\underset{i}\\arg\\max \\Bigg(\\displaystyle\\frac{\\sum_{j=1}^{n}(TrainAcc_{j})I(\\hat{y}=1)}{\\sum_{j=1}^{n}(TrainAcc_{j})}, i \\in \\left\\{0,1 \\right\\}\\Bigg)\\)\n\nEX) \\(\\displaystyle\\frac{\\sum_{j=1}^{n}(TrainAcc_{j})I(\\hat{y}=0)}{\\sum_{j=1}^{n}(TrainAcc_{j})} = 0.4\\) , \\(\\displaystyle\\frac{\\sum_{j=1}^{n}(TrainAcc_{j})I(\\hat{y}=1)}{\\sum_{j=1}^{n}(TrainAcc_{j})} = 0.6\\)\n\n\\(\\Rightarrow\\; Ensemble(\\hat{y}) = 1\\) * 각 모델의 Accuracy를 바탕으로 가중치가 부여됩니다. 높은 Accuracy를 가진 모델은 더 많은 가중치를 갖게 됩니다.(가중치 평균을 이용한 Voting)\n\n\n\n\n\\(Ensemble(\\hat{y}) = \\underset{i}\\arg\\max\\big(\\displaystyle\\frac1n \\sum_{j=1}^{n}P(\\hat{y}=i), i \\in \\left\\{0,1 \\right\\}\\big)\\)\n\nEX) \\(\\displaystyle\\frac1n \\sum_{j=1}^{n}P(\\hat{y}=0)=0.3\\) , \\(\\displaystyle\\frac1n \\sum_{j=1}^{n}P(\\hat{y}=1)=0.7 \\;\\Rightarrow\\; Ensemble(\\hat{y}) = 1\\)\n\n각 모델들이 예측할 확률들의 평균을 구한 후에, 다수결을 이용하여 최조 선택하는 방식입니다.\n0이 나올 확률들의 평균값은 0.3 , 1이 나올 확률들의 평균값은 0.7이므로 1이 예측값이 되었습니다.(Soft Voting)\n\n\n\n실제로 Voting은 랜덤포레스트에서만 사용되는 앙상블 기법은 아닙니다. 랜덤포레스트에서 Voting은 동일한 모델(의사결정나무)들에서 Voting을 하는 반면에, 좀 더 일반적인 Voting은 다양한 모델에서 투표를 하는 방식으로 이루어집니다.\n\n\n\n\n\n(추후 업로드 예정)"
  },
  {
    "objectID": "posts/의사결정나무.html",
    "href": "posts/의사결정나무.html",
    "title": "의사결정나무",
    "section": "",
    "text": "유튜브 김성범 교수님 강의 , 전북대학교 최규빈 교수님 강의노트로 학습한 것을 토대로 작성하였습니다.\n\n- 파이썬 코드로 간단한 예시를 들어보겠습니다.\n\n# import\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.animation\nimport IPython\nimport sklearn.tree\n#---#\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\n# visualization\nnp.random.seed(43052)\ntemp = pd.read_csv('https://raw.githubusercontent.com/guebin/DV2022/master/posts/temp.csv').iloc[:,3].to_numpy()[:100]\ntemp.sort()\neps = np.random.randn(100)*3 # 오차\nicecream_sales = 20 + temp * 2.5 + eps \ndf_train = pd.DataFrame({'temp':temp,'sales':icecream_sales})\nX = df_train[['temp']]\ny = df_train['sales']\nplt.plot(X,y,'o')\nplt.xlabel('temp')\nplt.ylabel('sales')\nplt.show()\n\n\n\n\n\n\n\n\n\n그림과 같이 온도에 따른 아이스크림 판매량의 그래프가 있습니다.\n여기서 특정 온도에 대한 아이스크림 판매량을 의사결정나무로 예측해보겠습니다.\n\n\npdtr = sklearn.tree.DecisionTreeRegressor(max_depth=1)\npdtr.fit(X,y)\n\nDecisionTreeRegressor(max_depth=1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DecisionTreeRegressorDecisionTreeRegressor(max_depth=1)\n\n\n\nsklearn.tree.plot_tree(\n    pdtr,\n    feature_names=X.columns\n                      );\nfig1 = plt.gcf()\nfig1.suptitle(\"Tree1\")\n\nText(0.5, 0.98, 'Tree1')\n\n\n\n\n\n\n\n\n\n\n그림과 같이 변수 temp를 특정 기준(5.05)으로 나누어서 분류해줍니다.\n지금은 최대깊이가(max_depth) 1로 지정한 트리입니다.\n최대깊이(max_depth)를 늘린 트리는 다음과 같습니다.\n\n\npdtr2 = sklearn.tree.DecisionTreeRegressor(max_depth=3)\npdtr2.fit(X,y)\n\nDecisionTreeRegressor(max_depth=3)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DecisionTreeRegressorDecisionTreeRegressor(max_depth=3)\n\n\n\nsklearn.tree.plot_tree(\n    pdtr2,\n    max_depth=3,\n    feature_names=X.columns\n                      );\nfig2 = plt.gcf()\nfig2.suptitle(\"Tree2\")\n\nText(0.5, 0.98, 'Tree2')\n\n\n\n\n\n\n\n\n\n\n이처럼 의사결정나무는 특정 변수에 대해 기준을 나누어 가지를 뻗어나가는 메커니즘입니다.\n나무를 뒤짚어놓은 모양처럼 생겨서 의사결정나무라고 불립니다.\n각각의 박스들을 노드라고 부릅니다.\n맨 위의 노드를 뿌리노드(Root node), 맨 아리의 노드를 끝노드(Terminal node)라고 부릅니다.\n\n- Tree1의 좌표평면에 대한 시각화는 다음과 같습니다.\n\nplt.plot(X,y,'o',alpha=0.5,label='True')\nplt.plot(X,pdtr.predict(X),'--.',label='Predicted')\nplt.legend()\n\n\n\n\n\n\n\n\n- Tree2의 좌표평면에 대한 시각화는 다음과 같습니다.\n\nplt.plot(X,y,'o',alpha=0.5,label='True')\nplt.plot(X,pdtr2.predict(X),'.--',label='Predicted')\nplt.legend()\n\n\n\n\n\n\n\n\n\nTree1에 비해서 Tree2가 더 잘맞출 것 같습니다.\n하지만 max depth가 너무 많이 커진다면 과적합(Overfittig)의 위험이 생깁니다.\nmax depth에 따른 좌포평면 시각화 애니메이션은 다음과 같습니다.\n\n\n\n\n## step1 \nX = df_train[['temp']]\ny = df_train['sales']\n## step2\npredictrs = [sklearn.tree.DecisionTreeRegressor(max_depth=k) for k in range(1,11)]\n## step3 \nfor k in range(10):\n    predictrs[k].fit(X,y)\n## step4 -- pass\nfig = plt.figure()\n\n&lt;Figure size 432x288 with 0 Axes&gt;\n\n\n\ndef func(frame):\n    ax = fig.gca()\n    ax.clear()\n    ax.plot(X,y,'o',alpha=0.5) \n    ax.plot(X,predictrs[frame].predict(X),'.--') \n    ax.set_title(f'max_depth={predictrs[frame].max_depth}')\n\n\nani = matplotlib.animation.FuncAnimation(\n    fig,\n    func,\n    frames=10\n)\n\n\ndisplay(IPython.display.HTML(ani.to_jshtml()))\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n- 시각화처럼 max depth가 너무 커지면 과적합(Overfitting)의 위험이 증가합니다.\n\n\n\n(추후 업로드 예정)"
  },
  {
    "objectID": "posts/의사결정나무.html#의사결정나무",
    "href": "posts/의사결정나무.html#의사결정나무",
    "title": "의사결정나무",
    "section": "",
    "text": "유튜브 김성범 교수님 강의 , 전북대학교 최규빈 교수님 강의노트로 학습한 것을 토대로 작성하였습니다.\n\n- 파이썬 코드로 간단한 예시를 들어보겠습니다.\n\n# import\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.animation\nimport IPython\nimport sklearn.tree\n#---#\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\n# visualization\nnp.random.seed(43052)\ntemp = pd.read_csv('https://raw.githubusercontent.com/guebin/DV2022/master/posts/temp.csv').iloc[:,3].to_numpy()[:100]\ntemp.sort()\neps = np.random.randn(100)*3 # 오차\nicecream_sales = 20 + temp * 2.5 + eps \ndf_train = pd.DataFrame({'temp':temp,'sales':icecream_sales})\nX = df_train[['temp']]\ny = df_train['sales']\nplt.plot(X,y,'o')\nplt.xlabel('temp')\nplt.ylabel('sales')\nplt.show()\n\n\n\n\n\n\n\n\n\n그림과 같이 온도에 따른 아이스크림 판매량의 그래프가 있습니다.\n여기서 특정 온도에 대한 아이스크림 판매량을 의사결정나무로 예측해보겠습니다.\n\n\npdtr = sklearn.tree.DecisionTreeRegressor(max_depth=1)\npdtr.fit(X,y)\n\nDecisionTreeRegressor(max_depth=1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DecisionTreeRegressorDecisionTreeRegressor(max_depth=1)\n\n\n\nsklearn.tree.plot_tree(\n    pdtr,\n    feature_names=X.columns\n                      );\nfig1 = plt.gcf()\nfig1.suptitle(\"Tree1\")\n\nText(0.5, 0.98, 'Tree1')\n\n\n\n\n\n\n\n\n\n\n그림과 같이 변수 temp를 특정 기준(5.05)으로 나누어서 분류해줍니다.\n지금은 최대깊이가(max_depth) 1로 지정한 트리입니다.\n최대깊이(max_depth)를 늘린 트리는 다음과 같습니다.\n\n\npdtr2 = sklearn.tree.DecisionTreeRegressor(max_depth=3)\npdtr2.fit(X,y)\n\nDecisionTreeRegressor(max_depth=3)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DecisionTreeRegressorDecisionTreeRegressor(max_depth=3)\n\n\n\nsklearn.tree.plot_tree(\n    pdtr2,\n    max_depth=3,\n    feature_names=X.columns\n                      );\nfig2 = plt.gcf()\nfig2.suptitle(\"Tree2\")\n\nText(0.5, 0.98, 'Tree2')\n\n\n\n\n\n\n\n\n\n\n이처럼 의사결정나무는 특정 변수에 대해 기준을 나누어 가지를 뻗어나가는 메커니즘입니다.\n나무를 뒤짚어놓은 모양처럼 생겨서 의사결정나무라고 불립니다.\n각각의 박스들을 노드라고 부릅니다.\n맨 위의 노드를 뿌리노드(Root node), 맨 아리의 노드를 끝노드(Terminal node)라고 부릅니다.\n\n- Tree1의 좌표평면에 대한 시각화는 다음과 같습니다.\n\nplt.plot(X,y,'o',alpha=0.5,label='True')\nplt.plot(X,pdtr.predict(X),'--.',label='Predicted')\nplt.legend()\n\n\n\n\n\n\n\n\n- Tree2의 좌표평면에 대한 시각화는 다음과 같습니다.\n\nplt.plot(X,y,'o',alpha=0.5,label='True')\nplt.plot(X,pdtr2.predict(X),'.--',label='Predicted')\nplt.legend()\n\n\n\n\n\n\n\n\n\nTree1에 비해서 Tree2가 더 잘맞출 것 같습니다.\n하지만 max depth가 너무 많이 커진다면 과적합(Overfittig)의 위험이 생깁니다.\nmax depth에 따른 좌포평면 시각화 애니메이션은 다음과 같습니다.\n\n\n\n\n## step1 \nX = df_train[['temp']]\ny = df_train['sales']\n## step2\npredictrs = [sklearn.tree.DecisionTreeRegressor(max_depth=k) for k in range(1,11)]\n## step3 \nfor k in range(10):\n    predictrs[k].fit(X,y)\n## step4 -- pass\nfig = plt.figure()\n\n&lt;Figure size 432x288 with 0 Axes&gt;\n\n\n\ndef func(frame):\n    ax = fig.gca()\n    ax.clear()\n    ax.plot(X,y,'o',alpha=0.5) \n    ax.plot(X,predictrs[frame].predict(X),'.--') \n    ax.set_title(f'max_depth={predictrs[frame].max_depth}')\n\n\nani = matplotlib.animation.FuncAnimation(\n    fig,\n    func,\n    frames=10\n)\n\n\ndisplay(IPython.display.HTML(ani.to_jshtml()))\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n- 시각화처럼 max depth가 너무 커지면 과적합(Overfitting)의 위험이 증가합니다.\n\n\n\n(추후 업로드 예정)"
  },
  {
    "objectID": "posts/GBM.html",
    "href": "posts/GBM.html",
    "title": "GBM",
    "section": "",
    "text": "- 다음은 wikipedia의 Gradient Boosting Algorithm입니다. 다음의 수식을 먼저 읽고 최대한 이해하려고 노력해봅시다.\n\n\n\n\n\n\n\n\n1. initialize model with a constant value:\n\\(F_0(x) = \\underset{\\gamma}{\\arg\\min}\\sum_{i=1}^n L(y_i, \\gamma)\\)\n\n\n2. For m = 1 to M : 1. Compute so-called pseudo-residuals\n\\(r_{im} = - \\Bigg[\\displaystyle\\frac{\\partial L(y_i, F(x_i))}{\\partial F(x_i)}\\Bigg]_{F(x) = F_{m-1}(x)}\\) for \\(i = 1, ... ,n\\) 2. Fit a base learner (or weak learner, e.g. tree) closed under scaling \\(h_m(x)\\) to pseuo-residuals, i.e. train it using the training set \\(\\{(x_i, r_{im})\\}_{i=1}^{n}\\) 3. Compute multiplier \\(\\gamma_{m}\\) by solving the following one-dimensional optimization problem:\n\\(\\gamma_m = \\underset{\\gamma}{\\arg\\min}\\sum_{i=1}^n L(y_i, F_{m-1}(x_i) + \\gamma h_m(x_i))\\) 4. Update the model:\n\\(F_m(x) = F_{m-1}(x) + \\gamma_m h_m(x)\\)\n\n\n3. Output \\(\\hat{f(x)} = F_M(x)\\)\n\n\n\n수식으로만 완전히 이해하기가 어렵기 때문에 간단한 예시와 pandas dataframe을 이용하여 설명해보겠습니다.\n다음과 같은 데이터가 있다고 가정해보겠습니다.\n\nimport pandas as pd\nidx = ['학생1', '학생2', '학생3', '학생4']\ndata = pd.DataFrame({'공부시간(m)':[150, 120, 60, 80], '성별':['여','남','남', '여'], '수학점수':[90, 65, 55, '???']},index=idx)\ndata\n\n\n\n\n\n\n\n\n공부시간(m)\n성별\n수학점수\n\n\n\n\n학생1\n150\n여\n90\n\n\n학생2\n120\n남\n65\n\n\n학생3\n60\n남\n55\n\n\n학생4\n80\n여\n???\n\n\n\n\n\n\n\n다음과 같은 데이터가 있다고 할 때, 저희는 학생4의 수학점수를 공부시간(m)을 통하여 예측하고 싶습니다.\n\n\n\n\n\n1. initialize model with a constant value:\\\n\n\n\\(F_0(x) = \\underset{\\gamma}{\\arg\\min}\\sum_{i=1}^n L(y_i, \\gamma)\\)\n\n\n\n\n초기모델을 상수로 정의합니다. 여기서 상수는 \\(y_i\\)와 \\(\\gamma\\)입니다.\n\\(y_i\\)는 수학점수를 의미하고 \\(\\gamma\\)는 초기 예측값입니다.\n\n\\(y_1\\) = 90, \\(y_2\\) = 65, \\(y_3\\) = 60\n\n여기서의 Loss Function \\(L(y_i, \\gamma) = \\displaystyle\\frac1n \\sum_{i=1}^{n} (y_i - \\gamma)^2\\)입니다. (잘 모르시는 분들은 MSE 학습하시면 좋을 것 같습니다.)\n예시의 수치를 대입하면\n\\(L(y_i, \\gamma) = \\displaystyle\\frac13 \\sum_{i=1}^{3} (y_i - \\gamma)^2 =  \\displaystyle\\frac13(90 - \\gamma)^2 + \\displaystyle\\frac13(65 - \\gamma)^2 + \\displaystyle\\frac13(55 - \\gamma)^2\\)입니다.\n\\(L(y_i, \\gamma)\\)가 최소가 되는 \\(\\gamma\\)를 찾아야 하므로 \\(\\displaystyle\\frac{\\partial L}{\\partial \\gamma} = \\displaystyle\\frac23(\\gamma - 90) + \\displaystyle\\frac23(\\gamma - 65) + \\displaystyle\\frac23(\\gamma - 55) = \\displaystyle\\frac23(3\\gamma - 210)\\)입니다.\n\\(\\displaystyle\\frac{\\partial L}{\\partial \\gamma} = 0\\)이 되어야 하므로 \\(\\gamma\\) = 70입니다. 따라서 초기 예측값 \\(\\gamma\\)는 70이 됩니다.\n초기 예측값을 이용한 첫번째 잔차는 각각 20, -5, -15가 됩니다.\n\n\nimport pandas as pd\nidx = ['Student1', 'Student2', 'Student3']\ndata = pd.DataFrame({'StudyTime(m)':[150, 120, 60], 'Sex':['female','male','male'], \n                     'MathScore':[90, 65, 55], 'F_0':[70 for i in range(3)],'r1': [20, -5, -15]},index=idx)\ndata\n\n\n\n\n\n\n\n\nStudyTime(m)\nSex\nMathScore\nF_0\nr1\n\n\n\n\nStudent1\n150\nfemale\n90\n70\n20\n\n\nStudent2\n120\nmale\n65\n70\n-5\n\n\nStudent3\n60\nmale\n55\n70\n-15\n\n\n\n\n\n\n\n- 위의 데이터를 의사결정나무로 시각화를 하면 다음과 같습니다.\n\nimport sklearn.tree\n## step1\nx = data[['StudyTime(m)']]\ny = data['MathScore']\n\n## step2\npredictr = sklearn.tree.DecisionTreeRegressor(max_depth=1)\n\n## step3\npredictr.fit(x,y)\n\n## visualization\nsklearn.tree.plot_tree(predictr,feature_names=x.columns);\n\n\n\n\n\n\n\n\n\n\n\n\nFor \\(m\\) = 1 to \\(M\\):\n\nCompute so-called pseudo-residuals \\(r_{im} = - \\Bigg[\\displaystyle\\frac{\\partial L(y_i, F(x_i))}{\\partial F(x_i)}\\Bigg]_{F(x) = F_{m-1}(x)}\\) for \\(i = 1, ... ,n\\)\nFit a base learner (or weak learner, e.g. tree) closed under scaling \\(h_m(x)\\) to pseuo-residuals, i.e. train it using the training set \\(\\{(x_i, r_{im})\\}_{i=1}^{n}\\)\nCompute multiplier \\(\\gamma_{m}\\) by solving the following one-dimensional optimization problem:\n\\(\\gamma_m = \\underset{\\gamma}{\\arg\\min}\\sum_{i=1}^n L(y_i, F_{m-1}(x_i) + \\gamma h_m(x_i))\\)\nUpdate the model:\n\\(F_m(x) = F_{m-1}(x) + \\gamma_m h_m(x)\\)\n\nA단계부터 D단계를 총 M번 반복하는 단계입니다.\n여기서 \\(M\\)은 예측기의 개수, \\(n\\)은 데이터의 개수입니다.\n예측기에 번호를 붙여 \\(m\\)=1인 경우는 1번 예측기 \\(m=M\\)인 경우는 M번 예측기라고 부르겠습니다.\n1번 예측기(\\(m=1\\))가 생성 되는 과정을 살펴보겠습니다. \\(L(y_i,F(x_i)) = \\displaystyle\\frac12(y_i - F(x_i))^2\\)이므로\n\n\\(r_{i1} = - \\Bigg[\\displaystyle\\frac{\\partial L(y_i, F(x_i))}{\\partial F(x_i)}\\Bigg]_{F(x) = F_{0}(x)} = y_i - F_0(x_i)\\)입니다. (잔차와 식이 동일합니다.)\n0번 예측기 \\(F_0(x)\\)는 1단계의 초기모델에서 생성되었습니다. 따라서 \\(F_0(x)=70\\)입니다.(1단계 참고)\n따라서 \\(r_{11}=20\\), \\(r_{21}=-5\\), \\(r_{31}=-15\\)입니다. 1단계 표에 있는 잔차1과 같습니다.\n함수\\(h_1(x)\\)는 앞서 계산한 잔차를 트리 계열로 학습합니다.(의사 결정 나무에 대해서는 추후에 업로드 하겠습니다.)\n모든 데이터(\\(n=3\\))에 대해서 \\(\\sum_{i=1}^{3} L(y_i, F_0(x_i) + \\gamma h_m(x_i)\\) 식이 최소가 되는 \\(\\gamma\\)(learning rate)를 찾습니다.(\\(\\gamma = \\gamma_{1}\\))\n모델을 업데이트 합니다 : \\(F_1(x) = F_0(x) + \\gamma_1 h_1(x)\\)\n\n4번의 과정을 M번 반복합니다.\n\n- 추가설명\n\n\n\n\n\n\n3. Output \\(\\hat{f(x)} = F_M(x)\\)\n\n\n\n\n최종 모델을 산출합니다.\n\n\n\n\n\n(추후 업로드 예정)"
  },
  {
    "objectID": "posts/GBM.html#algorithm",
    "href": "posts/GBM.html#algorithm",
    "title": "GBM",
    "section": "",
    "text": "1. initialize model with a constant value:\n\\(F_0(x) = \\underset{\\gamma}{\\arg\\min}\\sum_{i=1}^n L(y_i, \\gamma)\\)\n\n\n2. For m = 1 to M : 1. Compute so-called pseudo-residuals\n\\(r_{im} = - \\Bigg[\\displaystyle\\frac{\\partial L(y_i, F(x_i))}{\\partial F(x_i)}\\Bigg]_{F(x) = F_{m-1}(x)}\\) for \\(i = 1, ... ,n\\) 2. Fit a base learner (or weak learner, e.g. tree) closed under scaling \\(h_m(x)\\) to pseuo-residuals, i.e. train it using the training set \\(\\{(x_i, r_{im})\\}_{i=1}^{n}\\) 3. Compute multiplier \\(\\gamma_{m}\\) by solving the following one-dimensional optimization problem:\n\\(\\gamma_m = \\underset{\\gamma}{\\arg\\min}\\sum_{i=1}^n L(y_i, F_{m-1}(x_i) + \\gamma h_m(x_i))\\) 4. Update the model:\n\\(F_m(x) = F_{m-1}(x) + \\gamma_m h_m(x)\\)\n\n\n3. Output \\(\\hat{f(x)} = F_M(x)\\)\n\n\n\n수식으로만 완전히 이해하기가 어렵기 때문에 간단한 예시와 pandas dataframe을 이용하여 설명해보겠습니다.\n다음과 같은 데이터가 있다고 가정해보겠습니다.\n\nimport pandas as pd\nidx = ['학생1', '학생2', '학생3', '학생4']\ndata = pd.DataFrame({'공부시간(m)':[150, 120, 60, 80], '성별':['여','남','남', '여'], '수학점수':[90, 65, 55, '???']},index=idx)\ndata\n\n\n\n\n\n\n\n\n공부시간(m)\n성별\n수학점수\n\n\n\n\n학생1\n150\n여\n90\n\n\n학생2\n120\n남\n65\n\n\n학생3\n60\n남\n55\n\n\n학생4\n80\n여\n???\n\n\n\n\n\n\n\n다음과 같은 데이터가 있다고 할 때, 저희는 학생4의 수학점수를 공부시간(m)을 통하여 예측하고 싶습니다.\n\n\n\n\n\n1. initialize model with a constant value:\\\n\n\n\\(F_0(x) = \\underset{\\gamma}{\\arg\\min}\\sum_{i=1}^n L(y_i, \\gamma)\\)\n\n\n\n\n초기모델을 상수로 정의합니다. 여기서 상수는 \\(y_i\\)와 \\(\\gamma\\)입니다.\n\\(y_i\\)는 수학점수를 의미하고 \\(\\gamma\\)는 초기 예측값입니다.\n\n\\(y_1\\) = 90, \\(y_2\\) = 65, \\(y_3\\) = 60\n\n여기서의 Loss Function \\(L(y_i, \\gamma) = \\displaystyle\\frac1n \\sum_{i=1}^{n} (y_i - \\gamma)^2\\)입니다. (잘 모르시는 분들은 MSE 학습하시면 좋을 것 같습니다.)\n예시의 수치를 대입하면\n\\(L(y_i, \\gamma) = \\displaystyle\\frac13 \\sum_{i=1}^{3} (y_i - \\gamma)^2 =  \\displaystyle\\frac13(90 - \\gamma)^2 + \\displaystyle\\frac13(65 - \\gamma)^2 + \\displaystyle\\frac13(55 - \\gamma)^2\\)입니다.\n\\(L(y_i, \\gamma)\\)가 최소가 되는 \\(\\gamma\\)를 찾아야 하므로 \\(\\displaystyle\\frac{\\partial L}{\\partial \\gamma} = \\displaystyle\\frac23(\\gamma - 90) + \\displaystyle\\frac23(\\gamma - 65) + \\displaystyle\\frac23(\\gamma - 55) = \\displaystyle\\frac23(3\\gamma - 210)\\)입니다.\n\\(\\displaystyle\\frac{\\partial L}{\\partial \\gamma} = 0\\)이 되어야 하므로 \\(\\gamma\\) = 70입니다. 따라서 초기 예측값 \\(\\gamma\\)는 70이 됩니다.\n초기 예측값을 이용한 첫번째 잔차는 각각 20, -5, -15가 됩니다.\n\n\nimport pandas as pd\nidx = ['Student1', 'Student2', 'Student3']\ndata = pd.DataFrame({'StudyTime(m)':[150, 120, 60], 'Sex':['female','male','male'], \n                     'MathScore':[90, 65, 55], 'F_0':[70 for i in range(3)],'r1': [20, -5, -15]},index=idx)\ndata\n\n\n\n\n\n\n\n\nStudyTime(m)\nSex\nMathScore\nF_0\nr1\n\n\n\n\nStudent1\n150\nfemale\n90\n70\n20\n\n\nStudent2\n120\nmale\n65\n70\n-5\n\n\nStudent3\n60\nmale\n55\n70\n-15\n\n\n\n\n\n\n\n- 위의 데이터를 의사결정나무로 시각화를 하면 다음과 같습니다.\n\nimport sklearn.tree\n## step1\nx = data[['StudyTime(m)']]\ny = data['MathScore']\n\n## step2\npredictr = sklearn.tree.DecisionTreeRegressor(max_depth=1)\n\n## step3\npredictr.fit(x,y)\n\n## visualization\nsklearn.tree.plot_tree(predictr,feature_names=x.columns);\n\n\n\n\n\n\n\n\n\n\n\n\nFor \\(m\\) = 1 to \\(M\\):\n\nCompute so-called pseudo-residuals \\(r_{im} = - \\Bigg[\\displaystyle\\frac{\\partial L(y_i, F(x_i))}{\\partial F(x_i)}\\Bigg]_{F(x) = F_{m-1}(x)}\\) for \\(i = 1, ... ,n\\)\nFit a base learner (or weak learner, e.g. tree) closed under scaling \\(h_m(x)\\) to pseuo-residuals, i.e. train it using the training set \\(\\{(x_i, r_{im})\\}_{i=1}^{n}\\)\nCompute multiplier \\(\\gamma_{m}\\) by solving the following one-dimensional optimization problem:\n\\(\\gamma_m = \\underset{\\gamma}{\\arg\\min}\\sum_{i=1}^n L(y_i, F_{m-1}(x_i) + \\gamma h_m(x_i))\\)\nUpdate the model:\n\\(F_m(x) = F_{m-1}(x) + \\gamma_m h_m(x)\\)\n\nA단계부터 D단계를 총 M번 반복하는 단계입니다.\n여기서 \\(M\\)은 예측기의 개수, \\(n\\)은 데이터의 개수입니다.\n예측기에 번호를 붙여 \\(m\\)=1인 경우는 1번 예측기 \\(m=M\\)인 경우는 M번 예측기라고 부르겠습니다.\n1번 예측기(\\(m=1\\))가 생성 되는 과정을 살펴보겠습니다. \\(L(y_i,F(x_i)) = \\displaystyle\\frac12(y_i - F(x_i))^2\\)이므로\n\n\\(r_{i1} = - \\Bigg[\\displaystyle\\frac{\\partial L(y_i, F(x_i))}{\\partial F(x_i)}\\Bigg]_{F(x) = F_{0}(x)} = y_i - F_0(x_i)\\)입니다. (잔차와 식이 동일합니다.)\n0번 예측기 \\(F_0(x)\\)는 1단계의 초기모델에서 생성되었습니다. 따라서 \\(F_0(x)=70\\)입니다.(1단계 참고)\n따라서 \\(r_{11}=20\\), \\(r_{21}=-5\\), \\(r_{31}=-15\\)입니다. 1단계 표에 있는 잔차1과 같습니다.\n함수\\(h_1(x)\\)는 앞서 계산한 잔차를 트리 계열로 학습합니다.(의사 결정 나무에 대해서는 추후에 업로드 하겠습니다.)\n모든 데이터(\\(n=3\\))에 대해서 \\(\\sum_{i=1}^{3} L(y_i, F_0(x_i) + \\gamma h_m(x_i)\\) 식이 최소가 되는 \\(\\gamma\\)(learning rate)를 찾습니다.(\\(\\gamma = \\gamma_{1}\\))\n모델을 업데이트 합니다 : \\(F_1(x) = F_0(x) + \\gamma_1 h_1(x)\\)\n\n4번의 과정을 M번 반복합니다.\n\n- 추가설명\n\n\n\n\n\n\n3. Output \\(\\hat{f(x)} = F_M(x)\\)\n\n\n\n\n최종 모델을 산출합니다."
  },
  {
    "objectID": "posts/GBM.html#코드-구현",
    "href": "posts/GBM.html#코드-구현",
    "title": "GBM",
    "section": "",
    "text": "(추후 업로드 예정)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "AI_theory",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\n \n\n\n \n\n\n이정재 \n\n\n\n\nMar 26, 2024\n\n\nRegularization\n\n\n이정재 \n\n\n\n\nMar 24, 2024\n\n\nJBIG코드실습_타이타닉(2주차)\n\n\n이정재 \n\n\n\n\nMar 24, 2024\n\n\n랜덤포레스트(배깅,보팅)\n\n\n이정재 \n\n\n\n\nMar 24, 2024\n\n\n로지스틱회귀\n\n\n이정재 \n\n\n\n\nMar 24, 2024\n\n\n의사결정나무\n\n\n이정재 \n\n\n\n\nMar 24, 2024\n\n\nJBIG코드실습_타이타닉(1주차)\n\n\n이정재 \n\n\n\n\nMar 22, 2024\n\n\nANN(1) - ANN의 구조\n\n\n이정재 \n\n\n\n\nMar 16, 2024\n\n\nGBM\n\n\n이정재 \n\n\n\n\n\nNo matching items"
  }
]