[
  {
    "objectID": "GBM.html",
    "href": "GBM.html",
    "title": "GBM (Gradient Boosting Machine)",
    "section": "",
    "text": "- 다음은 wikipedia의 Gradient Boosting Algorithm이다. 다음의 수식을 먼저 읽고 최대한 이해하려고 노력해보자\n\n\n\ninitialize model with a constant value:\n\\(F_0(x) = \\underset{\\gamma}{\\arg\\min}\\sum_{i=1}^n L(y_i, \\gamma)\\)\nFor m = 1 to M:\n\nCompute so-called pseudo-residuals \\(r_{im} = - \\Bigg[\\displaystyle\\frac{\\partial L(y_i, F(x_i))}{\\partial F(x_i)}\\Bigg]_{F(x) = F_{m-1}(x)}\\) for \\(i = 1, ... ,n\\)\nFit a base learner (or weak learner, e.g. tree) closed under scaling \\(h_m(x)\\) to pseuo-residuals, i.e. train it using the training set \\(\\{(x_i, r_{im})\\}_{i=1}^{n}\\)\nCompute multiplier \\(\\gamma_{m}\\) by solving the following one-dimensional optimization problem:\n\\(\\gamma_m = \\underset{\\gamma}{\\arg\\min}\\sum_{i=1}^n L(y_i, F_{m-1}(x_i) + \\gamma h_m(x_i))\\)\nUpdate the model:\n\\(F_m(x) = F_{m-1}(x) + \\gamma_m h_m(x)\\)\n\nOutput \\(\\hat{f(x)} = F_M(x)\\)\n\n수식으로만 완전히 이해하기가 어렵기 때문에 간단한 예시와 pandas dataframe을 이용하여 설명해보겠습니다.\n다음과 같은 데이터가 있다고 가정해보겠습니다.\n\nimport pandas as pd\ndata = pd.DataFrame({'공부시간(m)':[150, 120, 60, 80], '성별':['여','남','남', '여'], '수학점수':[90, 65, 55, '???']})\ndata\n\n\n\n\n\n\n\n\n공부시간(m)\n성별\n수학점수\n\n\n\n\n0\n150\n여\n90\n\n\n1\n120\n남\n65\n\n\n2\n60\n남\n55\n\n\n3\n80\n여\n???\n\n\n\n\n\n\n\n다음과 같은 데이터가 있다고 할 때, 저희는 공부시간(m)을 통하여 수학점수를 예측하고 싶습니다.\n\n\n\ninitialize model with a constant value:\n\\(F_0(x) = \\underset{\\gamma}{\\arg\\min}\\sum_{i=1}^n L(y_i, \\gamma)\\)\n초기모델을 상수로 정의합니다. 여기서 상수는 \\(y_i\\)와 \\(\\gamma\\)입니다.\n\\(y_i\\)는 수학점수를 의미하고 $$는 초기 예측값입니다.\n\n\\(y_1\\) = 90, \\(y_2\\) = 65, \\(y_3\\) = 60\n\n여기서의 Loss Function \\(L(y_i, \\gamma) = \\displaystyle\\frac1n \\sum_{i=1}^{n} (y_i - \\gamma)^2\\)입니다. (잘 모르시는 분들은 MSE 학습하시면 좋을 것 같습니다.)\n예시의 수치를 대입하면\n\\(L(y_i, \\gamma) = \\displaystyle\\frac13 \\sum_{i=1}^{3} (y_i - \\gamma)^2 =  \\displaystyle\\frac13(90 - \\gamma)^2 + \\displaystyle\\frac13(65 - \\gamma)^2 + \\displaystyle\\frac13(55 - \\gamma)^2\\)입니다.\n\\(L(y_i, \\gamma)\\)가 최소가 되는 \\(\\gamma\\)를 찾아야 하므로 \\(\\displaystyle\\frac{\\partial L}{\\partial \\gamma} = \\displaystyle\\frac23(\\gamma - 90) + \\displaystyle\\frac23(\\gamma - 65) + \\displaystyle\\frac23(\\gamma - 55) = \\displaystyle\\frac23(3\\gamma - 210)\\)입니다.\n\\(\\displaystyle\\frac{\\partial L}{\\partial \\gamma} = 0\\)이 되어야 하므로 \\(\\gamma\\) = 70입니다.\n\n\n\n\n\nFor \\(m\\) = 1 to \\(M\\):\n\nCompute so-called pseudo-residuals \\(r_{im} = - \\Bigg[\\displaystyle\\frac{\\partial L(y_i, F(x_i))}{\\partial F(x_i)}\\Bigg]_{F(x) = F_{m-1}(x)}\\) for \\(i = 1, ... ,n\\)\nFit a base learner (or weak learner, e.g. tree) closed under scaling \\(h_m(x)\\) to pseuo-residuals, i.e. train it using the training set \\(\\{(x_i, r_{im})\\}_{i=1}^{n}\\)\n\n\n\\(r_{im}\\)은 가짜 잔차(pseudo-residuals)라고 부릅니다.\n\\(L(y_i, F(x_i)) = \\displaystyle\\frac1n \\sum_{i=1}^{n}(y_i - F(x_i))^2\\)이므로\n\\(r_{im} = -\\Bigg[\\displaystyle\\frac{\\partial L(y_i, F(x_i))}{\\partial F(x_i)} \\Bigg]_{F(x)=F_{m-1}(x)} =\\displaystyle\\frac2n\\sum_{i=1}^{n}(y_i - F_{m-1}(x_i))\\)입니다. 정리하면,\n\\(r_{im} = \\displaystyle\\frac2n\\sum_{i=1}^{n}(y_i - F_{m-1}(x_i))\\)입니다.\n\n여기서 \\(M\\)은 예측기의 개수, \\(n\\)은 데이터의 개수입니다.\nFor \\(m\\) = 1 to \\(M\\)의 의미는 M개의 예측기에 대해서 특정 작업을 수행한다는 의미입니다.\nfor \\(i\\) = \\(1, ... ,n\\)은 \\(n\\)개의 설명변수 \\(x\\)에 대해서 \\(x_1\\)부터 \\(x_n\\)까지 특정 작업을 수행한다는 의미입니다.\n\\(r_{im}\\)은 가짜 잔차(pseudo-residuals)라고 부릅니다.\n\\((m-1)\\)번째 학습한 모델은 \\(F_{m-1}(x)\\)입니다. 이떄의 Loss Function을 \\(F_{m-1}(x)\\)로 미분한 것이 \\(-r_{im}\\)입니다.\n식을 확인해보면 실제 잔차\\((y_i - F_{m-1}(x))\\)와 유사해서 가짜 잔차로 불립니다.\n이때 \\(h_{m}(x)\\)는 가짜 잔차를 학습합니다.\n즉, \\(h_{m}(x) = \\widehat{y-F_{m-1}(x)}\\)\n\n\n\n\n\nFor m = 1 to M:\nC. Compute multiplier \\(\\gamma_{m}\\) by solving the following one-dimensional optimization problem:\n\\(\\gamma_m = \\underset{\\gamma}{\\arg\\min}\\sum_{i=1}^n L(y_i, F_{m-1}(x_i) + \\gamma h_m(x_i))\\)"
  },
  {
    "objectID": "GBM.html#algorithm",
    "href": "GBM.html#algorithm",
    "title": "GBM (Gradient Boosting Machine)",
    "section": "",
    "text": "initialize model with a constant value:\n\\(F_0(x) = \\underset{\\gamma}{\\arg\\min}\\sum_{i=1}^n L(y_i, \\gamma)\\)\nFor m = 1 to M:\n\nCompute so-called pseudo-residuals \\(r_{im} = - \\Bigg[\\displaystyle\\frac{\\partial L(y_i, F(x_i))}{\\partial F(x_i)}\\Bigg]_{F(x) = F_{m-1}(x)}\\) for \\(i = 1, ... ,n\\)\nFit a base learner (or weak learner, e.g. tree) closed under scaling \\(h_m(x)\\) to pseuo-residuals, i.e. train it using the training set \\(\\{(x_i, r_{im})\\}_{i=1}^{n}\\)\nCompute multiplier \\(\\gamma_{m}\\) by solving the following one-dimensional optimization problem:\n\\(\\gamma_m = \\underset{\\gamma}{\\arg\\min}\\sum_{i=1}^n L(y_i, F_{m-1}(x_i) + \\gamma h_m(x_i))\\)\nUpdate the model:\n\\(F_m(x) = F_{m-1}(x) + \\gamma_m h_m(x)\\)\n\nOutput \\(\\hat{f(x)} = F_M(x)\\)\n\n수식으로만 완전히 이해하기가 어렵기 때문에 간단한 예시와 pandas dataframe을 이용하여 설명해보겠습니다.\n다음과 같은 데이터가 있다고 가정해보겠습니다.\n\nimport pandas as pd\ndata = pd.DataFrame({'공부시간(m)':[150, 120, 60, 80], '성별':['여','남','남', '여'], '수학점수':[90, 65, 55, '???']})\ndata\n\n\n\n\n\n\n\n\n공부시간(m)\n성별\n수학점수\n\n\n\n\n0\n150\n여\n90\n\n\n1\n120\n남\n65\n\n\n2\n60\n남\n55\n\n\n3\n80\n여\n???\n\n\n\n\n\n\n\n다음과 같은 데이터가 있다고 할 때, 저희는 공부시간(m)을 통하여 수학점수를 예측하고 싶습니다.\n\n\n\ninitialize model with a constant value:\n\\(F_0(x) = \\underset{\\gamma}{\\arg\\min}\\sum_{i=1}^n L(y_i, \\gamma)\\)\n초기모델을 상수로 정의합니다. 여기서 상수는 \\(y_i\\)와 \\(\\gamma\\)입니다.\n\\(y_i\\)는 수학점수를 의미하고 $$는 초기 예측값입니다.\n\n\\(y_1\\) = 90, \\(y_2\\) = 65, \\(y_3\\) = 60\n\n여기서의 Loss Function \\(L(y_i, \\gamma) = \\displaystyle\\frac1n \\sum_{i=1}^{n} (y_i - \\gamma)^2\\)입니다. (잘 모르시는 분들은 MSE 학습하시면 좋을 것 같습니다.)\n예시의 수치를 대입하면\n\\(L(y_i, \\gamma) = \\displaystyle\\frac13 \\sum_{i=1}^{3} (y_i - \\gamma)^2 =  \\displaystyle\\frac13(90 - \\gamma)^2 + \\displaystyle\\frac13(65 - \\gamma)^2 + \\displaystyle\\frac13(55 - \\gamma)^2\\)입니다.\n\\(L(y_i, \\gamma)\\)가 최소가 되는 \\(\\gamma\\)를 찾아야 하므로 \\(\\displaystyle\\frac{\\partial L}{\\partial \\gamma} = \\displaystyle\\frac23(\\gamma - 90) + \\displaystyle\\frac23(\\gamma - 65) + \\displaystyle\\frac23(\\gamma - 55) = \\displaystyle\\frac23(3\\gamma - 210)\\)입니다.\n\\(\\displaystyle\\frac{\\partial L}{\\partial \\gamma} = 0\\)이 되어야 하므로 \\(\\gamma\\) = 70입니다.\n\n\n\n\n\nFor \\(m\\) = 1 to \\(M\\):\n\nCompute so-called pseudo-residuals \\(r_{im} = - \\Bigg[\\displaystyle\\frac{\\partial L(y_i, F(x_i))}{\\partial F(x_i)}\\Bigg]_{F(x) = F_{m-1}(x)}\\) for \\(i = 1, ... ,n\\)\nFit a base learner (or weak learner, e.g. tree) closed under scaling \\(h_m(x)\\) to pseuo-residuals, i.e. train it using the training set \\(\\{(x_i, r_{im})\\}_{i=1}^{n}\\)\n\n\n\\(r_{im}\\)은 가짜 잔차(pseudo-residuals)라고 부릅니다.\n\\(L(y_i, F(x_i)) = \\displaystyle\\frac1n \\sum_{i=1}^{n}(y_i - F(x_i))^2\\)이므로\n\\(r_{im} = -\\Bigg[\\displaystyle\\frac{\\partial L(y_i, F(x_i))}{\\partial F(x_i)} \\Bigg]_{F(x)=F_{m-1}(x)} =\\displaystyle\\frac2n\\sum_{i=1}^{n}(y_i - F_{m-1}(x_i))\\)입니다. 정리하면,\n\\(r_{im} = \\displaystyle\\frac2n\\sum_{i=1}^{n}(y_i - F_{m-1}(x_i))\\)입니다.\n\n여기서 \\(M\\)은 예측기의 개수, \\(n\\)은 데이터의 개수입니다.\nFor \\(m\\) = 1 to \\(M\\)의 의미는 M개의 예측기에 대해서 특정 작업을 수행한다는 의미입니다.\nfor \\(i\\) = \\(1, ... ,n\\)은 \\(n\\)개의 설명변수 \\(x\\)에 대해서 \\(x_1\\)부터 \\(x_n\\)까지 특정 작업을 수행한다는 의미입니다.\n\\(r_{im}\\)은 가짜 잔차(pseudo-residuals)라고 부릅니다.\n\\((m-1)\\)번째 학습한 모델은 \\(F_{m-1}(x)\\)입니다. 이떄의 Loss Function을 \\(F_{m-1}(x)\\)로 미분한 것이 \\(-r_{im}\\)입니다.\n식을 확인해보면 실제 잔차\\((y_i - F_{m-1}(x))\\)와 유사해서 가짜 잔차로 불립니다.\n이때 \\(h_{m}(x)\\)는 가짜 잔차를 학습합니다.\n즉, \\(h_{m}(x) = \\widehat{y-F_{m-1}(x)}\\)\n\n\n\n\n\nFor m = 1 to M:\nC. Compute multiplier \\(\\gamma_{m}\\) by solving the following one-dimensional optimization problem:\n\\(\\gamma_m = \\underset{\\gamma}{\\arg\\min}\\sum_{i=1}^n L(y_i, F_{m-1}(x_i) + \\gamma h_m(x_i))\\)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "AI_theory",
    "section": "",
    "text": "Post With Code\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nMar 16, 2024\n\n\nHarlow Malloc\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nMar 13, 2024\n\n\nTristan O’Malley\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  }
]